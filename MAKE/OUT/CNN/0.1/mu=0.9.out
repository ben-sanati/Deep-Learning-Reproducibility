Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.9}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.1
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.56143
	Epoch[2/200]: Training Loss = 1.05906
	Epoch[3/200]: Training Loss = 0.80274
	Epoch[4/200]: Training Loss = 0.65725
	Epoch[5/200]: Training Loss = 0.56237
	Epoch[6/200]: Training Loss = 0.48902
	Epoch[7/200]: Training Loss = 0.42089
	Epoch[8/200]: Training Loss = 0.37258
	Epoch[9/200]: Training Loss = 0.32619
	Epoch[10/200]: Training Loss = 0.29202
	Epoch[11/200]: Training Loss = 0.24508
	Epoch[12/200]: Training Loss = 0.22225
	Epoch[13/200]: Training Loss = 0.19211
	Epoch[14/200]: Training Loss = 0.16672
	Epoch[15/200]: Training Loss = 0.13652
	Epoch[16/200]: Training Loss = 0.13014
	Epoch[17/200]: Training Loss = 0.10639
	Epoch[18/200]: Training Loss = 0.09880
	Epoch[19/200]: Training Loss = 0.08751
	Epoch[20/200]: Training Loss = 0.07226
	Epoch[21/200]: Training Loss = 0.06112
	Epoch[22/200]: Training Loss = 0.05955
	Epoch[23/200]: Training Loss = 0.06808
	Epoch[24/200]: Training Loss = 0.05055
	Epoch[25/200]: Training Loss = 0.04025
	Epoch[26/200]: Training Loss = 0.04482
	Epoch[27/200]: Training Loss = 0.04173
	Epoch[28/200]: Training Loss = 0.03305
	Epoch[29/200]: Training Loss = 0.02574
	Epoch[30/200]: Training Loss = 0.02721
	Epoch[31/200]: Training Loss = 0.03460
	Epoch[32/200]: Training Loss = 0.01884
	Epoch[33/200]: Training Loss = 0.01668
	Epoch[34/200]: Training Loss = 0.01088
	Epoch[35/200]: Training Loss = 0.01024
	Epoch[36/200]: Training Loss = 0.00805
	Epoch[37/200]: Training Loss = 0.00651
	Epoch[38/200]: Training Loss = 0.00536
	Epoch[39/200]: Training Loss = 0.00993
	Epoch[40/200]: Training Loss = 0.02236
	Epoch[41/200]: Training Loss = 0.02710
	Epoch[42/200]: Training Loss = 0.03364
	Epoch[43/200]: Training Loss = 0.02872
	Epoch[44/200]: Training Loss = 0.02183
	Epoch[45/200]: Training Loss = 0.00793
	Epoch[46/200]: Training Loss = 0.00517
	Epoch[47/200]: Training Loss = 0.00339
	Epoch[48/200]: Training Loss = 0.00223
	Epoch[49/200]: Training Loss = 0.00161
	Epoch[50/200]: Training Loss = 0.00061
	Epoch[51/200]: Training Loss = 0.00040
	Epoch[52/200]: Training Loss = 0.00023
	Epoch[53/200]: Training Loss = 0.00017
	Epoch[54/200]: Training Loss = 0.00018
	Epoch[55/200]: Training Loss = 0.00014
	Epoch[56/200]: Training Loss = 0.00014
	Epoch[57/200]: Training Loss = 0.00011
	Epoch[58/200]: Training Loss = 0.00013
	Epoch[59/200]: Training Loss = 0.00060
	Epoch[60/200]: Training Loss = 0.00021
	Epoch[61/200]: Training Loss = 0.00016
	Epoch[62/200]: Training Loss = 0.00012
	Epoch[63/200]: Training Loss = 0.00011
	Epoch[64/200]: Training Loss = 0.00009
	Epoch[65/200]: Training Loss = 0.00008
	Epoch[66/200]: Training Loss = 0.00008
	Epoch[67/200]: Training Loss = 0.00008
	Epoch[68/200]: Training Loss = 0.00007
	Epoch[69/200]: Training Loss = 0.00007
	Epoch[70/200]: Training Loss = 0.00006
	Epoch[71/200]: Training Loss = 0.00006
	Epoch[72/200]: Training Loss = 0.00007
	Epoch[73/200]: Training Loss = 0.00007
	Epoch[74/200]: Training Loss = 0.00011
	Epoch[75/200]: Training Loss = 0.00007
	Epoch[76/200]: Training Loss = 0.00007
	Epoch[77/200]: Training Loss = 0.00006
	Epoch[78/200]: Training Loss = 0.00006
	Epoch[79/200]: Training Loss = 0.00005
	Epoch[80/200]: Training Loss = 0.00005
	Epoch[81/200]: Training Loss = 0.00005
	Epoch[82/200]: Training Loss = 0.00008
	Epoch[83/200]: Training Loss = 0.00008
	Epoch[84/200]: Training Loss = 0.00008
	Epoch[85/200]: Training Loss = 0.00009
	Epoch[86/200]: Training Loss = 0.00011
	Epoch[87/200]: Training Loss = 0.00006
	Epoch[88/200]: Training Loss = 0.00005
	Epoch[89/200]: Training Loss = 0.00005
	Epoch[90/200]: Training Loss = 0.00004
	Epoch[91/200]: Training Loss = 0.00004
	Epoch[92/200]: Training Loss = 0.00004
	Epoch[93/200]: Training Loss = 0.00005
	Epoch[94/200]: Training Loss = 0.00004
	Epoch[95/200]: Training Loss = 0.00004
	Epoch[96/200]: Training Loss = 0.00004
	Epoch[97/200]: Training Loss = 0.00004
	Epoch[98/200]: Training Loss = 0.00003
	Epoch[99/200]: Training Loss = 0.00004
	Epoch[100/200]: Training Loss = 0.00003
	Epoch[101/200]: Training Loss = 0.00003
	Epoch[102/200]: Training Loss = 0.00003
	Epoch[103/200]: Training Loss = 0.00003
	Epoch[104/200]: Training Loss = 0.00003
	Epoch[105/200]: Training Loss = 0.00003
	Epoch[106/200]: Training Loss = 0.00003
	Epoch[107/200]: Training Loss = 0.00003
	Epoch[108/200]: Training Loss = 0.00003
	Epoch[109/200]: Training Loss = 0.00003
	Epoch[110/200]: Training Loss = 0.00003
	Epoch[111/200]: Training Loss = 0.00003
	Epoch[112/200]: Training Loss = 0.00003
	Epoch[113/200]: Training Loss = 0.00003
	Epoch[114/200]: Training Loss = 0.00003
	Epoch[115/200]: Training Loss = 0.00003
	Epoch[116/200]: Training Loss = 0.00002
	Epoch[117/200]: Training Loss = 0.00004
	Epoch[118/200]: Training Loss = 0.00003
	Epoch[119/200]: Training Loss = 0.00003
	Epoch[120/200]: Training Loss = 0.00003
	Epoch[121/200]: Training Loss = 0.00002
	Epoch[122/200]: Training Loss = 0.00004
	Epoch[123/200]: Training Loss = 0.00003
	Epoch[124/200]: Training Loss = 0.00003
	Epoch[125/200]: Training Loss = 0.00002
	Epoch[126/200]: Training Loss = 0.00003
	Epoch[127/200]: Training Loss = 0.00002
	Epoch[128/200]: Training Loss = 0.00002
	Epoch[129/200]: Training Loss = 0.00002
	Epoch[130/200]: Training Loss = 0.00002
	Epoch[131/200]: Training Loss = 0.00002
	Epoch[132/200]: Training Loss = 0.00002
	Epoch[133/200]: Training Loss = 0.00002
	Epoch[134/200]: Training Loss = 0.00002
	Epoch[135/200]: Training Loss = 0.00002
	Epoch[136/200]: Training Loss = 0.00002
	Epoch[137/200]: Training Loss = 0.00002
	Epoch[138/200]: Training Loss = 0.00002
	Epoch[139/200]: Training Loss = 0.00002
	Epoch[140/200]: Training Loss = 0.00002
	Epoch[141/200]: Training Loss = 0.00002
	Epoch[142/200]: Training Loss = 0.00002
	Epoch[143/200]: Training Loss = 0.00002
	Epoch[144/200]: Training Loss = 0.00002
	Epoch[145/200]: Training Loss = 0.00002
	Epoch[146/200]: Training Loss = 0.00002
	Epoch[147/200]: Training Loss = 0.00002
	Epoch[148/200]: Training Loss = 0.00002
	Epoch[149/200]: Training Loss = 0.00002
	Epoch[150/200]: Training Loss = 0.00002
	Epoch[151/200]: Training Loss = 0.00003
	Epoch[152/200]: Training Loss = 0.00009
	Epoch[153/200]: Training Loss = 0.00004
	Epoch[154/200]: Training Loss = 0.00003
	Epoch[155/200]: Training Loss = 0.00002
	Epoch[156/200]: Training Loss = 0.00003
	Epoch[157/200]: Training Loss = 0.00003
	Epoch[158/200]: Training Loss = 0.00003
	Epoch[159/200]: Training Loss = 0.00002
	Epoch[160/200]: Training Loss = 0.00002
	Epoch[161/200]: Training Loss = 0.00002
	Epoch[162/200]: Training Loss = 0.00002
	Epoch[163/200]: Training Loss = 0.00002
	Epoch[164/200]: Training Loss = 0.00002
	Epoch[165/200]: Training Loss = 0.00002
	Epoch[166/200]: Training Loss = 0.00002
	Epoch[167/200]: Training Loss = 0.00002
	Epoch[168/200]: Training Loss = 0.00002
	Epoch[169/200]: Training Loss = 0.00002
	Epoch[170/200]: Training Loss = 0.00002
	Epoch[171/200]: Training Loss = 0.00002
	Epoch[172/200]: Training Loss = 0.00003
	Epoch[173/200]: Training Loss = 0.00033
	Epoch[174/200]: Training Loss = 0.00007
	Epoch[175/200]: Training Loss = 0.00005
	Epoch[176/200]: Training Loss = 0.00004
	Epoch[177/200]: Training Loss = 0.00003
	Epoch[178/200]: Training Loss = 0.00003
	Epoch[179/200]: Training Loss = 0.00003
	Epoch[180/200]: Training Loss = 0.00003
	Epoch[181/200]: Training Loss = 0.00004
	Epoch[182/200]: Training Loss = 0.00003
	Epoch[183/200]: Training Loss = 0.00003
	Epoch[184/200]: Training Loss = 0.00002
	Epoch[185/200]: Training Loss = 0.00002
	Epoch[186/200]: Training Loss = 0.00002
	Epoch[187/200]: Training Loss = 0.00002
	Epoch[188/200]: Training Loss = 0.00002
	Epoch[189/200]: Training Loss = 0.00003
	Epoch[190/200]: Training Loss = 0.00002
	Epoch[191/200]: Training Loss = 0.00002
	Epoch[192/200]: Training Loss = 0.00002
	Epoch[193/200]: Training Loss = 0.00002
	Epoch[194/200]: Training Loss = 0.00002
	Epoch[195/200]: Training Loss = 0.00002
	Epoch[196/200]: Training Loss = 0.00002
	Epoch[197/200]: Training Loss = 0.00002
	Epoch[198/200]: Training Loss = 0.00002
	Epoch[199/200]: Training Loss = 0.00002
	Epoch[200/200]: Training Loss = 0.00002
***Training Complete***

Final Optimizer Parameters
	alpha : 0.09647516161203384
	mu : 0.8991591930389404

***Testing Results***
==============================
Test Accuracy = 83.730 %
Test Error = 16.270 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.10000000149011612, 0.09909196943044662, 0.09869515895843506, 0.09842707216739655, 0.09822659939527512, 0.09807056188583374, 0.09792736917734146, 0.09779228270053864, 0.09767063707113266, 0.09756642580032349, 0.09747500717639923, 0.0973857045173645, 0.09730450809001923, 0.09721697121858597, 0.09714990109205246, 0.09707780182361603, 0.09702329337596893, 0.09696907550096512, 0.09691619127988815, 0.09686208516359329, 0.0968196913599968, 0.09678582102060318, 0.0967491865158081, 0.09671524912118912, 0.09669269621372223, 0.09667300432920456, 0.09664791077375412, 0.0966334342956543, 0.09662327170372009, 0.09661325067281723, 0.09660247713327408, 0.09658219665288925, 0.09657804667949677, 0.09657082706689835, 0.09656906127929688, 0.09656434506177902, 0.0965622141957283, 0.09656061977148056, 0.0965578630566597, 0.09655258059501648, 0.09653793275356293, 0.09652143716812134, 0.09650522470474243, 0.09649115800857544, 0.09648153930902481, 0.0964791476726532, 0.09647669643163681, 0.09647635370492935, 0.0964757576584816, 0.09647569805383682, 0.09647571295499802, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647568315267563, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548943758011, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647545963525772, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647546708583832, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647548198699951, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384, 0.09647516161203384]
mu: [0.8999999761581421, 0.8998185396194458, 0.8997155427932739, 0.8996472954750061, 0.8995972275733948, 0.8995544910430908, 0.8995173573493958, 0.8994866609573364, 0.8994576930999756, 0.8994293212890625, 0.8994029760360718, 0.8993803262710571, 0.8993581533432007, 0.899336040019989, 0.8993175625801086, 0.899300754070282, 0.899284839630127, 0.89927077293396, 0.8992571234703064, 0.899245023727417, 0.8992358446121216, 0.8992287516593933, 0.8992205262184143, 0.8992102742195129, 0.8992047309875488, 0.8992000222206116, 0.8991935849189758, 0.8991886377334595, 0.8991854190826416, 0.8991840481758118, 0.8991816639900208, 0.8991770148277283, 0.8991763591766357, 0.8991756439208984, 0.8991756439208984, 0.899175226688385, 0.8991750478744507, 0.8991749882698059, 0.8991749286651611, 0.8991744518280029, 0.8991716504096985, 0.8991683721542358, 0.8991642594337463, 0.8991614580154419, 0.8991592526435852, 0.8991592526435852, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404, 0.8991591930389404]
Loss: [2.551303486328125, 1.5614294130706787, 1.0590604189682007, 0.8027413343811035, 0.657254949760437, 0.562366284942627, 0.48902012471199036, 0.4208889620113373, 0.3725839118862152, 0.32618774006843565, 0.29201840915679933, 0.24508397504806517, 0.22224847786426544, 0.19211394231796264, 0.16672338085651398, 0.13652031532287598, 0.13013607818603515, 0.1063915624666214, 0.09879668148994446, 0.0875070731306076, 0.07225766628742218, 0.061118805177211764, 0.059552164425849916, 0.0680751902103424, 0.05054727277517319, 0.04024965566635132, 0.0448206168025732, 0.0417337420797348, 0.03304624961972236, 0.025737178491353988, 0.027205956161022185, 0.03460023236632347, 0.0188369514298439, 0.01667748171389103, 0.010876807192265987, 0.010239079584702849, 0.008052521042227746, 0.0065091636797785755, 0.0053595196279883385, 0.009928805676996708, 0.02235884681940079, 0.027104481532722713, 0.03363657011270523, 0.028719210323095323, 0.021826554394066333, 0.007931042892932892, 0.005170699412338436, 0.003392014583349228, 0.002233092845082283, 0.0016083802053332328, 0.0006129797516018152, 0.0004046753077255562, 0.00022761572193354368, 0.0001749902638560161, 0.00017580403050640597, 0.00014453539276495576, 0.000135002899649553, 0.00011376817509066313, 0.00012600928647443652, 0.0006006444920226932, 0.0002073872560542077, 0.00016342147943563759, 0.00011839608645066619, 0.00011193434573709965, 9.366304406779819e-05, 8.136919682845474e-05, 7.762421366292984e-05, 8.205980459228158e-05, 6.546819577924907e-05, 7.233419827069156e-05, 6.22486060814117e-05, 6.293590947519988e-05, 6.51397280395031e-05, 7.244319513905793e-05, 0.0001136791427526623, 7.203524097800255e-05, 7.10856629954651e-05, 6.182196978479624e-05, 6.050578085356392e-05, 5.2707447092980146e-05, 5.1937189092859624e-05, 4.77152234991081e-05, 7.79627293383237e-05, 8.063463622238488e-05, 7.542369742877781e-05, 9.106832658406347e-05, 0.00010566822800785303, 6.295088190352544e-05, 5.229357260512188e-05, 4.8843611208721994e-05, 4.410973013145849e-05, 4.261206525377929e-05, 3.967511885100976e-05, 4.6976362765417436e-05, 3.81172620668076e-05, 3.7294099740684034e-05, 3.655374235007912e-05, 3.577419899404049e-05, 3.057702406076714e-05, 3.73124376591295e-05, 3.10996865760535e-05, 3.144074689480476e-05, 3.154150155838579e-05, 3.213276977417991e-05, 2.878717927960679e-05, 2.9700198792852463e-05, 3.066252626507776e-05, 2.7834294852800668e-05, 2.598100063740276e-05, 2.542235696222633e-05, 2.9797783312387765e-05, 2.819626796990633e-05, 3.15738101862371e-05, 2.531402860302478e-05, 2.603197859658394e-05, 2.5390731814550235e-05, 2.4115821975283325e-05, 3.5231295444536954e-05, 2.8423707054462283e-05, 2.585950530861737e-05, 2.657618236262351e-05, 2.4983439937932415e-05, 4.183072465122677e-05, 2.9932911770883946e-05, 3.0049884677864613e-05, 2.388233007164672e-05, 2.5660079386434517e-05, 2.3113049992825835e-05, 2.279747964057606e-05, 2.3152599154273048e-05, 2.1107308633509093e-05, 2.229384724982083e-05, 2.2949770416598765e-05, 2.136083501478424e-05, 2.07104844157584e-05, 2.1550046589691193e-05, 2.2888569711940362e-05, 1.8783292677253485e-05, 1.9746487233787774e-05, 1.9429387846030296e-05, 2.1340786686632784e-05, 2.0120097293984146e-05, 1.7627960357931443e-05, 1.704829817870632e-05, 1.88886910059955e-05, 1.9408534405520187e-05, 1.8640312069328503e-05, 1.9420711904531345e-05, 1.911347312910948e-05, 1.7549144064541906e-05, 2.208890753099695e-05, 3.044654526282102e-05, 8.615112063358538e-05, 3.583434713422321e-05, 2.5313605037517845e-05, 2.4222337203100325e-05, 2.646665431326255e-05, 2.81831053562928e-05, 3.0327890319749714e-05, 2.2796041718684137e-05, 1.912815027520992e-05, 2.258124224608764e-05, 2.2246106942184268e-05, 1.97058541829756e-05, 2.236719727749005e-05, 2.0741336222272368e-05, 2.125261911423877e-05, 2.0058774346252904e-05, 2.0129727724852272e-05, 1.7669155565672553e-05, 1.7866675413679332e-05, 1.763209709431976e-05, 3.461701387539506e-05, 0.0003257742001116276, 6.98402917617932e-05, 5.3758234980050476e-05, 4.239494133740664e-05, 3.136208487674594e-05, 3.2929322433192284e-05, 2.8716097218130018e-05, 3.290789075894281e-05, 3.537678703316487e-05, 2.9977585233864375e-05, 2.7404965923633427e-05, 2.260140191530809e-05, 2.0299144133459775e-05, 2.119354348280467e-05, 1.881531481980346e-05, 2.076863702852279e-05, 2.7708409586921335e-05, 2.0125703267985955e-05, 1.6886914011556654e-05, 1.7745726675493644e-05, 1.9365980344591664e-05, 1.7754294821061193e-05, 1.684376064920798e-05, 1.8424268180970102e-05, 1.9134400188922883e-05, 1.5690411762334408e-05, 1.867237403988838e-05, 1.643734901386779e-05]
