Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.1
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.87084
	Epoch[2/200]: Training Loss = 1.62765
	Epoch[3/200]: Training Loss = 1.45575
	Epoch[4/200]: Training Loss = 1.32448
	Epoch[5/200]: Training Loss = 1.20082
	Epoch[6/200]: Training Loss = 1.06350
	Epoch[7/200]: Training Loss = 0.95747
	Epoch[8/200]: Training Loss = 0.87584
	Epoch[9/200]: Training Loss = 0.81378
	Epoch[10/200]: Training Loss = 0.74174
	Epoch[11/200]: Training Loss = 0.67121
	Epoch[12/200]: Training Loss = 0.62796
	Epoch[13/200]: Training Loss = 0.55946
	Epoch[14/200]: Training Loss = 0.52500
	Epoch[15/200]: Training Loss = 0.49617
	Epoch[16/200]: Training Loss = 0.46070
	Epoch[17/200]: Training Loss = 0.41361
	Epoch[18/200]: Training Loss = 0.38110
	Epoch[19/200]: Training Loss = 0.34690
	Epoch[20/200]: Training Loss = 0.31023
	Epoch[21/200]: Training Loss = 0.27729
	Epoch[22/200]: Training Loss = 0.24861
	Epoch[23/200]: Training Loss = 0.21567
	Epoch[24/200]: Training Loss = 0.18562
	Epoch[25/200]: Training Loss = 0.17574
	Epoch[26/200]: Training Loss = 0.16082
	Epoch[27/200]: Training Loss = 0.13718
	Epoch[28/200]: Training Loss = 0.13371
	Epoch[29/200]: Training Loss = 0.13003
	Epoch[30/200]: Training Loss = 0.12680
	Epoch[31/200]: Training Loss = 0.09996
	Epoch[32/200]: Training Loss = 0.07985
	Epoch[33/200]: Training Loss = 0.07514
	Epoch[34/200]: Training Loss = 0.08309
	Epoch[35/200]: Training Loss = 0.07643
	Epoch[36/200]: Training Loss = 0.06510
	Epoch[37/200]: Training Loss = 0.05947
	Epoch[38/200]: Training Loss = 0.06152
	Epoch[39/200]: Training Loss = 0.05372
	Epoch[40/200]: Training Loss = 0.04993
	Epoch[41/200]: Training Loss = 0.03881
	Epoch[42/200]: Training Loss = 0.03452
	Epoch[43/200]: Training Loss = 0.02824
	Epoch[44/200]: Training Loss = 0.03757
	Epoch[45/200]: Training Loss = 0.03732
	Epoch[46/200]: Training Loss = 0.04112
	Epoch[47/200]: Training Loss = 0.03510
	Epoch[48/200]: Training Loss = 0.03009
	Epoch[49/200]: Training Loss = 0.02772
	Epoch[50/200]: Training Loss = 0.02527
	Epoch[51/200]: Training Loss = 0.02036
	Epoch[52/200]: Training Loss = 0.02222
	Epoch[53/200]: Training Loss = 0.02138
	Epoch[54/200]: Training Loss = 0.02128
	Epoch[55/200]: Training Loss = 0.02139
	Epoch[56/200]: Training Loss = 0.01960
	Epoch[57/200]: Training Loss = 0.01971
	Epoch[58/200]: Training Loss = 0.01975
	Epoch[59/200]: Training Loss = 0.02111
	Epoch[60/200]: Training Loss = 0.01923
	Epoch[61/200]: Training Loss = 0.01517
	Epoch[62/200]: Training Loss = 0.01196
	Epoch[63/200]: Training Loss = 0.01218
	Epoch[64/200]: Training Loss = 0.01250
	Epoch[65/200]: Training Loss = 0.01346
	Epoch[66/200]: Training Loss = 0.01603
	Epoch[67/200]: Training Loss = 0.01143
	Epoch[68/200]: Training Loss = 0.01335
	Epoch[69/200]: Training Loss = 0.01174
	Epoch[70/200]: Training Loss = 0.01368
	Epoch[71/200]: Training Loss = 0.01451
	Epoch[72/200]: Training Loss = 0.01505
	Epoch[73/200]: Training Loss = 0.01553
	Epoch[74/200]: Training Loss = 0.01859
	Epoch[75/200]: Training Loss = 0.01947
	Epoch[76/200]: Training Loss = 0.01601
	Epoch[77/200]: Training Loss = 0.01687
	Epoch[78/200]: Training Loss = 0.01417
	Epoch[79/200]: Training Loss = 0.00916
	Epoch[80/200]: Training Loss = 0.01247
	Epoch[81/200]: Training Loss = 0.01643
	Epoch[82/200]: Training Loss = 0.01715
	Epoch[83/200]: Training Loss = 0.01415
	Epoch[84/200]: Training Loss = 0.01062
	Epoch[85/200]: Training Loss = 0.01005
	Epoch[86/200]: Training Loss = 0.00998
	Epoch[87/200]: Training Loss = 0.00746
	Epoch[88/200]: Training Loss = 0.00494
	Epoch[89/200]: Training Loss = 0.00348
	Epoch[90/200]: Training Loss = 0.00530
	Epoch[91/200]: Training Loss = 0.00724
	Epoch[92/200]: Training Loss = 0.00581
	Epoch[93/200]: Training Loss = 0.00461
	Epoch[94/200]: Training Loss = 0.00654
	Epoch[95/200]: Training Loss = 0.00504
	Epoch[96/200]: Training Loss = 0.00341
	Epoch[97/200]: Training Loss = 0.00322
	Epoch[98/200]: Training Loss = 0.00328
	Epoch[99/200]: Training Loss = 0.00269
	Epoch[100/200]: Training Loss = 0.00173
	Epoch[101/200]: Training Loss = 0.00091
	Epoch[102/200]: Training Loss = 0.00070
	Epoch[103/200]: Training Loss = 0.00022
	Epoch[104/200]: Training Loss = 0.00013
	Epoch[105/200]: Training Loss = 0.00007
	Epoch[106/200]: Training Loss = 0.00006
	Epoch[107/200]: Training Loss = 0.00005
	Epoch[108/200]: Training Loss = 0.00005
	Epoch[109/200]: Training Loss = 0.00004
	Epoch[110/200]: Training Loss = 0.00004
	Epoch[111/200]: Training Loss = 0.00003
	Epoch[112/200]: Training Loss = 0.00003
	Epoch[113/200]: Training Loss = 0.00003
	Epoch[114/200]: Training Loss = 0.00003
	Epoch[115/200]: Training Loss = 0.00005
	Epoch[116/200]: Training Loss = 0.00004
	Epoch[117/200]: Training Loss = 0.00003
	Epoch[118/200]: Training Loss = 0.00003
	Epoch[119/200]: Training Loss = 0.00003
	Epoch[120/200]: Training Loss = 0.00003
	Epoch[121/200]: Training Loss = 0.00003
	Epoch[122/200]: Training Loss = 0.00002
	Epoch[123/200]: Training Loss = 0.00002
	Epoch[124/200]: Training Loss = 0.00002
	Epoch[125/200]: Training Loss = 0.00002
	Epoch[126/200]: Training Loss = 0.00002
	Epoch[127/200]: Training Loss = 0.00002
	Epoch[128/200]: Training Loss = 0.00002
	Epoch[129/200]: Training Loss = 0.00002
	Epoch[130/200]: Training Loss = 0.00002
	Epoch[131/200]: Training Loss = 0.00002
	Epoch[132/200]: Training Loss = 0.00002
	Epoch[133/200]: Training Loss = 0.00002
	Epoch[134/200]: Training Loss = 0.00002
	Epoch[135/200]: Training Loss = 0.00002
	Epoch[136/200]: Training Loss = 0.00002
	Epoch[137/200]: Training Loss = 0.00002
	Epoch[138/200]: Training Loss = 0.00001
	Epoch[139/200]: Training Loss = 0.00001
	Epoch[140/200]: Training Loss = 0.00002
	Epoch[141/200]: Training Loss = 0.00002
	Epoch[142/200]: Training Loss = 0.00002
	Epoch[143/200]: Training Loss = 0.00001
	Epoch[144/200]: Training Loss = 0.00001
	Epoch[145/200]: Training Loss = 0.00001
	Epoch[146/200]: Training Loss = 0.00001
	Epoch[147/200]: Training Loss = 0.00001
	Epoch[148/200]: Training Loss = 0.00001
	Epoch[149/200]: Training Loss = 0.00001
	Epoch[150/200]: Training Loss = 0.00003
	Epoch[151/200]: Training Loss = 0.00010
	Epoch[152/200]: Training Loss = 0.00008
	Epoch[153/200]: Training Loss = 0.00003
	Epoch[154/200]: Training Loss = 0.00003
	Epoch[155/200]: Training Loss = 0.00003
	Epoch[156/200]: Training Loss = 0.00002
	Epoch[157/200]: Training Loss = 0.00002
	Epoch[158/200]: Training Loss = 0.00002
	Epoch[159/200]: Training Loss = 0.00002
	Epoch[160/200]: Training Loss = 0.00002
	Epoch[161/200]: Training Loss = 0.00002
	Epoch[162/200]: Training Loss = 0.00002
	Epoch[163/200]: Training Loss = 0.00002
	Epoch[164/200]: Training Loss = 0.00002
	Epoch[165/200]: Training Loss = 0.00001
	Epoch[166/200]: Training Loss = 0.00001
	Epoch[167/200]: Training Loss = 0.00002
	Epoch[168/200]: Training Loss = 0.00001
	Epoch[169/200]: Training Loss = 0.00001
	Epoch[170/200]: Training Loss = 0.00001
	Epoch[171/200]: Training Loss = 0.00001
	Epoch[172/200]: Training Loss = 0.00001
	Epoch[173/200]: Training Loss = 0.00001
	Epoch[174/200]: Training Loss = 0.00001
	Epoch[175/200]: Training Loss = 0.00001
	Epoch[176/200]: Training Loss = 0.00001
	Epoch[177/200]: Training Loss = 0.00001
	Epoch[178/200]: Training Loss = 0.00001
	Epoch[179/200]: Training Loss = 0.00001
	Epoch[180/200]: Training Loss = 0.00001
	Epoch[181/200]: Training Loss = 0.00001
	Epoch[182/200]: Training Loss = 0.00001
	Epoch[183/200]: Training Loss = 0.00001
	Epoch[184/200]: Training Loss = 0.00001
	Epoch[185/200]: Training Loss = 0.00001
	Epoch[186/200]: Training Loss = 0.00001
	Epoch[187/200]: Training Loss = 0.00001
	Epoch[188/200]: Training Loss = 0.00001
	Epoch[189/200]: Training Loss = 0.00001
	Epoch[190/200]: Training Loss = 0.00001
	Epoch[191/200]: Training Loss = 0.00001
	Epoch[192/200]: Training Loss = 0.00001
	Epoch[193/200]: Training Loss = 0.00001
	Epoch[194/200]: Training Loss = 0.00001
	Epoch[195/200]: Training Loss = 0.00001
	Epoch[196/200]: Training Loss = 0.00001
	Epoch[197/200]: Training Loss = 0.00001
	Epoch[198/200]: Training Loss = 0.00001
	Epoch[199/200]: Training Loss = 0.00001
	Epoch[200/200]: Training Loss = 0.00001
***Training Complete***

Final Optimizer Parameters
	alpha : 0.09863964468240738
	mu : 0.9896580576896667

***Testing Results***
==============================
Test Accuracy = 82.050 %
Test Error = 17.950 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.10000000149011612, 0.09967824816703796, 0.09964415431022644, 0.09960512071847916, 0.09956642240285873, 0.09952732175588608, 0.09949420392513275, 0.09944988042116165, 0.0994129478931427, 0.09938967227935791, 0.09935501962900162, 0.09932195395231247, 0.09929357469081879, 0.09926480799913406, 0.09923242777585983, 0.09920619428157806, 0.09918086975812912, 0.09915096312761307, 0.09913003444671631, 0.09910555183887482, 0.0990816205739975, 0.0990607887506485, 0.09903469681739807, 0.09901268035173416, 0.09898919612169266, 0.09896238148212433, 0.09893697500228882, 0.09891670197248459, 0.09889951348304749, 0.09887991100549698, 0.09886263310909271, 0.09884598106145859, 0.0988328754901886, 0.09882175922393799, 0.09880819171667099, 0.09879433363676071, 0.09878610819578171, 0.09877750277519226, 0.09876736998558044, 0.09875856339931488, 0.09875264018774033, 0.09874682128429413, 0.09874158352613449, 0.09873800724744797, 0.09873133152723312, 0.09872705489397049, 0.09871996194124222, 0.09871579706668854, 0.09871295839548111, 0.09870833903551102, 0.09870460629463196, 0.09870047122240067, 0.09869803488254547, 0.09869597852230072, 0.0986928790807724, 0.09869210422039032, 0.09869024902582169, 0.09868723899126053, 0.09868530184030533, 0.09868402034044266, 0.09868089109659195, 0.09867943078279495, 0.09867852926254272, 0.09867715835571289, 0.0986756905913353, 0.09867529571056366, 0.09867232292890549, 0.0986720472574234, 0.09867046773433685, 0.09866945445537567, 0.09866856783628464, 0.09866641461849213, 0.09866495430469513, 0.09866329282522202, 0.09866180270910263, 0.09865955263376236, 0.0986584946513176, 0.09865644574165344, 0.09865391999483109, 0.09865336120128632, 0.09865083545446396, 0.09864965081214905, 0.09864795953035355, 0.09864742308855057, 0.09864640980958939, 0.09864573180675507, 0.09864471852779388, 0.09864377230405807, 0.09864341467618942, 0.09864313155412674, 0.09864256531000137, 0.0986415222287178, 0.09864163398742676, 0.09864149987697601, 0.09864109009504318, 0.09864065051078796, 0.09864018112421036, 0.09864023327827454, 0.09863995015621185, 0.09863998740911484, 0.09863974153995514, 0.09863971918821335, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863963723182678, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738, 0.09863964468240738]
mu: [0.9900000095367432, 0.9899410009384155, 0.9899272918701172, 0.9899131059646606, 0.9898982048034668, 0.9898833632469177, 0.9898719191551208, 0.9898591637611389, 0.9898477792739868, 0.9898383617401123, 0.9898286461830139, 0.9898197054862976, 0.9898102879524231, 0.9898033142089844, 0.9897939562797546, 0.9897857308387756, 0.9897758960723877, 0.989768385887146, 0.989761471748352, 0.9897547960281372, 0.9897475838661194, 0.9897410273551941, 0.9897345304489136, 0.9897295832633972, 0.9897240996360779, 0.9897180199623108, 0.9897117614746094, 0.9897072315216064, 0.9897025227546692, 0.9896976947784424, 0.9896928071975708, 0.9896887540817261, 0.989686131477356, 0.9896837472915649, 0.9896805882453918, 0.9896769523620605, 0.9896754622459412, 0.9896733164787292, 0.9896703958511353, 0.9896687865257263, 0.9896675944328308, 0.9896669387817383, 0.9896663427352905, 0.9896659851074219, 0.9896647334098816, 0.9896637797355652, 0.9896624684333801, 0.9896620512008667, 0.989661693572998, 0.9896612763404846, 0.9896610379219055, 0.9896607398986816, 0.9896606206893921, 0.9896604418754578, 0.9896602630615234, 0.9896601438522339, 0.9896600842475891, 0.9896599650382996, 0.9896599054336548, 0.9896597862243652, 0.9896596074104309, 0.9896596074104309, 0.9896596074104309, 0.9896595478057861, 0.9896595478057861, 0.9896596074104309, 0.9896594285964966, 0.9896594285964966, 0.9896594285964966, 0.9896593689918518, 0.989659309387207, 0.9896592497825623, 0.9896591305732727, 0.9896588921546936, 0.9896585941314697, 0.989658534526825, 0.9896584749221802, 0.9896584153175354, 0.9896584153175354, 0.9896584153175354, 0.9896584153175354, 0.9896582961082458, 0.9896581768989563, 0.9896581768989563, 0.9896581768989563, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667, 0.9896580576896667]
Loss: [2.6028220854949953, 1.870838800315857, 1.6276524839019775, 1.4557544416046142, 1.3244777462005615, 1.2008231314849853, 1.063496276473999, 0.9574669853973389, 0.8758409674453735, 0.8137835784530639, 0.7417358022117615, 0.6712137087631226, 0.6279580861473083, 0.5594562964057922, 0.5249958211326599, 0.4961699745941162, 0.46070123052597045, 0.41360990518569946, 0.3811005794906616, 0.3468972157478333, 0.3102336085128784, 0.27729309222221377, 0.24860749687194825, 0.21567404713630675, 0.1856162985897064, 0.17573572113990785, 0.1608218902540207, 0.1371824444770813, 0.13371194520950316, 0.13002568346500396, 0.12680316390872, 0.09996419534683228, 0.07985452954292298, 0.075136148147583, 0.08308862491846085, 0.07642545295000076, 0.06510403100132942, 0.059469056224823, 0.061519720926880836, 0.05372494330406189, 0.049931657452583315, 0.03880777786940336, 0.03451517128944397, 0.028239450216293337, 0.037570496373176575, 0.037320289142131806, 0.041119490954875944, 0.03510168222904205, 0.03008940696001053, 0.027716751053333284, 0.025266708765029906, 0.020358533833026885, 0.02221920450747013, 0.021384947613477708, 0.021276638952493668, 0.02138532275915146, 0.019598662869930266, 0.01971421483039856, 0.01975301465332508, 0.021109436557590962, 0.019228459375575185, 0.015168220418840648, 0.01195890421807766, 0.012177892790399492, 0.01250287254780531, 0.013458725850582123, 0.01602896044805646, 0.011428141750395297, 0.013349474293589592, 0.01173760180771351, 0.013679737820923328, 0.014507894712090493, 0.015045689854621886, 0.01553344028621912, 0.01858891802713275, 0.019467585253715516, 0.016009995847940446, 0.016867026727199554, 0.014167797835357487, 0.009158470364809036, 0.012474630542993545, 0.016432957623004913, 0.01715469681441784, 0.014148273556679487, 0.010618953227251768, 0.010052653433680534, 0.009980199115201831, 0.007462362933196127, 0.004936844775378704, 0.0034781901711598037, 0.005304918135628104, 0.007236395339489682, 0.0058126410014927386, 0.004613117473721505, 0.006537617804221809, 0.005042339339479804, 0.0034084310752153396, 0.0032161750999838115, 0.0032794099309667946, 0.002687938983477652, 0.0017290992579748854, 0.0009122076683491468, 0.0007020010059617925, 0.00021634337287396193, 0.00013017345655476673, 7.449282085523009e-05, 6.270051217870787e-05, 4.716860905056819e-05, 5.0442000271286814e-05, 3.663256099214777e-05, 3.753424332011491e-05, 3.331820930121467e-05, 3.025215487694368e-05, 2.8991093411459588e-05, 3.466784464195371e-05, 4.524983440292999e-05, 3.9993983856402335e-05, 3.250664254534058e-05, 2.713086222531274e-05, 2.756042019696906e-05, 2.8750351597554982e-05, 2.608151318039745e-05, 2.2389231230481528e-05, 2.028233841760084e-05, 2.240201926557347e-05, 2.027836077148095e-05, 1.8639482093276455e-05, 2.065421401988715e-05, 2.212732535554096e-05, 2.214174685301259e-05, 2.281174628820736e-05, 2.044784060679376e-05, 1.7606104131555186e-05, 1.735721494886093e-05, 1.980962046305649e-05, 1.6015496925683692e-05, 1.566255039128009e-05, 1.6003992167534307e-05, 1.3638367466337514e-05, 1.4029393841046839e-05, 2.046131717885146e-05, 1.7152268862118944e-05, 1.6440742404665798e-05, 1.4878548729466275e-05, 1.4176554534351453e-05, 1.4361594687215984e-05, 1.347603221074678e-05, 1.182008178497199e-05, 1.21852147084428e-05, 1.1345723420381545e-05, 2.7397522900719197e-05, 0.00010047405984572834, 7.711078370688483e-05, 3.2371101742610334e-05, 3.2152252941159533e-05, 2.6523358651902526e-05, 1.7459663782501593e-05, 1.7054864793317394e-05, 1.9065110337687657e-05, 1.548834021319635e-05, 1.6539602079719772e-05, 1.767133158748038e-05, 1.7912982513662426e-05, 1.856070224312134e-05, 1.6970873688696885e-05, 1.3841966179898008e-05, 1.3983559792977758e-05, 1.6011939002200963e-05, 1.31406414572848e-05, 1.2422436459746678e-05, 1.3080052874283865e-05, 1.065796447219327e-05, 1.1934002235066146e-05, 1.0522048206767068e-05, 1.1590407860348933e-05, 1.0031657454092056e-05, 9.292201658827253e-06, 1.0165391761111095e-05, 9.69555752875749e-06, 8.890274530131138e-06, 8.93491281371098e-06, 1.0009455569088459e-05, 9.407945221028059e-06, 8.976027131138836e-06, 8.308500495622866e-06, 8.469174721976743e-06, 8.38822179415729e-06, 8.802978344901931e-06, 7.246385148609989e-06, 8.94027303555049e-06, 7.421845192147885e-06, 7.76647488004528e-06, 9.250321004510624e-06, 7.338189489091746e-06, 8.048610263795127e-06, 8.113640572410077e-06, 7.510651959455572e-06, 7.935765217407606e-06, 6.995556413749e-06, 8.620583363226615e-06, 7.50065691783675e-06]
