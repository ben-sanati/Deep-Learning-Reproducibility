Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.09}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.1
	kappa: 0.0
	num_epochs: 50
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.5955910762023926
	Epoch[1/50]: Training Loss = 1.53192
	Epoch[2/50]: Training Loss = 1.05426
	Epoch[3/50]: Training Loss = 0.85342
	Epoch[4/50]: Training Loss = 0.72878
	Epoch[5/50]: Training Loss = 0.63505
	Epoch[6/50]: Training Loss = 0.56972
	Epoch[7/50]: Training Loss = 0.51365
	Epoch[8/50]: Training Loss = 0.46726
	Epoch[9/50]: Training Loss = 0.42002
	Epoch[10/50]: Training Loss = 0.38730
	Epoch[11/50]: Training Loss = 0.34398
	Epoch[12/50]: Training Loss = 0.30637
	Epoch[13/50]: Training Loss = 0.27565
	Epoch[14/50]: Training Loss = 0.24617
	Epoch[15/50]: Training Loss = 0.21806
	Epoch[16/50]: Training Loss = 0.20598
	Epoch[17/50]: Training Loss = 0.17159
	Epoch[18/50]: Training Loss = 0.15133
	Epoch[19/50]: Training Loss = 0.14032
	Epoch[20/50]: Training Loss = 0.11660
	Epoch[21/50]: Training Loss = 0.11663
	Epoch[22/50]: Training Loss = 0.09349
	Epoch[23/50]: Training Loss = 0.08332
	Epoch[24/50]: Training Loss = 0.07327
	Epoch[25/50]: Training Loss = 0.06612
	Epoch[26/50]: Training Loss = 0.05686
	Epoch[27/50]: Training Loss = 0.05048
	Epoch[28/50]: Training Loss = 0.05386
	Epoch[29/50]: Training Loss = 0.04590
	Epoch[30/50]: Training Loss = 0.04037
	Epoch[31/50]: Training Loss = 0.03624
	Epoch[32/50]: Training Loss = 0.02324
	Epoch[33/50]: Training Loss = 0.01587
	Epoch[34/50]: Training Loss = 0.01747
	Epoch[35/50]: Training Loss = 0.02028
	Epoch[36/50]: Training Loss = 0.01813
	Epoch[37/50]: Training Loss = 0.02303
	Epoch[38/50]: Training Loss = 0.02678
	Epoch[39/50]: Training Loss = 0.04442
	Epoch[40/50]: Training Loss = 0.04007
	Epoch[41/50]: Training Loss = 0.03670
	Epoch[42/50]: Training Loss = 0.02604
	Epoch[43/50]: Training Loss = 0.01620
	Epoch[44/50]: Training Loss = 0.01280
	Epoch[45/50]: Training Loss = 0.02239
	Epoch[46/50]: Training Loss = 0.01321
	Epoch[47/50]: Training Loss = 0.00964
	Epoch[48/50]: Training Loss = 0.00904
	Epoch[49/50]: Training Loss = 0.00538
	Epoch[50/50]: Training Loss = 0.00307
***Training Complete***

Final Optimizer Parameters
	alpha : 0.10000000149011612
	mu : 0.09000000357627869

***Testing Results***
==============================
Test Accuracy = 81.430 %
Test Error = 18.570 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]
alpha: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]
mu: [0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869, 0.09000000357627869]
Loss: [2.5955910762023926, 1.531924183883667, 1.0542560130119323, 0.853424256477356, 0.7287798526954651, 0.6350495909976959, 0.5697152077102661, 0.5136452521133423, 0.4672614302444458, 0.4200225041294098, 0.3873003003692627, 0.3439830518245697, 0.3063724492740631, 0.27564898302078245, 0.24617046686172486, 0.21805580404281616, 0.2059756833457947, 0.17159478821754456, 0.15132557704925537, 0.1403172976398468, 0.11660023864746094, 0.11662531269073487, 0.09348638704299926, 0.08332283992290497, 0.07327346485853195, 0.06612063261032104, 0.05686090173959732, 0.05048313543319702, 0.0538567920422554, 0.045897749803066254, 0.040369467437267305, 0.03623638882279396, 0.02324361676096916, 0.01586823900461197, 0.017469575266838074, 0.020281398538351057, 0.018134348797798155, 0.023031011636257172, 0.026782889137268066, 0.04442252068519592, 0.04006557176828384, 0.03670335701942444, 0.026043928917646408, 0.016204763407707216, 0.012795413649082184, 0.02239485186815262, 0.013207241394519805, 0.009638460188508034, 0.009044522224664689, 0.005379089490473271, 0.0030723305294662715]
