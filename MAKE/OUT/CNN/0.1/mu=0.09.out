Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.09}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.1
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.54573
	Epoch[2/200]: Training Loss = 1.07647
	Epoch[3/200]: Training Loss = 0.86075
	Epoch[4/200]: Training Loss = 0.72582
	Epoch[5/200]: Training Loss = 0.63263
	Epoch[6/200]: Training Loss = 0.56071
	Epoch[7/200]: Training Loss = 0.49558
	Epoch[8/200]: Training Loss = 0.44855
	Epoch[9/200]: Training Loss = 0.40208
	Epoch[10/200]: Training Loss = 0.35836
	Epoch[11/200]: Training Loss = 0.31776
	Epoch[12/200]: Training Loss = 0.28058
	Epoch[13/200]: Training Loss = 0.23940
	Epoch[14/200]: Training Loss = 0.21005
	Epoch[15/200]: Training Loss = 0.17874
	Epoch[16/200]: Training Loss = 0.15282
	Epoch[17/200]: Training Loss = 0.12721
	Epoch[18/200]: Training Loss = 0.10481
	Epoch[19/200]: Training Loss = 0.08524
	Epoch[20/200]: Training Loss = 0.07231
	Epoch[21/200]: Training Loss = 0.05661
	Epoch[22/200]: Training Loss = 0.04332
	Epoch[23/200]: Training Loss = 0.03234
	Epoch[24/200]: Training Loss = 0.02723
	Epoch[25/200]: Training Loss = 0.01436
	Epoch[26/200]: Training Loss = 0.01170
	Epoch[27/200]: Training Loss = 0.00548
	Epoch[28/200]: Training Loss = 0.00324
	Epoch[29/200]: Training Loss = 0.00240
	Epoch[30/200]: Training Loss = 0.00194
	Epoch[31/200]: Training Loss = 0.00168
	Epoch[32/200]: Training Loss = 0.00153
	Epoch[33/200]: Training Loss = 0.00143
	Epoch[34/200]: Training Loss = 0.00124
	Epoch[35/200]: Training Loss = 0.00112
	Epoch[36/200]: Training Loss = 0.00107
	Epoch[37/200]: Training Loss = 0.00099
	Epoch[38/200]: Training Loss = 0.00098
	Epoch[39/200]: Training Loss = 0.00091
	Epoch[40/200]: Training Loss = 0.00100
	Epoch[41/200]: Training Loss = 0.00084
	Epoch[42/200]: Training Loss = 0.00086
	Epoch[43/200]: Training Loss = 0.00077
	Epoch[44/200]: Training Loss = 0.00074
	Epoch[45/200]: Training Loss = 0.00074
	Epoch[46/200]: Training Loss = 0.00072
	Epoch[47/200]: Training Loss = 0.00066
	Epoch[48/200]: Training Loss = 0.00061
	Epoch[49/200]: Training Loss = 0.00064
	Epoch[50/200]: Training Loss = 0.00059
	Epoch[51/200]: Training Loss = 0.00055
	Epoch[52/200]: Training Loss = 0.00056
	Epoch[53/200]: Training Loss = 0.00052
	Epoch[54/200]: Training Loss = 0.00053
	Epoch[55/200]: Training Loss = 0.00050
	Epoch[56/200]: Training Loss = 0.00053
	Epoch[57/200]: Training Loss = 0.00058
	Epoch[58/200]: Training Loss = 0.00052
	Epoch[59/200]: Training Loss = 0.00047
	Epoch[60/200]: Training Loss = 0.00046
	Epoch[61/200]: Training Loss = 0.00042
	Epoch[62/200]: Training Loss = 0.00043
	Epoch[63/200]: Training Loss = 0.00041
	Epoch[64/200]: Training Loss = 0.00043
	Epoch[65/200]: Training Loss = 0.00049
	Epoch[66/200]: Training Loss = 0.00042
	Epoch[67/200]: Training Loss = 0.00043
	Epoch[68/200]: Training Loss = 0.00042
	Epoch[69/200]: Training Loss = 0.00040
	Epoch[70/200]: Training Loss = 0.00038
	Epoch[71/200]: Training Loss = 0.00040
	Epoch[72/200]: Training Loss = 0.00037
	Epoch[73/200]: Training Loss = 0.00037
	Epoch[74/200]: Training Loss = 0.00033
	Epoch[75/200]: Training Loss = 0.00035
	Epoch[76/200]: Training Loss = 0.00077
	Epoch[77/200]: Training Loss = 0.00042
	Epoch[78/200]: Training Loss = 0.00034
	Epoch[79/200]: Training Loss = 0.00037
	Epoch[80/200]: Training Loss = 0.00045
	Epoch[81/200]: Training Loss = 0.00036
	Epoch[82/200]: Training Loss = 0.00042
	Epoch[83/200]: Training Loss = 0.00035
	Epoch[84/200]: Training Loss = 0.00034
	Epoch[85/200]: Training Loss = 0.00032
	Epoch[86/200]: Training Loss = 0.00037
	Epoch[87/200]: Training Loss = 0.00031
	Epoch[88/200]: Training Loss = 0.00029
	Epoch[89/200]: Training Loss = 0.00028
	Epoch[90/200]: Training Loss = 0.00028
	Epoch[91/200]: Training Loss = 0.00027
	Epoch[92/200]: Training Loss = 0.00029
	Epoch[93/200]: Training Loss = 0.00027
	Epoch[94/200]: Training Loss = 0.00024
	Epoch[95/200]: Training Loss = 0.00027
	Epoch[96/200]: Training Loss = 0.00070
	Epoch[97/200]: Training Loss = 0.00035
	Epoch[98/200]: Training Loss = 0.00031
	Epoch[99/200]: Training Loss = 0.00030
	Epoch[100/200]: Training Loss = 0.00028
	Epoch[101/200]: Training Loss = 0.00025
	Epoch[102/200]: Training Loss = 0.00030
	Epoch[103/200]: Training Loss = 0.00026
	Epoch[104/200]: Training Loss = 0.00024
	Epoch[105/200]: Training Loss = 0.00069
	Epoch[106/200]: Training Loss = 0.00033
	Epoch[107/200]: Training Loss = 0.00027
	Epoch[108/200]: Training Loss = 0.00028
	Epoch[109/200]: Training Loss = 0.00024
	Epoch[110/200]: Training Loss = 0.00022
	Epoch[111/200]: Training Loss = 0.00023
	Epoch[112/200]: Training Loss = 0.00026
	Epoch[113/200]: Training Loss = 0.00024
	Epoch[114/200]: Training Loss = 0.00023
	Epoch[115/200]: Training Loss = 0.00021
	Epoch[116/200]: Training Loss = 0.00021
	Epoch[117/200]: Training Loss = 0.00019
	Epoch[118/200]: Training Loss = 0.00022
	Epoch[119/200]: Training Loss = 0.00044
	Epoch[120/200]: Training Loss = 0.00024
	Epoch[121/200]: Training Loss = 0.00022
	Epoch[122/200]: Training Loss = 0.00021
	Epoch[123/200]: Training Loss = 0.00021
	Epoch[124/200]: Training Loss = 0.00020
	Epoch[125/200]: Training Loss = 0.00016
	Epoch[126/200]: Training Loss = 0.00018
	Epoch[127/200]: Training Loss = 0.00017
	Epoch[128/200]: Training Loss = 0.00017
	Epoch[129/200]: Training Loss = 0.00017
	Epoch[130/200]: Training Loss = 0.00017
	Epoch[131/200]: Training Loss = 0.00018
	Epoch[132/200]: Training Loss = 0.00016
	Epoch[133/200]: Training Loss = 0.00016
	Epoch[134/200]: Training Loss = 0.00016
	Epoch[135/200]: Training Loss = 0.00017
	Epoch[136/200]: Training Loss = 0.00017
	Epoch[137/200]: Training Loss = 0.00016
	Epoch[138/200]: Training Loss = 0.00015
	Epoch[139/200]: Training Loss = 0.00016
	Epoch[140/200]: Training Loss = 0.00014
	Epoch[141/200]: Training Loss = 0.00014
	Epoch[142/200]: Training Loss = 0.00014
	Epoch[143/200]: Training Loss = 0.00020
	Epoch[144/200]: Training Loss = 0.20315
	Epoch[145/200]: Training Loss = 0.06357
	Epoch[146/200]: Training Loss = 0.02667
	Epoch[147/200]: Training Loss = 0.01235
	Epoch[148/200]: Training Loss = 0.01726
	Epoch[149/200]: Training Loss = 0.00674
	Epoch[150/200]: Training Loss = 0.00240
	Epoch[151/200]: Training Loss = 0.00154
	Epoch[152/200]: Training Loss = 0.00103
	Epoch[153/200]: Training Loss = 0.00088
	Epoch[154/200]: Training Loss = 0.00072
	Epoch[155/200]: Training Loss = 0.00079
	Epoch[156/200]: Training Loss = 0.00073
	Epoch[157/200]: Training Loss = 0.00059
	Epoch[158/200]: Training Loss = 0.00052
	Epoch[159/200]: Training Loss = 0.00050
	Epoch[160/200]: Training Loss = 0.00062
	Epoch[161/200]: Training Loss = 0.00049
	Epoch[162/200]: Training Loss = 0.00046
	Epoch[163/200]: Training Loss = 0.00043
	Epoch[164/200]: Training Loss = 0.00041
	Epoch[165/200]: Training Loss = 0.00037
	Epoch[166/200]: Training Loss = 0.00038
	Epoch[167/200]: Training Loss = 0.00034
	Epoch[168/200]: Training Loss = 0.00034
	Epoch[169/200]: Training Loss = 0.00032
	Epoch[170/200]: Training Loss = 0.00033
	Epoch[171/200]: Training Loss = 0.00033
	Epoch[172/200]: Training Loss = 0.00030
	Epoch[173/200]: Training Loss = 0.00030
	Epoch[174/200]: Training Loss = 0.00029
	Epoch[175/200]: Training Loss = 0.00029
	Epoch[176/200]: Training Loss = 0.00027
	Epoch[177/200]: Training Loss = 0.00027
	Epoch[178/200]: Training Loss = 0.00025
	Epoch[179/200]: Training Loss = 0.00025
	Epoch[180/200]: Training Loss = 0.00023
	Epoch[181/200]: Training Loss = 0.00023
	Epoch[182/200]: Training Loss = 0.00023
	Epoch[183/200]: Training Loss = 0.00023
	Epoch[184/200]: Training Loss = 0.00023
	Epoch[185/200]: Training Loss = 0.00023
	Epoch[186/200]: Training Loss = 0.00023
	Epoch[187/200]: Training Loss = 0.00023
	Epoch[188/200]: Training Loss = 0.00021
	Epoch[189/200]: Training Loss = 0.00021
	Epoch[190/200]: Training Loss = 0.00023
	Epoch[191/200]: Training Loss = 0.00022
	Epoch[192/200]: Training Loss = 0.00020
	Epoch[193/200]: Training Loss = 0.00020
	Epoch[194/200]: Training Loss = 0.00019
	Epoch[195/200]: Training Loss = 0.00021
	Epoch[196/200]: Training Loss = 0.00020
	Epoch[197/200]: Training Loss = 0.00019
	Epoch[198/200]: Training Loss = 0.00019
	Epoch[199/200]: Training Loss = 0.00020
	Epoch[200/200]: Training Loss = 0.00088
***Training Complete***

Final Optimizer Parameters
	alpha : 0.054150767624378204
	mu : 0.08878066390752792

***Testing Results***
==============================
Test Accuracy = 79.970 %
Test Error = 20.030 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.10000000149011612, 0.09557151794433594, 0.09172406792640686, 0.0889340490102768, 0.08645885437726974, 0.08430273085832596, 0.08231431245803833, 0.08050097525119781, 0.0786013975739479, 0.07681292295455933, 0.07518952339887619, 0.07352863997220993, 0.07175793498754501, 0.07027191668748856, 0.06881197541952133, 0.06751473993062973, 0.06632387638092041, 0.06520461291074753, 0.06427370756864548, 0.06348378211259842, 0.06282584369182587, 0.0623120553791523, 0.061940740793943405, 0.06167592853307724, 0.06144509091973305, 0.06138496473431587, 0.06129748374223709, 0.06129269301891327, 0.061291471123695374, 0.06129080802202225, 0.06129046529531479, 0.06129029020667076, 0.061290010809898376, 0.06128966808319092, 0.061289578676223755, 0.06128952279686928, 0.061289459466934204, 0.06128937751054764, 0.06128924712538719, 0.061289213597774506, 0.06128915026783943, 0.06128913164138794, 0.061287570744752884, 0.06128757819533348, 0.061287567019462585, 0.06128754839301109, 0.061287522315979004, 0.06128747761249542, 0.061287473887205124, 0.061287470161914825, 0.061287447810173035, 0.06128742918372154, 0.061287421733140945, 0.06128741428256035, 0.06128740683197975, 0.06128741055727005, 0.06128739193081856, 0.061287350952625275, 0.061287347227334976, 0.06128733605146408, 0.06128733232617378, 0.061287328600883484, 0.06128733232617378, 0.061287328600883484, 0.06128732115030289, 0.061287060379981995, 0.061287056654691696, 0.06128699332475662, 0.06128697097301483, 0.061286963522434235, 0.061286941170692444, 0.061286844313144684, 0.061286840587854385, 0.06128682941198349, 0.06128682941198349, 0.061286814510822296, 0.06128140538930893, 0.061281394213438034, 0.061281394213438034, 0.061281364411115646, 0.0612812377512455, 0.0612812265753746, 0.06128133833408356, 0.06128132715821266, 0.06128132715821266, 0.06128131225705147, 0.0612812265753746, 0.061281222850084305, 0.061281222850084305, 0.0612812265753746, 0.0612812265753746, 0.06128121539950371, 0.06128120422363281, 0.06128118187189102, 0.06128118187189102, 0.06128118932247162, 0.06127973273396492, 0.061279717832803726, 0.06127971410751343, 0.06127972900867462, 0.061279717832803726, 0.06127972528338432, 0.06127971038222313, 0.06127970293164253, 0.06127968803048134, 0.06127786636352539, 0.0612778402864933, 0.0612778477370739, 0.0612778477370739, 0.0612778477370739, 0.0612778477370739, 0.061277832835912704, 0.06127780303359032, 0.06127779185771942, 0.06127777695655823, 0.06127777695655823, 0.06127777695655823, 0.06127777695655823, 0.061277784407138824, 0.06127646192908287, 0.06127645820379257, 0.06127645820379257, 0.06127645820379257, 0.061276454478502274, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276454478502274, 0.061276454478502274, 0.061276454478502274, 0.061276454478502274, 0.061276454478502274, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.061276450753211975, 0.06127643957734108, 0.05543360486626625, 0.05464382469654083, 0.05442602559924126, 0.05434158816933632, 0.054204415529966354, 0.05417688563466072, 0.0541759692132473, 0.05417602136731148, 0.054175958037376404, 0.05417587235569954, 0.054175954312086105, 0.05417585000395775, 0.05417577922344208, 0.054175715893507004, 0.0541757270693779, 0.05417567864060402, 0.054175566881895065, 0.05417560413479805, 0.054175592958927155, 0.05417561158537865, 0.05417555198073387, 0.054175540804862976, 0.054175522178411484, 0.05417550727725029, 0.05417549982666969, 0.05417550355195999, 0.054175492376089096, 0.05417545884847641, 0.05417545884847641, 0.05417545139789581, 0.05417545139789581, 0.054175443947315216, 0.054175443947315216, 0.05417544022202492, 0.05417544022202492, 0.05417544022202492, 0.05417544022202492, 0.05417543649673462, 0.05417543649673462, 0.05417543649673462, 0.05417543649673462, 0.05417544022202492, 0.054175421595573425, 0.05417541787028313, 0.05417541787028313, 0.05417541787028313, 0.05417536199092865, 0.05417536199092865, 0.05417536199092865, 0.05417536199092865, 0.05417536199092865, 0.05417535454034805, 0.05417535826563835, 0.05417535826563835, 0.05417535826563835, 0.05417535454034805, 0.054150767624378204]
mu: [0.09000000357627869, 0.0900852158665657, 0.09009136259555817, 0.09002435207366943, 0.08996248990297318, 0.08989633619785309, 0.089823417365551, 0.08975619077682495, 0.08969070762395859, 0.08961906284093857, 0.08954547345638275, 0.08947327733039856, 0.08940301090478897, 0.08933978527784348, 0.08927536010742188, 0.08921806514263153, 0.08916480094194412, 0.08912039548158646, 0.08908029645681381, 0.08904610574245453, 0.08901476860046387, 0.0889897495508194, 0.0889703705906868, 0.08895662426948547, 0.08894479274749756, 0.08894167840480804, 0.08893776684999466, 0.0889374315738678, 0.08893735706806183, 0.08893733471632004, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731981515884, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893731236457825, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893729746341705, 0.08893728256225586, 0.08893728256225586, 0.08893728256225586, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893727511167526, 0.08893701434135437, 0.08893701434135437, 0.08893701434135437, 0.08893701434135437, 0.08893698453903198, 0.08893698453903198, 0.08893691748380661, 0.08893691748380661, 0.08893691748380661, 0.08893691748380661, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893688768148422, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893660455942154, 0.08893661946058273, 0.08893658965826035, 0.08893658965826035, 0.08893658965826035, 0.08893658965826035, 0.08893658965826035, 0.08893658965826035, 0.08893658965826035, 0.08893658220767975, 0.08893658220767975, 0.08893658220767975, 0.08893658220767975, 0.08893658220767975, 0.08893658220767975, 0.08893658220767975, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08893647789955139, 0.08883783221244812, 0.08880777657032013, 0.08879845589399338, 0.0887952595949173, 0.0887843444943428, 0.08878248929977417, 0.08878247439861298, 0.08878243714570999, 0.0887824296951294, 0.0887824296951294, 0.0887824296951294, 0.0887824222445488, 0.0887824147939682, 0.0887824147939682, 0.0887824147939682, 0.0887824147939682, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878239244222641, 0.08878066390752792]
Loss: [2.4214714778137205, 1.5457336122894287, 1.0764694359016418, 0.8607461607742309, 0.7258225456619263, 0.6326284281539917, 0.5607140455436707, 0.4955831512451172, 0.44854914083480835, 0.40208192335128784, 0.3583611177444458, 0.31776244146347044, 0.2805768490505218, 0.23940177330970763, 0.21004939108848572, 0.17874103894233703, 0.15282063817501068, 0.1272147150993347, 0.10481455636024475, 0.08524233426094055, 0.07230683954715729, 0.056608108911514285, 0.04332489902496338, 0.03233764592289925, 0.027226637017428876, 0.014364801328182221, 0.011704884693026543, 0.005480116020143032, 0.003240936652570963, 0.002400551375821233, 0.001944156490340829, 0.0016797848570346832, 0.0015309343141317368, 0.0014301887501776218, 0.0012359202967956661, 0.0011166988273710013, 0.0010674396947026252, 0.0009875552324950694, 0.0009764616949856282, 0.000907803380638361, 0.0009955338056385518, 0.0008430918938666582, 0.00085763223875314, 0.0007701808400638402, 0.0007427351365238428, 0.0007385909380018711, 0.0007236414266377688, 0.0006573200099170208, 0.0006143891902454197, 0.0006403180020302534, 0.0005897211333364248, 0.0005475038665533066, 0.0005633700136095285, 0.0005203695578873158, 0.0005285032882448286, 0.0005036924932245165, 0.0005286350400745868, 0.0005754161117225886, 0.0005185765876434743, 0.000466358142234385, 0.00045795974932610987, 0.0004197192829102278, 0.0004264158722013235, 0.0004137859061360359, 0.00042772688522934915, 0.0004932785710692406, 0.0004157274017482996, 0.0004278424474596977, 0.00042207146234810353, 0.00040372039325535297, 0.000380019109249115, 0.00039863401369191704, 0.00036515457302331925, 0.00037491002001799643, 0.0003295840275287628, 0.0003480217044055462, 0.0007731261753290892, 0.00041962066289037464, 0.0003438623187597841, 0.0003678328640758991, 0.0004549499054811895, 0.00035673418037593363, 0.00041887095127254723, 0.00035297830725088716, 0.0003397836481034756, 0.00032227766305208203, 0.0003709275897219777, 0.00031106302957981824, 0.00029121753752231596, 0.0002754849897325039, 0.00027904266841709616, 0.00026548186782747506, 0.00028654777333140375, 0.0002682287583500147, 0.0002444235105998814, 0.000272878738604486, 0.0006964300996623933, 0.0003538402774184942, 0.00031276016945019366, 0.00029619953267276286, 0.00027725749086588623, 0.0002542976040393114, 0.0002965502714365721, 0.00026234504781663415, 0.00024424137271940706, 0.0006867916044592858, 0.00033420605592429637, 0.00026917345214635133, 0.00027935886396095156, 0.0002446056717261672, 0.00022187908313237132, 0.0002329240869730711, 0.0002621441974490881, 0.00023625472210347652, 0.00022977282525971532, 0.00020508482214063407, 0.00020969171190634369, 0.0001934301405120641, 0.0002181336610764265, 0.0004392642939090729, 0.00023627311781048774, 0.00022118113808333874, 0.00020929735243320466, 0.00020981267414987089, 0.00019997453428804873, 0.00016495124291162938, 0.0001822940788604319, 0.00017023660019040108, 0.00017093714687041938, 0.00017411469865590335, 0.00017097968325018883, 0.0001778742443025112, 0.0001596792570874095, 0.00016399960201233626, 0.00015849002882838248, 0.00017176320871338248, 0.00016678259782493114, 0.00015640544436872005, 0.00014692873135209084, 0.0001579095368180424, 0.0001433904602844268, 0.00014091270588338376, 0.00014203490808606148, 0.00020484673358500005, 0.20315284386873245, 0.06356827995419502, 0.026666436607539652, 0.012352840261459351, 0.017255976130962372, 0.0067409030050784345, 0.0024003934288024903, 0.0015404249495640397, 0.001030063971877098, 0.0008808892972581089, 0.0007220927184820175, 0.0007909534624218941, 0.0007252286003902555, 0.0005879675551503896, 0.0005167523677274585, 0.0005038791478425265, 0.0006195047993957997, 0.00048754857763648034, 0.0004578684627264738, 0.00043295479409396646, 0.0004066362749412656, 0.00037230536006391046, 0.0003800652067363262, 0.00034195023365318777, 0.00034447266751900314, 0.0003247659382224083, 0.00032805053509771823, 0.000331670295894146, 0.00029713383764028546, 0.0003016294197738171, 0.0002852583070844412, 0.0002868465054035187, 0.0002749319676309824, 0.0002679464606195688, 0.00025079787913709876, 0.0002463092686980963, 0.00023254513997584582, 0.0002349725100584328, 0.0002335075171198696, 0.00022815115856938064, 0.00023278146715834737, 0.0002292788514494896, 0.00022866276456043124, 0.00022630777656566353, 0.00021303454289212822, 0.00020640013165771961, 0.0002258220848441124, 0.0002230969361960888, 0.00019846904929727316, 0.00020260112401098012, 0.00019032780604436993, 0.0002142421905696392, 0.00019589302801527084, 0.00019342919495888054, 0.00019272295743227004, 0.00020032583612948656, 0.0008800179590471088]
