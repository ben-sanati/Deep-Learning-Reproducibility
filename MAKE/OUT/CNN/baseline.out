Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.9}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.1
	kappa: None
	num_epochs: 200
	batch_size: 256
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.50135
	Epoch[2/200]: Training Loss = 0.97229
	Epoch[3/200]: Training Loss = 0.76532
	Epoch[4/200]: Training Loss = 0.63528
	Epoch[5/200]: Training Loss = 0.54643
	Epoch[6/200]: Training Loss = 0.47590
	Epoch[7/200]: Training Loss = 0.42213
	Epoch[8/200]: Training Loss = 0.36843
	Epoch[9/200]: Training Loss = 0.32765
	Epoch[10/200]: Training Loss = 0.28303
	Epoch[11/200]: Training Loss = 0.24885
	Epoch[12/200]: Training Loss = 0.20882
	Epoch[13/200]: Training Loss = 0.18565
	Epoch[14/200]: Training Loss = 0.16512
	Epoch[15/200]: Training Loss = 0.14279
	Epoch[16/200]: Training Loss = 0.11774
	Epoch[17/200]: Training Loss = 0.11474
	Epoch[18/200]: Training Loss = 0.09601
	Epoch[19/200]: Training Loss = 0.08645
	Epoch[20/200]: Training Loss = 0.08583
	Epoch[21/200]: Training Loss = 0.06672
	Epoch[22/200]: Training Loss = 0.06316
	Epoch[23/200]: Training Loss = 0.05914
	Epoch[24/200]: Training Loss = 0.05377
	Epoch[25/200]: Training Loss = 0.04064
	Epoch[26/200]: Training Loss = 0.04601
	Epoch[27/200]: Training Loss = 0.03855
	Epoch[28/200]: Training Loss = 0.04228
	Epoch[29/200]: Training Loss = 0.04658
	Epoch[30/200]: Training Loss = 0.03403
	Epoch[31/200]: Training Loss = 0.01994
	Epoch[32/200]: Training Loss = 0.01921
	Epoch[33/200]: Training Loss = 0.01970
	Epoch[34/200]: Training Loss = 0.02351
	Epoch[35/200]: Training Loss = 0.02741
	Epoch[36/200]: Training Loss = 0.02801
	Epoch[37/200]: Training Loss = 0.02830
	Epoch[38/200]: Training Loss = 0.03116
	Epoch[39/200]: Training Loss = 0.01794
	Epoch[40/200]: Training Loss = 0.01363
	Epoch[41/200]: Training Loss = 0.02030
	Epoch[42/200]: Training Loss = 0.01735
	Epoch[43/200]: Training Loss = 0.01545
	Epoch[44/200]: Training Loss = 0.01307
	Epoch[45/200]: Training Loss = 0.00878
	Epoch[46/200]: Training Loss = 0.01425
	Epoch[47/200]: Training Loss = 0.01713
	Epoch[48/200]: Training Loss = 0.01499
	Epoch[49/200]: Training Loss = 0.01410
	Epoch[50/200]: Training Loss = 0.01751
	Epoch[51/200]: Training Loss = 0.01993
	Epoch[52/200]: Training Loss = 0.01934
	Epoch[53/200]: Training Loss = 0.01267
	Epoch[54/200]: Training Loss = 0.01278
	Epoch[55/200]: Training Loss = 0.01278
	Epoch[56/200]: Training Loss = 0.00826
	Epoch[57/200]: Training Loss = 0.00681
	Epoch[58/200]: Training Loss = 0.00622
	Epoch[59/200]: Training Loss = 0.00586
	Epoch[60/200]: Training Loss = 0.00604
	Epoch[61/200]: Training Loss = 0.00903
	Epoch[62/200]: Training Loss = 0.00701
	Epoch[63/200]: Training Loss = 0.00717
	Epoch[64/200]: Training Loss = 0.00733
	Epoch[65/200]: Training Loss = 0.00610
	Epoch[66/200]: Training Loss = 0.00537
	Epoch[67/200]: Training Loss = 0.00690
	Epoch[68/200]: Training Loss = 0.00536
	Epoch[69/200]: Training Loss = 0.00592
	Epoch[70/200]: Training Loss = 0.00488
	Epoch[71/200]: Training Loss = 0.00723
	Epoch[72/200]: Training Loss = 0.00730
	Epoch[73/200]: Training Loss = 0.00953
	Epoch[74/200]: Training Loss = 0.00950
	Epoch[75/200]: Training Loss = 0.00652
	Epoch[76/200]: Training Loss = 0.00637
	Epoch[77/200]: Training Loss = 0.00933
	Epoch[78/200]: Training Loss = 0.00787
	Epoch[79/200]: Training Loss = 0.00919
	Epoch[80/200]: Training Loss = 0.01007
	Epoch[81/200]: Training Loss = 0.01391
	Epoch[82/200]: Training Loss = 0.01311
	Epoch[83/200]: Training Loss = 0.01119
	Epoch[84/200]: Training Loss = 0.00916
	Epoch[85/200]: Training Loss = 0.00804
	Epoch[86/200]: Training Loss = 0.00554
	Epoch[87/200]: Training Loss = 0.00446
	Epoch[88/200]: Training Loss = 0.00266
	Epoch[89/200]: Training Loss = 0.00213
	Epoch[90/200]: Training Loss = 0.00549
	Epoch[91/200]: Training Loss = 0.00664
	Epoch[92/200]: Training Loss = 0.00431
	Epoch[93/200]: Training Loss = 0.00345
	Epoch[94/200]: Training Loss = 0.00641
	Epoch[95/200]: Training Loss = 0.00879
	Epoch[96/200]: Training Loss = 0.00747
	Epoch[97/200]: Training Loss = 0.00664
	Epoch[98/200]: Training Loss = 0.00483
	Epoch[99/200]: Training Loss = 0.00556
	Epoch[100/200]: Training Loss = 0.00324
	Epoch[101/200]: Training Loss = 0.00886
	Epoch[102/200]: Training Loss = 0.00667
	Epoch[103/200]: Training Loss = 0.00559
	Epoch[104/200]: Training Loss = 0.00575
	Epoch[105/200]: Training Loss = 0.00328
	Epoch[106/200]: Training Loss = 0.00692
	Epoch[107/200]: Training Loss = 0.00281
	Epoch[108/200]: Training Loss = 0.00494
	Epoch[109/200]: Training Loss = 0.01266
	Epoch[110/200]: Training Loss = 0.01167
	Epoch[111/200]: Training Loss = 0.00727
	Epoch[112/200]: Training Loss = 0.00523
	Epoch[113/200]: Training Loss = 0.00397
	Epoch[114/200]: Training Loss = 0.00393
	Epoch[115/200]: Training Loss = 0.00443
	Epoch[116/200]: Training Loss = 0.00646
	Epoch[117/200]: Training Loss = 0.00464
	Epoch[118/200]: Training Loss = 0.00769
	Epoch[119/200]: Training Loss = 0.00445
	Epoch[120/200]: Training Loss = 0.00356
	Epoch[121/200]: Training Loss = 0.00255
	Epoch[122/200]: Training Loss = 0.00320
	Epoch[123/200]: Training Loss = 0.00221
	Epoch[124/200]: Training Loss = 0.00350
	Epoch[125/200]: Training Loss = 0.00136
	Epoch[126/200]: Training Loss = 0.00130
	Epoch[127/200]: Training Loss = 0.00093
	Epoch[128/200]: Training Loss = 0.00123
	Epoch[129/200]: Training Loss = 0.00106
	Epoch[130/200]: Training Loss = 0.00131
	Epoch[131/200]: Training Loss = 0.00087
	Epoch[132/200]: Training Loss = 0.00041
	Epoch[133/200]: Training Loss = 0.00044
	Epoch[134/200]: Training Loss = 0.00060
	Epoch[135/200]: Training Loss = 0.00063
	Epoch[136/200]: Training Loss = 0.00033
	Epoch[137/200]: Training Loss = 0.00039
	Epoch[138/200]: Training Loss = 0.00175
	Epoch[139/200]: Training Loss = 0.00058
	Epoch[140/200]: Training Loss = 0.00052
	Epoch[141/200]: Training Loss = 0.00194
	Epoch[142/200]: Training Loss = 0.00052
	Epoch[143/200]: Training Loss = 0.00128
	Epoch[144/200]: Training Loss = 0.00201
	Epoch[145/200]: Training Loss = 0.00406
	Epoch[146/200]: Training Loss = 0.00130
	Epoch[147/200]: Training Loss = 0.00159
	Epoch[148/200]: Training Loss = 0.00181
	Epoch[149/200]: Training Loss = 0.00183
	Epoch[150/200]: Training Loss = 0.00165
	Epoch[151/200]: Training Loss = 0.00259
	Epoch[152/200]: Training Loss = 0.00443
	Epoch[153/200]: Training Loss = 0.00191
	Epoch[154/200]: Training Loss = 0.00488
	Epoch[155/200]: Training Loss = 0.00739
	Epoch[156/200]: Training Loss = 0.00453
	Epoch[157/200]: Training Loss = 0.00478
	Epoch[158/200]: Training Loss = 0.00719
	Epoch[159/200]: Training Loss = 0.00480
	Epoch[160/200]: Training Loss = 0.00410
	Epoch[161/200]: Training Loss = 0.00474
	Epoch[162/200]: Training Loss = 0.00433
	Epoch[163/200]: Training Loss = 0.00931
	Epoch[164/200]: Training Loss = 0.00573
	Epoch[165/200]: Training Loss = 0.00584
	Epoch[166/200]: Training Loss = 0.00347
	Epoch[167/200]: Training Loss = 0.00155
	Epoch[168/200]: Training Loss = 0.00219
	Epoch[169/200]: Training Loss = 0.00115
	Epoch[170/200]: Training Loss = 0.00121
	Epoch[171/200]: Training Loss = 0.00181
	Epoch[172/200]: Training Loss = 0.00125
	Epoch[173/200]: Training Loss = 0.00141
	Epoch[174/200]: Training Loss = 0.00107
	Epoch[175/200]: Training Loss = 0.00053
	Epoch[176/200]: Training Loss = 0.00070
	Epoch[177/200]: Training Loss = 0.00594
	Epoch[178/200]: Training Loss = 0.00247
	Epoch[179/200]: Training Loss = 0.00339
	Epoch[180/200]: Training Loss = 0.00229
	Epoch[181/200]: Training Loss = 0.00297
	Epoch[182/200]: Training Loss = 0.00490
	Epoch[183/200]: Training Loss = 0.00496
	Epoch[184/200]: Training Loss = 0.00324
	Epoch[185/200]: Training Loss = 0.00182
	Epoch[186/200]: Training Loss = 0.00143
	Epoch[187/200]: Training Loss = 0.00104
	Epoch[188/200]: Training Loss = 0.00160
	Epoch[189/200]: Training Loss = 0.00085
	Epoch[190/200]: Training Loss = 0.00070
	Epoch[191/200]: Training Loss = 0.00049
	Epoch[192/200]: Training Loss = 0.00047
	Epoch[193/200]: Training Loss = 0.00055
	Epoch[194/200]: Training Loss = 0.00029
	Epoch[195/200]: Training Loss = 0.00042
	Epoch[196/200]: Training Loss = 0.00023
	Epoch[197/200]: Training Loss = 0.00014
	Epoch[198/200]: Training Loss = 0.00018
	Epoch[199/200]: Training Loss = 0.00043
	Epoch[200/200]: Training Loss = 0.00120
***Training Complete***

Final Optimizer Parameters
	alpha : 0.10000000149011612
	mu : 0.8999999761581421

***Testing Results***
==============================
Test Accuracy = 82.760 %
Test Error = 17.240 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]
mu: [0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421]
Loss: [0.010049087438583374, 1.5013482723999023, 0.9722914511489869, 0.7653154297065735, 0.6352798997116089, 0.5464268581008911, 0.4759025127220154, 0.4221274789619446, 0.3684288311576843, 0.3276496801662445, 0.2830302741241455, 0.248853283700943, 0.20882152769088744, 0.18565076872825623, 0.16511507337570192, 0.14279024906635285, 0.11773589522361755, 0.11473606343269348, 0.09601136560916901, 0.08645071727752686, 0.08583406913280488, 0.06671836811542511, 0.06315545527458191, 0.05913975296497345, 0.05377477956533432, 0.040637605304718015, 0.046006187039613726, 0.03855421780586243, 0.04228491934299469, 0.04658238701581955, 0.0340269144590199, 0.019936544365882873, 0.019207350764572622, 0.019704710610508918, 0.023513194056749342, 0.027413489878177644, 0.028011029603481293, 0.02829830022662878, 0.03115850648641586, 0.017939679576158524, 0.013626888303160667, 0.02030120238006115, 0.017349560310840607, 0.015454725618958474, 0.013074984785020352, 0.008776338233053684, 0.014253859832584858, 0.0171282996827364, 0.014985497623085976, 0.01410364516377449, 0.01751253971993923, 0.019931451919078826, 0.019342178338766097, 0.012668332988023757, 0.012784977998137475, 0.012782086772918701, 0.008257595790624619, 0.006810269518345595, 0.006216478771865368, 0.005857048069983721, 0.006035599229708314, 0.009027785184681416, 0.007009900467693806, 0.007172301800996065, 0.0073342821423895655, 0.006101105464473367, 0.005370816268473863, 0.006904695235565305, 0.005355900098569691, 0.005921421805433929, 0.004882193968594074, 0.007229098935425281, 0.007304594272896647, 0.009532764104604722, 0.00950181393623352, 0.006516003533601761, 0.0063741848492622375, 0.009334755119234324, 0.007869906556904315, 0.009192875082790852, 0.010068196572959423, 0.01390523160457611, 0.013111512558460235, 0.011191926677823066, 0.009158902047872543, 0.008041296102702617, 0.005537653135694563, 0.004455786510035396, 0.0026623991606384515, 0.0021304417642205955, 0.005490165342390537, 0.0066371766988933085, 0.004308104934617877, 0.003453371444158256, 0.006413419057577849, 0.008792921416461468, 0.007470925333946944, 0.006644803978577256, 0.0048284772567451, 0.005560726552158594, 0.0032352979543060064, 0.008857006505131722, 0.0066692048817127945, 0.0055892745386809115, 0.005753085069432854, 0.0032822320076823235, 0.0069208554136753085, 0.002809034191556275, 0.004943328156024217, 0.012656360612511635, 0.011667376546561718, 0.007272909707501531, 0.005227359095588326, 0.003974368046149612, 0.003925772086754441, 0.004433525814563036, 0.006460358676314354, 0.004643323970511556, 0.007685774086862803, 0.004454914744123816, 0.00355771189160645, 0.002545375317465514, 0.0032034308492392303, 0.0022080447110533713, 0.00349994044654537, 0.0013606806662864984, 0.001298669012542814, 0.0009278598073497415, 0.001225831042136997, 0.0010606994971819222, 0.00130709209038876, 0.0008730932559072971, 0.000414423039178364, 0.00044094579587690533, 0.00059748923253268, 0.0006289781800284982, 0.00032556683542672547, 0.00039178850430296734, 0.0017450375322811305, 0.0005826076755020768, 0.0005229121920093895, 0.0019404004884697498, 0.0005203790320968256, 0.0012794621467776596, 0.002008138932650909, 0.004055078026503325, 0.0013047787092626095, 0.0015894889539480208, 0.001811796998186037, 0.0018279361114650965, 0.0016546219521947204, 0.0025943698754534125, 0.004426464927038178, 0.001906413969863206, 0.004880612820982933, 0.007386969356909395, 0.0045262618850916625, 0.00477629399523139, 0.007193634005561471, 0.004804955227375031, 0.004095656649954617, 0.00473960762232542, 0.004326202129069715, 0.009313895325511693, 0.00573332368850708, 0.005836861525299028, 0.0034655784046649933, 0.0015456445656903087, 0.0021938986331224443, 0.0011477942142635584, 0.0012139723858516664, 0.0018093348025809974, 0.0012540934024751187, 0.0014054499097168446, 0.0010698207232821733, 0.0005323002622462809, 0.000703070385302417, 0.005938021653369069, 0.002466582516785711, 0.003394810514692217, 0.002293574164537713, 0.00297113627191633, 0.004899440570631996, 0.004959415633715689, 0.003243479057699442, 0.0018210509947873652, 0.0014312872631754726, 0.0010431706439889968, 0.001603546056561172, 0.0008451869576075115, 0.0007029135008901358, 0.00049492694493616, 0.0004667556069931015, 0.0005485339352674782, 0.0002912845636438578, 0.00042341815295629206, 0.0002324392405198887, 0.0001381496785208583, 0.00017645902410149574, 0.00043023933542892336, 0.0012040062806475908]
