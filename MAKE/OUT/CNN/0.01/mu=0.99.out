Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.01
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.68549
	Epoch[2/200]: Training Loss = 1.26886
	Epoch[3/200]: Training Loss = 1.03852
	Epoch[4/200]: Training Loss = 0.89137
	Epoch[5/200]: Training Loss = 0.76620
	Epoch[6/200]: Training Loss = 0.68706
	Epoch[7/200]: Training Loss = 0.61398
	Epoch[8/200]: Training Loss = 0.54598
	Epoch[9/200]: Training Loss = 0.47176
	Epoch[10/200]: Training Loss = 0.41697
	Epoch[11/200]: Training Loss = 0.38203
	Epoch[12/200]: Training Loss = 0.33811
	Epoch[13/200]: Training Loss = 0.29279
	Epoch[14/200]: Training Loss = 0.26045
	Epoch[15/200]: Training Loss = 0.22419
	Epoch[16/200]: Training Loss = 0.20512
	Epoch[17/200]: Training Loss = 0.17068
	Epoch[18/200]: Training Loss = 0.14420
	Epoch[19/200]: Training Loss = 0.12727
	Epoch[20/200]: Training Loss = 0.11778
	Epoch[21/200]: Training Loss = 0.11351
	Epoch[22/200]: Training Loss = 0.09913
	Epoch[23/200]: Training Loss = 0.08771
	Epoch[24/200]: Training Loss = 0.07385
	Epoch[25/200]: Training Loss = 0.06757
	Epoch[26/200]: Training Loss = 0.05303
	Epoch[27/200]: Training Loss = 0.03943
	Epoch[28/200]: Training Loss = 0.03779
	Epoch[29/200]: Training Loss = 0.03914
	Epoch[30/200]: Training Loss = 0.03759
	Epoch[31/200]: Training Loss = 0.03801
	Epoch[32/200]: Training Loss = 0.03851
	Epoch[33/200]: Training Loss = 0.04540
	Epoch[34/200]: Training Loss = 0.05227
	Epoch[35/200]: Training Loss = 0.04754
	Epoch[36/200]: Training Loss = 0.03880
	Epoch[37/200]: Training Loss = 0.02914
	Epoch[38/200]: Training Loss = 0.01997
	Epoch[39/200]: Training Loss = 0.01323
	Epoch[40/200]: Training Loss = 0.00875
	Epoch[41/200]: Training Loss = 0.00598
	Epoch[42/200]: Training Loss = 0.00461
	Epoch[43/200]: Training Loss = 0.00364
	Epoch[44/200]: Training Loss = 0.00248
	Epoch[45/200]: Training Loss = 0.00166
	Epoch[46/200]: Training Loss = 0.00136
	Epoch[47/200]: Training Loss = 0.00106
	Epoch[48/200]: Training Loss = 0.00061
	Epoch[49/200]: Training Loss = 0.00038
	Epoch[50/200]: Training Loss = 0.00023
	Epoch[51/200]: Training Loss = 0.00018
	Epoch[52/200]: Training Loss = 0.00015
	Epoch[53/200]: Training Loss = 0.00015
	Epoch[54/200]: Training Loss = 0.00051
	Epoch[55/200]: Training Loss = 0.00037
	Epoch[56/200]: Training Loss = 0.00024
	Epoch[57/200]: Training Loss = 0.00018
	Epoch[58/200]: Training Loss = 0.00015
	Epoch[59/200]: Training Loss = 0.00012
	Epoch[60/200]: Training Loss = 0.00011
	Epoch[61/200]: Training Loss = 0.00009
	Epoch[62/200]: Training Loss = 0.00008
	Epoch[63/200]: Training Loss = 0.00009
	Epoch[64/200]: Training Loss = 0.00012
	Epoch[65/200]: Training Loss = 0.00014
	Epoch[66/200]: Training Loss = 0.00013
	Epoch[67/200]: Training Loss = 0.00011
	Epoch[68/200]: Training Loss = 0.00008
	Epoch[69/200]: Training Loss = 0.00008
	Epoch[70/200]: Training Loss = 0.00007
	Epoch[71/200]: Training Loss = 0.00007
	Epoch[72/200]: Training Loss = 0.00007
	Epoch[73/200]: Training Loss = 0.00005
	Epoch[74/200]: Training Loss = 0.00005
	Epoch[75/200]: Training Loss = 0.00006
	Epoch[76/200]: Training Loss = 0.00005
	Epoch[77/200]: Training Loss = 0.00005
	Epoch[78/200]: Training Loss = 0.00005
	Epoch[79/200]: Training Loss = 0.00005
	Epoch[80/200]: Training Loss = 0.00004
	Epoch[81/200]: Training Loss = 0.00004
	Epoch[82/200]: Training Loss = 0.00004
	Epoch[83/200]: Training Loss = 0.00004
	Epoch[84/200]: Training Loss = 0.00004
	Epoch[85/200]: Training Loss = 0.00004
	Epoch[86/200]: Training Loss = 0.00004
	Epoch[87/200]: Training Loss = 0.00004
	Epoch[88/200]: Training Loss = 0.00003
	Epoch[89/200]: Training Loss = 0.00003
	Epoch[90/200]: Training Loss = 0.00004
	Epoch[91/200]: Training Loss = 0.00003
	Epoch[92/200]: Training Loss = 0.00004
	Epoch[93/200]: Training Loss = 0.00003
	Epoch[94/200]: Training Loss = 0.00003
	Epoch[95/200]: Training Loss = 0.00004
	Epoch[96/200]: Training Loss = 0.00003
	Epoch[97/200]: Training Loss = 0.00003
	Epoch[98/200]: Training Loss = 0.00003
	Epoch[99/200]: Training Loss = 0.00003
	Epoch[100/200]: Training Loss = 0.00003
	Epoch[101/200]: Training Loss = 0.00003
	Epoch[102/200]: Training Loss = 0.00003
	Epoch[103/200]: Training Loss = 0.00003
	Epoch[104/200]: Training Loss = 0.00003
	Epoch[105/200]: Training Loss = 0.00003
	Epoch[106/200]: Training Loss = 0.00003
	Epoch[107/200]: Training Loss = 0.00002
	Epoch[108/200]: Training Loss = 0.00003
	Epoch[109/200]: Training Loss = 0.00002
	Epoch[110/200]: Training Loss = 0.00003
	Epoch[111/200]: Training Loss = 0.00009
	Epoch[112/200]: Training Loss = 0.00012
	Epoch[113/200]: Training Loss = 0.00011
	Epoch[114/200]: Training Loss = 0.00007
	Epoch[115/200]: Training Loss = 0.00005
	Epoch[116/200]: Training Loss = 0.00004
	Epoch[117/200]: Training Loss = 0.00003
	Epoch[118/200]: Training Loss = 0.00004
	Epoch[119/200]: Training Loss = 0.00003
	Epoch[120/200]: Training Loss = 0.00003
	Epoch[121/200]: Training Loss = 0.00003
	Epoch[122/200]: Training Loss = 0.00003
	Epoch[123/200]: Training Loss = 0.00003
	Epoch[124/200]: Training Loss = 0.00003
	Epoch[125/200]: Training Loss = 0.00003
	Epoch[126/200]: Training Loss = 0.00002
	Epoch[127/200]: Training Loss = 0.00003
	Epoch[128/200]: Training Loss = 0.00003
	Epoch[129/200]: Training Loss = 0.00003
	Epoch[130/200]: Training Loss = 0.00002
	Epoch[131/200]: Training Loss = 0.00002
	Epoch[132/200]: Training Loss = 0.00002
	Epoch[133/200]: Training Loss = 0.00002
	Epoch[134/200]: Training Loss = 0.00002
	Epoch[135/200]: Training Loss = 0.00002
	Epoch[136/200]: Training Loss = 0.00002
	Epoch[137/200]: Training Loss = 0.00002
	Epoch[138/200]: Training Loss = 0.00002
	Epoch[139/200]: Training Loss = 0.00002
	Epoch[140/200]: Training Loss = 0.00002
	Epoch[141/200]: Training Loss = 0.00002
	Epoch[142/200]: Training Loss = 0.00002
	Epoch[143/200]: Training Loss = 0.00002
	Epoch[144/200]: Training Loss = 0.00002
	Epoch[145/200]: Training Loss = 0.00002
	Epoch[146/200]: Training Loss = 0.00003
	Epoch[147/200]: Training Loss = 0.00002
	Epoch[148/200]: Training Loss = 0.00002
	Epoch[149/200]: Training Loss = 0.00002
	Epoch[150/200]: Training Loss = 0.00003
	Epoch[151/200]: Training Loss = 0.00004
	Epoch[152/200]: Training Loss = 0.00004
	Epoch[153/200]: Training Loss = 0.00003
	Epoch[154/200]: Training Loss = 0.00002
	Epoch[155/200]: Training Loss = 0.00002
	Epoch[156/200]: Training Loss = 0.00002
	Epoch[157/200]: Training Loss = 0.00002
	Epoch[158/200]: Training Loss = 0.00003
	Epoch[159/200]: Training Loss = 0.00002
	Epoch[160/200]: Training Loss = 0.00002
	Epoch[161/200]: Training Loss = 0.00002
	Epoch[162/200]: Training Loss = 0.00002
	Epoch[163/200]: Training Loss = 0.00002
	Epoch[164/200]: Training Loss = 0.00002
	Epoch[165/200]: Training Loss = 0.00002
	Epoch[166/200]: Training Loss = 0.00002
	Epoch[167/200]: Training Loss = 0.00002
	Epoch[168/200]: Training Loss = 0.00002
	Epoch[169/200]: Training Loss = 0.00002
	Epoch[170/200]: Training Loss = 0.00002
	Epoch[171/200]: Training Loss = 0.00002
	Epoch[172/200]: Training Loss = 0.00002
	Epoch[173/200]: Training Loss = 0.00002
	Epoch[174/200]: Training Loss = 0.00002
	Epoch[175/200]: Training Loss = 0.00002
	Epoch[176/200]: Training Loss = 0.00002
	Epoch[177/200]: Training Loss = 0.00002
	Epoch[178/200]: Training Loss = 0.00001
	Epoch[179/200]: Training Loss = 0.00002
	Epoch[180/200]: Training Loss = 0.00002
	Epoch[181/200]: Training Loss = 0.00002
	Epoch[182/200]: Training Loss = 0.00002
	Epoch[183/200]: Training Loss = 0.00002
	Epoch[184/200]: Training Loss = 0.00001
	Epoch[185/200]: Training Loss = 0.00002
	Epoch[186/200]: Training Loss = 0.00001
	Epoch[187/200]: Training Loss = 0.00001
	Epoch[188/200]: Training Loss = 0.00001
	Epoch[189/200]: Training Loss = 0.00001
	Epoch[190/200]: Training Loss = 0.00001
	Epoch[191/200]: Training Loss = 0.00001
	Epoch[192/200]: Training Loss = 0.00002
	Epoch[193/200]: Training Loss = 0.00001
	Epoch[194/200]: Training Loss = 0.00001
	Epoch[195/200]: Training Loss = 0.00001
	Epoch[196/200]: Training Loss = 0.00001
	Epoch[197/200]: Training Loss = 0.00001
	Epoch[198/200]: Training Loss = 0.00001
	Epoch[199/200]: Training Loss = 0.00001
	Epoch[200/200]: Training Loss = 0.00001
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009967120364308357
	mu : 0.9900000095367432

***Testing Results***
==============================
Test Accuracy = 82.450 %
Test Error = 17.550 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.009999999776482582, 0.009993048384785652, 0.009991301223635674, 0.009990930557250977, 0.009990442544221878, 0.00998910516500473, 0.009988673031330109, 0.009987721219658852, 0.009986919350922108, 0.009985680691897869, 0.009984360076487064, 0.009983401745557785, 0.009982493706047535, 0.00998139102011919, 0.009980208240449429, 0.009979247115552425, 0.009978380054235458, 0.009977318346500397, 0.009976265951991081, 0.009975289925932884, 0.009974585846066475, 0.00997353158891201, 0.009973099455237389, 0.009972555562853813, 0.009972081519663334, 0.009971560910344124, 0.009971094317734241, 0.009970759972929955, 0.009970445185899734, 0.009970025159418583, 0.009969591163098812, 0.00996934249997139, 0.009968902915716171, 0.00996862817555666, 0.009968266822397709, 0.009967891499400139, 0.009967714548110962, 0.009967450983822346, 0.00996728241443634, 0.009967192076146603, 0.009967155754566193, 0.009967174381017685, 0.009967174381017685, 0.009967146441340446, 0.009967136196792126, 0.00996713899075985, 0.009967138059437275, 0.00996713899075985, 0.0099671371281147, 0.0099671371281147, 0.0099671371281147, 0.0099671371281147, 0.0099671371281147, 0.0099671371281147, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967124089598656, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.009967123158276081, 0.00996712502092123, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357, 0.009967120364308357]
mu: [0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432]
Loss: [2.4253193006134035, 1.6854882997131349, 1.2688554786491395, 1.0385183433151246, 0.8913711465644837, 0.7662026498031617, 0.6870571788406372, 0.6139804319763184, 0.5459806045436859, 0.4717635656738281, 0.4169666385650635, 0.38202926038742063, 0.3381062009143829, 0.2927885570907593, 0.26045039360046385, 0.22418677161693573, 0.20511510779857636, 0.17068301375865935, 0.14419561578750612, 0.12726589528799057, 0.11778482875823974, 0.11351352809071541, 0.09912556862831115, 0.0877126024389267, 0.07384619438648224, 0.06756962476432324, 0.05302696232438087, 0.039433882825374605, 0.037788643014431, 0.039135251677036284, 0.037587892743349074, 0.038005019969940185, 0.0385114061665535, 0.04540272322535515, 0.052268660902976986, 0.04753832356929779, 0.038801019670963284, 0.029141085511446, 0.019965940771102905, 0.013229364766180515, 0.00874707230091095, 0.005977080464884639, 0.004613116495609283, 0.003641050452142954, 0.0024804969454556704, 0.0016551827156543733, 0.001357661510668695, 0.0010646843006834389, 0.0006092028721421957, 0.00037653071723878386, 0.0002319363701529801, 0.00018440816382877528, 0.00014980791967594996, 0.00015169951466843486, 0.0005145927951578051, 0.0003718799570016563, 0.0002377900105714798, 0.00018129133301787078, 0.00015105403611436486, 0.0001216487264353782, 0.00010831659001298249, 9.122467396780849e-05, 8.024260448291898e-05, 8.686632207594812e-05, 0.00011554709668736905, 0.00013534825425595045, 0.00013015246519818902, 0.00011086209777742625, 8.128583751851693e-05, 7.548475103452802e-05, 6.978204793296755e-05, 7.042335219681263e-05, 6.924430221668445e-05, 5.3219550759531555e-05, 4.9900913331657645e-05, 5.5542834135703744e-05, 4.82380638923496e-05, 4.5373862725682554e-05, 4.6871163814794275e-05, 4.540611943928525e-05, 4.383238497190177e-05, 4.3611508412868717e-05, 4.4584091105498375e-05, 3.987672348099295e-05, 3.804848940111697e-05, 3.668696539243683e-05, 3.5152087695896624e-05, 3.86082818871364e-05, 3.2576671987189914e-05, 3.330104048829526e-05, 4.0482738588470965e-05, 3.381587014184333e-05, 3.918169225566089e-05, 3.236138557316735e-05, 3.0267142932862042e-05, 3.6514459423488005e-05, 3.24935141694732e-05, 3.2003984306938946e-05, 3.202818893361837e-05, 3.125274165446171e-05, 3.108783464645967e-05, 2.8799248081631958e-05, 2.9161694122012703e-05, 2.8250037848483773e-05, 2.9668707351665944e-05, 2.960999235045165e-05, 2.882010926725343e-05, 2.4250870021642186e-05, 2.6722382778534664e-05, 2.3890342977829277e-05, 3.237688017077744e-05, 9.431245813146233e-05, 0.0001191734206979163, 0.00010642797177191824, 6.81920984154567e-05, 4.910309902857989e-05, 4.181651367805898e-05, 3.331331585533917e-05, 3.662170724011958e-05, 2.6816030141199007e-05, 3.292588284937665e-05, 2.8426262603607027e-05, 2.9039390049874782e-05, 2.713959490414709e-05, 2.7179937532637267e-05, 2.5405329389031975e-05, 2.363633149303496e-05, 2.5449426919221877e-05, 2.5671210484579204e-05, 2.5020170807256364e-05, 2.3332490161992608e-05, 2.349836215376854e-05, 2.4336086096591315e-05, 2.271622597705573e-05, 2.1560846015345304e-05, 2.3722443918231873e-05, 2.2785809899214655e-05, 2.2233894054661504e-05, 2.3813125491142272e-05, 2.3253749755676837e-05, 1.874883371172473e-05, 2.3690859815105797e-05, 1.9533354791929015e-05, 1.9622941422276197e-05, 1.9639931223355232e-05, 1.9201797160785645e-05, 2.7426731720915997e-05, 2.0729672082234173e-05, 2.1957475983072072e-05, 2.002277955878526e-05, 2.936351023381576e-05, 3.851192655507475e-05, 4.1235227251891046e-05, 2.6275989839341492e-05, 2.4258497085538693e-05, 2.3138128091813995e-05, 2.1685768549796195e-05, 2.1470023442525417e-05, 2.6535981889464893e-05, 2.01525614480488e-05, 1.90507567464374e-05, 1.9277535760775207e-05, 1.7723650203552098e-05, 1.8673319700174034e-05, 1.596223366097547e-05, 1.7026811563409865e-05, 1.696933986939257e-05, 1.6752843045396732e-05, 1.7175173286814244e-05, 1.6542492739390582e-05, 1.613597800838761e-05, 1.660693317709956e-05, 1.6645965478383006e-05, 1.6773572638630867e-05, 1.530535218655132e-05, 1.5408471670234576e-05, 1.5317975737852975e-05, 1.6669232258573176e-05, 1.3957181115983985e-05, 1.680459511349909e-05, 1.5965873384848236e-05, 1.711421382147819e-05, 1.5140911660273559e-05, 1.5374423851026222e-05, 1.4089678473828826e-05, 1.5188817917369305e-05, 1.4593563235830516e-05, 1.3474598354659975e-05, 1.3603607994737103e-05, 1.3717132402816788e-05, 1.2594479969702661e-05, 1.3272693177859765e-05, 1.5170101452386006e-05, 1.3414670739439316e-05, 1.3668082939111628e-05, 1.2410472116898745e-05, 1.3552963751135395e-05, 1.2258829930797219e-05, 1.2222162777616177e-05, 1.3313808861421421e-05, 1.3383925063535571e-05]
