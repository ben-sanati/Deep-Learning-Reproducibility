Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.9}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.01
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.51964
	Epoch[2/200]: Training Loss = 1.06369
	Epoch[3/200]: Training Loss = 0.85943
	Epoch[4/200]: Training Loss = 0.72388
	Epoch[5/200]: Training Loss = 0.62933
	Epoch[6/200]: Training Loss = 0.56028
	Epoch[7/200]: Training Loss = 0.49914
	Epoch[8/200]: Training Loss = 0.44233
	Epoch[9/200]: Training Loss = 0.40309
	Epoch[10/200]: Training Loss = 0.35919
	Epoch[11/200]: Training Loss = 0.32090
	Epoch[12/200]: Training Loss = 0.28646
	Epoch[13/200]: Training Loss = 0.25291
	Epoch[14/200]: Training Loss = 0.21893
	Epoch[15/200]: Training Loss = 0.18920
	Epoch[16/200]: Training Loss = 0.16028
	Epoch[17/200]: Training Loss = 0.13550
	Epoch[18/200]: Training Loss = 0.13216
	Epoch[19/200]: Training Loss = 0.11758
	Epoch[20/200]: Training Loss = 0.09097
	Epoch[21/200]: Training Loss = 0.08505
	Epoch[22/200]: Training Loss = 0.07171
	Epoch[23/200]: Training Loss = 0.05982
	Epoch[24/200]: Training Loss = 0.05981
	Epoch[25/200]: Training Loss = 0.05594
	Epoch[26/200]: Training Loss = 0.04533
	Epoch[27/200]: Training Loss = 0.03692
	Epoch[28/200]: Training Loss = 0.03741
	Epoch[29/200]: Training Loss = 0.04647
	Epoch[30/200]: Training Loss = 0.02958
	Epoch[31/200]: Training Loss = 0.01517
	Epoch[32/200]: Training Loss = 0.00618
	Epoch[33/200]: Training Loss = 0.00245
	Epoch[34/200]: Training Loss = 0.00145
	Epoch[35/200]: Training Loss = 0.00119
	Epoch[36/200]: Training Loss = 0.00109
	Epoch[37/200]: Training Loss = 0.00090
	Epoch[38/200]: Training Loss = 0.00076
	Epoch[39/200]: Training Loss = 0.00077
	Epoch[40/200]: Training Loss = 0.00069
	Epoch[41/200]: Training Loss = 0.00075
	Epoch[42/200]: Training Loss = 0.00060
	Epoch[43/200]: Training Loss = 0.00054
	Epoch[44/200]: Training Loss = 0.00054
	Epoch[45/200]: Training Loss = 0.00048
	Epoch[46/200]: Training Loss = 0.00046
	Epoch[47/200]: Training Loss = 0.00045
	Epoch[48/200]: Training Loss = 0.00046
	Epoch[49/200]: Training Loss = 0.00044
	Epoch[50/200]: Training Loss = 0.00040
	Epoch[51/200]: Training Loss = 0.00039
	Epoch[52/200]: Training Loss = 0.00038
	Epoch[53/200]: Training Loss = 0.00035
	Epoch[54/200]: Training Loss = 0.00035
	Epoch[55/200]: Training Loss = 0.00033
	Epoch[56/200]: Training Loss = 0.00032
	Epoch[57/200]: Training Loss = 0.00031
	Epoch[58/200]: Training Loss = 0.00030
	Epoch[59/200]: Training Loss = 0.00030
	Epoch[60/200]: Training Loss = 0.00030
	Epoch[61/200]: Training Loss = 0.00029
	Epoch[62/200]: Training Loss = 0.00027
	Epoch[63/200]: Training Loss = 0.00028
	Epoch[64/200]: Training Loss = 0.00026
	Epoch[65/200]: Training Loss = 0.00025
	Epoch[66/200]: Training Loss = 0.00024
	Epoch[67/200]: Training Loss = 0.00025
	Epoch[68/200]: Training Loss = 0.00025
	Epoch[69/200]: Training Loss = 0.00023
	Epoch[70/200]: Training Loss = 0.00024
	Epoch[71/200]: Training Loss = 0.00023
	Epoch[72/200]: Training Loss = 0.00022
	Epoch[73/200]: Training Loss = 0.00023
	Epoch[74/200]: Training Loss = 0.00026
	Epoch[75/200]: Training Loss = 0.00022
	Epoch[76/200]: Training Loss = 0.00020
	Epoch[77/200]: Training Loss = 0.00019
	Epoch[78/200]: Training Loss = 0.00021
	Epoch[79/200]: Training Loss = 0.00020
	Epoch[80/200]: Training Loss = 0.00021
	Epoch[81/200]: Training Loss = 0.00021
	Epoch[82/200]: Training Loss = 0.00020
	Epoch[83/200]: Training Loss = 0.00020
	Epoch[84/200]: Training Loss = 0.00018
	Epoch[85/200]: Training Loss = 0.00018
	Epoch[86/200]: Training Loss = 0.00018
	Epoch[87/200]: Training Loss = 0.00017
	Epoch[88/200]: Training Loss = 0.00017
	Epoch[89/200]: Training Loss = 0.00017
	Epoch[90/200]: Training Loss = 0.00019
	Epoch[91/200]: Training Loss = 0.00018
	Epoch[92/200]: Training Loss = 0.00018
	Epoch[93/200]: Training Loss = 0.00016
	Epoch[94/200]: Training Loss = 0.00015
	Epoch[95/200]: Training Loss = 0.00016
	Epoch[96/200]: Training Loss = 0.00016
	Epoch[97/200]: Training Loss = 0.00016
	Epoch[98/200]: Training Loss = 0.00015
	Epoch[99/200]: Training Loss = 0.00015
	Epoch[100/200]: Training Loss = 0.00016
	Epoch[101/200]: Training Loss = 0.00038
	Epoch[102/200]: Training Loss = 0.00020
	Epoch[103/200]: Training Loss = 0.00019
	Epoch[104/200]: Training Loss = 0.00016
	Epoch[105/200]: Training Loss = 0.00016
	Epoch[106/200]: Training Loss = 0.00015
	Epoch[107/200]: Training Loss = 0.00014
	Epoch[108/200]: Training Loss = 0.00015
	Epoch[109/200]: Training Loss = 0.00014
	Epoch[110/200]: Training Loss = 0.00014
	Epoch[111/200]: Training Loss = 0.00014
	Epoch[112/200]: Training Loss = 0.00013
	Epoch[113/200]: Training Loss = 0.00013
	Epoch[114/200]: Training Loss = 0.00013
	Epoch[115/200]: Training Loss = 0.00013
	Epoch[116/200]: Training Loss = 0.00014
	Epoch[117/200]: Training Loss = 0.00012
	Epoch[118/200]: Training Loss = 0.00012
	Epoch[119/200]: Training Loss = 0.00012
	Epoch[120/200]: Training Loss = 0.00011
	Epoch[121/200]: Training Loss = 0.00013
	Epoch[122/200]: Training Loss = 0.00013
	Epoch[123/200]: Training Loss = 0.00012
	Epoch[124/200]: Training Loss = 0.00013
	Epoch[125/200]: Training Loss = 0.00013
	Epoch[126/200]: Training Loss = 0.00015
	Epoch[127/200]: Training Loss = 0.00012
	Epoch[128/200]: Training Loss = 0.00012
	Epoch[129/200]: Training Loss = 0.00011
	Epoch[130/200]: Training Loss = 0.00011
	Epoch[131/200]: Training Loss = 0.00012
	Epoch[132/200]: Training Loss = 0.00011
	Epoch[133/200]: Training Loss = 0.00011
	Epoch[134/200]: Training Loss = 0.00011
	Epoch[135/200]: Training Loss = 0.00010
	Epoch[136/200]: Training Loss = 0.00010
	Epoch[137/200]: Training Loss = 0.00010
	Epoch[138/200]: Training Loss = 0.00010
	Epoch[139/200]: Training Loss = 0.00011
	Epoch[140/200]: Training Loss = 0.00009
	Epoch[141/200]: Training Loss = 0.00010
	Epoch[142/200]: Training Loss = 0.00010
	Epoch[143/200]: Training Loss = 0.00010
	Epoch[144/200]: Training Loss = 0.00010
	Epoch[145/200]: Training Loss = 0.00009
	Epoch[146/200]: Training Loss = 0.00009
	Epoch[147/200]: Training Loss = 0.00009
	Epoch[148/200]: Training Loss = 0.00009
	Epoch[149/200]: Training Loss = 0.00009
	Epoch[150/200]: Training Loss = 0.00009
	Epoch[151/200]: Training Loss = 0.00009
	Epoch[152/200]: Training Loss = 0.00009
	Epoch[153/200]: Training Loss = 0.00009
	Epoch[154/200]: Training Loss = 0.00008
	Epoch[155/200]: Training Loss = 0.00009
	Epoch[156/200]: Training Loss = 0.00010
	Epoch[157/200]: Training Loss = 0.00009
	Epoch[158/200]: Training Loss = 0.00009
	Epoch[159/200]: Training Loss = 0.00008
	Epoch[160/200]: Training Loss = 0.00008
	Epoch[161/200]: Training Loss = 0.00008
	Epoch[162/200]: Training Loss = 0.00009
	Epoch[163/200]: Training Loss = 0.00009
	Epoch[164/200]: Training Loss = 0.00008
	Epoch[165/200]: Training Loss = 0.00008
	Epoch[166/200]: Training Loss = 0.00008
	Epoch[167/200]: Training Loss = 0.00008
	Epoch[168/200]: Training Loss = 0.00009
	Epoch[169/200]: Training Loss = 0.00008
	Epoch[170/200]: Training Loss = 0.00008
	Epoch[171/200]: Training Loss = 0.00008
	Epoch[172/200]: Training Loss = 0.00008
	Epoch[173/200]: Training Loss = 0.00008
	Epoch[174/200]: Training Loss = 0.00007
	Epoch[175/200]: Training Loss = 0.00008
	Epoch[176/200]: Training Loss = 0.00007
	Epoch[177/200]: Training Loss = 0.00007
	Epoch[178/200]: Training Loss = 0.00007
	Epoch[179/200]: Training Loss = 0.00008
	Epoch[180/200]: Training Loss = 0.00008
	Epoch[181/200]: Training Loss = 0.00007
	Epoch[182/200]: Training Loss = 0.00008
	Epoch[183/200]: Training Loss = 0.00007
	Epoch[184/200]: Training Loss = 0.00007
	Epoch[185/200]: Training Loss = 0.00007
	Epoch[186/200]: Training Loss = 0.00007
	Epoch[187/200]: Training Loss = 0.00007
	Epoch[188/200]: Training Loss = 0.00007
	Epoch[189/200]: Training Loss = 0.00007
	Epoch[190/200]: Training Loss = 0.00007
	Epoch[191/200]: Training Loss = 0.00007
	Epoch[192/200]: Training Loss = 0.00007
	Epoch[193/200]: Training Loss = 0.00007
	Epoch[194/200]: Training Loss = 0.00007
	Epoch[195/200]: Training Loss = 0.00007
	Epoch[196/200]: Training Loss = 0.00006
	Epoch[197/200]: Training Loss = 0.00007
	Epoch[198/200]: Training Loss = 0.00006
	Epoch[199/200]: Training Loss = 0.00007
	Epoch[200/200]: Training Loss = 0.00006
***Training Complete***

Final Optimizer Parameters
	alpha : 0.00974123552441597
	mu : 0.8999999761581421

***Testing Results***
==============================
Test Accuracy = 81.880 %
Test Error = 18.120 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.009999999776482582, 0.009983772411942482, 0.009962505660951138, 0.00994457770138979, 0.009927146136760712, 0.009911668486893177, 0.009897703304886818, 0.009884904138743877, 0.009873541072010994, 0.009862654842436314, 0.009851471520960331, 0.009841084480285645, 0.00983203761279583, 0.009822800755500793, 0.00981301162391901, 0.009804659523069859, 0.009796560741961002, 0.009789081290364265, 0.009782614186406136, 0.0097760995849967, 0.009771526791155338, 0.009766495786607265, 0.009762306697666645, 0.009758858941495419, 0.009755795821547508, 0.009752619080245495, 0.009750103577971458, 0.009748164564371109, 0.009746293537318707, 0.009743125177919865, 0.009741833433508873, 0.009741296991705894, 0.009741227142512798, 0.009741217829287052, 0.009741218760609627, 0.009741218760609627, 0.009741218760609627, 0.009741218760609627, 0.009741218760609627, 0.009741218760609627, 0.009741219691932201, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741222485899925, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.009741220623254776, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597, 0.00974123552441597]
mu: [0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421]
Loss: [2.4427517668151855, 1.5196426261901856, 1.0636886778068542, 0.8594298608207702, 0.7238765438079834, 0.6293333979988098, 0.5602845476913452, 0.49913654741287233, 0.442325984954834, 0.40309096101760866, 0.3591865434741974, 0.3208965909385681, 0.28646241971969605, 0.25291138540267943, 0.21892730335712432, 0.18919580204963685, 0.16028264231204986, 0.13550383063316346, 0.13216189772605896, 0.11758204066276551, 0.09096606904029846, 0.08505155230760575, 0.07171353751778603, 0.05982384970664978, 0.05981223000526428, 0.055942683205604554, 0.045331369446516036, 0.03691799548625946, 0.03741040154933929, 0.04646540922164917, 0.029575132714509966, 0.015172370018363, 0.006179375742375851, 0.0024493060376495125, 0.0014467446620762348, 0.0011872215096279978, 0.0010949708469025791, 0.0008973547617346048, 0.0007634834440052509, 0.0007661659158580005, 0.0006910312747955322, 0.0007501407423987984, 0.0006047259842976928, 0.0005403499620035291, 0.0005375953403487802, 0.0004840956785529852, 0.00045932829468511046, 0.0004517844184488058, 0.00046299618135206403, 0.00043794190253131093, 0.0004012642630562186, 0.0003917296016216278, 0.0003770853585749865, 0.00035282901760190727, 0.00035350014520809053, 0.000326202329993248, 0.0003227603195980191, 0.0003072483192011714, 0.00029666696965694427, 0.0002966387868672609, 0.0003026624637655914, 0.00028807737845927474, 0.0002714312968030572, 0.0002796027627773583, 0.0002559457628056407, 0.00024984382171183825, 0.00024397139348089695, 0.00024984412886202336, 0.00025370260912925004, 0.00023137660697102546, 0.00023878412746824323, 0.00022559820400550962, 0.00022028340784832834, 0.00022843058060854673, 0.00025792212782427666, 0.00022030199185945094, 0.00020447450458072126, 0.00019131506517529488, 0.00021190956952050327, 0.00019643713641911745, 0.00020723573139868676, 0.00020726860188879072, 0.00019872567642480136, 0.0001986603389494121, 0.00018027020609006286, 0.00018320520024746656, 0.0001773425893858075, 0.00017087623301893472, 0.00017416003119200468, 0.0001728585885837674, 0.00018509815516881646, 0.00018049978505820037, 0.0001765290684532374, 0.00016226993568241598, 0.00015012639943510293, 0.00015769357465207577, 0.00015622252131346613, 0.00015569058060180396, 0.00015365520316176117, 0.00015280168145895005, 0.0001621341929025948, 0.00037728438997641203, 0.0001958327821455896, 0.00018602510765194894, 0.00016329482594504952, 0.0001579693097283598, 0.0001463653266429901, 0.0001404005447309464, 0.0001512380721606314, 0.00014156937127932906, 0.0001428929304331541, 0.0001359608925320208, 0.00013392765852622687, 0.00013428136991336942, 0.00012600194914266468, 0.00013293310912325978, 0.0001416567412391305, 0.00012363897828385233, 0.00012424786679446698, 0.00012178968457505107, 0.00011421036422252655, 0.0001309045103378594, 0.0001299621850065887, 0.00012174103658646344, 0.0001271144242025912, 0.00012958201598376037, 0.00014800223799189553, 0.0001240082055516541, 0.00011647523885592819, 0.00011284213101491332, 0.00010865022805985064, 0.00011585975497961045, 0.00010919394731521607, 0.00011246567785274238, 0.0001058305715676397, 0.00010059588022530079, 0.00010090020408853889, 9.995497642550618e-05, 9.866374608129263e-05, 0.00010506060967221857, 9.432487046113237e-05, 9.91765314526856e-05, 9.694821068784222e-05, 9.582711329450831e-05, 9.677567522972822e-05, 9.066660180222244e-05, 8.877429572399706e-05, 9.457688335329294e-05, 8.68422214128077e-05, 9.084949351847172e-05, 8.967934725806117e-05, 8.841556431725621e-05, 8.873194814659656e-05, 8.787554106675089e-05, 8.210155986249446e-05, 8.923190934583545e-05, 9.950442999601364e-05, 8.933352426625788e-05, 9.01171575114131e-05, 8.17858709488064e-05, 8.183669770136476e-05, 8.320530144497753e-05, 8.873341457918287e-05, 9.211501374840737e-05, 8.429029835388064e-05, 7.578943325206637e-05, 8.104323932901025e-05, 7.552176769124344e-05, 8.606206210330129e-05, 8.095197397284209e-05, 7.6224967520684e-05, 7.615177784115077e-05, 7.804822340141982e-05, 7.748462037183345e-05, 7.293026512954384e-05, 7.693308628164232e-05, 7.286265393719077e-05, 7.46595378452912e-05, 7.224529610481113e-05, 7.63675057888031e-05, 7.543444504030049e-05, 6.957001563976519e-05, 7.592126061907039e-05, 7.440759407356381e-05, 7.266509734559804e-05, 7.455219763331115e-05, 7.222901310771704e-05, 6.928075522184372e-05, 7.275991443078965e-05, 7.173711450770498e-05, 6.681513295741751e-05, 6.531551799736917e-05, 6.57313062576577e-05, 6.87042178073898e-05, 6.64727261569351e-05, 7.338049514219164e-05, 6.352952842600644e-05, 6.615377763286233e-05, 6.344654828077182e-05, 6.597913548350334e-05, 6.490187062881887e-05]
