Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.09}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 0.01
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 1.84483
	Epoch[2/200]: Training Loss = 1.52323
	Epoch[3/200]: Training Loss = 1.37668
	Epoch[4/200]: Training Loss = 1.26083
	Epoch[5/200]: Training Loss = 1.15526
	Epoch[6/200]: Training Loss = 1.07297
	Epoch[7/200]: Training Loss = 1.00662
	Epoch[8/200]: Training Loss = 0.94838
	Epoch[9/200]: Training Loss = 0.90359
	Epoch[10/200]: Training Loss = 0.85815
	Epoch[11/200]: Training Loss = 0.82081
	Epoch[12/200]: Training Loss = 0.78553
	Epoch[13/200]: Training Loss = 0.75571
	Epoch[14/200]: Training Loss = 0.72626
	Epoch[15/200]: Training Loss = 0.69510
	Epoch[16/200]: Training Loss = 0.66430
	Epoch[17/200]: Training Loss = 0.63873
	Epoch[18/200]: Training Loss = 0.60794
	Epoch[19/200]: Training Loss = 0.58311
	Epoch[20/200]: Training Loss = 0.55623
	Epoch[21/200]: Training Loss = 0.53255
	Epoch[22/200]: Training Loss = 0.50737
	Epoch[23/200]: Training Loss = 0.48171
	Epoch[24/200]: Training Loss = 0.46242
	Epoch[25/200]: Training Loss = 0.43406
	Epoch[26/200]: Training Loss = 0.41473
	Epoch[27/200]: Training Loss = 0.38976
	Epoch[28/200]: Training Loss = 0.36708
	Epoch[29/200]: Training Loss = 0.34657
	Epoch[30/200]: Training Loss = 0.32280
	Epoch[31/200]: Training Loss = 0.30047
	Epoch[32/200]: Training Loss = 0.28187
	Epoch[33/200]: Training Loss = 0.25994
	Epoch[34/200]: Training Loss = 0.23632
	Epoch[35/200]: Training Loss = 0.21924
	Epoch[36/200]: Training Loss = 0.20329
	Epoch[37/200]: Training Loss = 0.18366
	Epoch[38/200]: Training Loss = 0.16801
	Epoch[39/200]: Training Loss = 0.15978
	Epoch[40/200]: Training Loss = 0.13840
	Epoch[41/200]: Training Loss = 0.12662
	Epoch[42/200]: Training Loss = 0.11615
	Epoch[43/200]: Training Loss = 0.10689
	Epoch[44/200]: Training Loss = 0.09365
	Epoch[45/200]: Training Loss = 0.08295
	Epoch[46/200]: Training Loss = 0.07666
	Epoch[47/200]: Training Loss = 0.06992
	Epoch[48/200]: Training Loss = 0.06325
	Epoch[49/200]: Training Loss = 0.06013
	Epoch[50/200]: Training Loss = 0.05575
	Epoch[51/200]: Training Loss = 0.05043
	Epoch[52/200]: Training Loss = 0.04699
	Epoch[53/200]: Training Loss = 0.04231
	Epoch[54/200]: Training Loss = 0.03955
	Epoch[55/200]: Training Loss = 0.03579
	Epoch[56/200]: Training Loss = 0.03660
	Epoch[57/200]: Training Loss = 0.03352
	Epoch[58/200]: Training Loss = 0.03070
	Epoch[59/200]: Training Loss = 0.02757
	Epoch[60/200]: Training Loss = 0.02659
	Epoch[61/200]: Training Loss = 0.02482
	Epoch[62/200]: Training Loss = 0.02359
	Epoch[63/200]: Training Loss = 0.02298
	Epoch[64/200]: Training Loss = 0.02216
	Epoch[65/200]: Training Loss = 0.02025
	Epoch[66/200]: Training Loss = 0.01961
	Epoch[67/200]: Training Loss = 0.01794
	Epoch[68/200]: Training Loss = 0.01832
	Epoch[69/200]: Training Loss = 0.01736
	Epoch[70/200]: Training Loss = 0.01641
	Epoch[71/200]: Training Loss = 0.01604
	Epoch[72/200]: Training Loss = 0.01556
	Epoch[73/200]: Training Loss = 0.01534
	Epoch[74/200]: Training Loss = 0.01447
	Epoch[75/200]: Training Loss = 0.01370
	Epoch[76/200]: Training Loss = 0.01383
	Epoch[77/200]: Training Loss = 0.01323
	Epoch[78/200]: Training Loss = 0.01262
	Epoch[79/200]: Training Loss = 0.01248
	Epoch[80/200]: Training Loss = 0.01208
	Epoch[81/200]: Training Loss = 0.01174
	Epoch[82/200]: Training Loss = 0.01122
	Epoch[83/200]: Training Loss = 0.01091
	Epoch[84/200]: Training Loss = 0.01262
	Epoch[85/200]: Training Loss = 0.01668
	Epoch[86/200]: Training Loss = 0.01069
	Epoch[87/200]: Training Loss = 0.00976
	Epoch[88/200]: Training Loss = 0.00959
	Epoch[89/200]: Training Loss = 0.01026
	Epoch[90/200]: Training Loss = 0.00918
	Epoch[91/200]: Training Loss = 0.00989
	Epoch[92/200]: Training Loss = 0.00825
	Epoch[93/200]: Training Loss = 0.00958
	Epoch[94/200]: Training Loss = 0.00825
	Epoch[95/200]: Training Loss = 0.00895
	Epoch[96/200]: Training Loss = 0.00813
	Epoch[97/200]: Training Loss = 0.00753
	Epoch[98/200]: Training Loss = 0.00858
	Epoch[99/200]: Training Loss = 0.00747
	Epoch[100/200]: Training Loss = 0.00727
	Epoch[101/200]: Training Loss = 0.00689
	Epoch[102/200]: Training Loss = 0.00668
	Epoch[103/200]: Training Loss = 0.00765
	Epoch[104/200]: Training Loss = 0.00718
	Epoch[105/200]: Training Loss = 0.00928
	Epoch[106/200]: Training Loss = 0.00648
	Epoch[107/200]: Training Loss = 0.00946
	Epoch[108/200]: Training Loss = 0.00658
	Epoch[109/200]: Training Loss = 0.00802
	Epoch[110/200]: Training Loss = 0.00628
	Epoch[111/200]: Training Loss = 0.00634
	Epoch[112/200]: Training Loss = 0.00597
	Epoch[113/200]: Training Loss = 0.00685
	Epoch[114/200]: Training Loss = 0.00636
	Epoch[115/200]: Training Loss = 0.00583
	Epoch[116/200]: Training Loss = 0.00528
	Epoch[117/200]: Training Loss = 0.00551
	Epoch[118/200]: Training Loss = 0.00523
	Epoch[119/200]: Training Loss = 0.00531
	Epoch[120/200]: Training Loss = 0.00516
	Epoch[121/200]: Training Loss = 0.00533
	Epoch[122/200]: Training Loss = 0.00535
	Epoch[123/200]: Training Loss = 0.00641
	Epoch[124/200]: Training Loss = 0.00495
	Epoch[125/200]: Training Loss = 0.00642
	Epoch[126/200]: Training Loss = 0.00453
	Epoch[127/200]: Training Loss = 0.00493
	Epoch[128/200]: Training Loss = 0.00448
	Epoch[129/200]: Training Loss = 0.00479
	Epoch[130/200]: Training Loss = 0.00422
	Epoch[131/200]: Training Loss = 0.00479
	Epoch[132/200]: Training Loss = 0.00470
	Epoch[133/200]: Training Loss = 0.00438
	Epoch[134/200]: Training Loss = 0.00398
	Epoch[135/200]: Training Loss = 0.00416
	Epoch[136/200]: Training Loss = 0.00434
	Epoch[137/200]: Training Loss = 0.00413
	Epoch[138/200]: Training Loss = 0.00385
	Epoch[139/200]: Training Loss = 0.00431
	Epoch[140/200]: Training Loss = 0.00386
	Epoch[141/200]: Training Loss = 0.00379
	Epoch[142/200]: Training Loss = 0.00389
	Epoch[143/200]: Training Loss = 0.00487
	Epoch[144/200]: Training Loss = 0.00420
	Epoch[145/200]: Training Loss = 0.00587
	Epoch[146/200]: Training Loss = 0.00408
	Epoch[147/200]: Training Loss = 0.00398
	Epoch[148/200]: Training Loss = 0.00369
	Epoch[149/200]: Training Loss = 0.00390
	Epoch[150/200]: Training Loss = 0.00378
	Epoch[151/200]: Training Loss = 0.00349
	Epoch[152/200]: Training Loss = 0.00348
	Epoch[153/200]: Training Loss = 0.00369
	Epoch[154/200]: Training Loss = 0.00337
	Epoch[155/200]: Training Loss = 0.00367
	Epoch[156/200]: Training Loss = 0.00359
	Epoch[157/200]: Training Loss = 0.00331
	Epoch[158/200]: Training Loss = 0.00364
	Epoch[159/200]: Training Loss = 0.00299
	Epoch[160/200]: Training Loss = 0.00304
	Epoch[161/200]: Training Loss = 0.00323
	Epoch[162/200]: Training Loss = 0.00330
	Epoch[163/200]: Training Loss = 0.00300
	Epoch[164/200]: Training Loss = 0.00385
	Epoch[165/200]: Training Loss = 0.00339
	Epoch[166/200]: Training Loss = 0.00348
	Epoch[167/200]: Training Loss = 0.00305
	Epoch[168/200]: Training Loss = 0.00306
	Epoch[169/200]: Training Loss = 0.00321
	Epoch[170/200]: Training Loss = 0.00327
	Epoch[171/200]: Training Loss = 0.00369
	Epoch[172/200]: Training Loss = 0.00327
	Epoch[173/200]: Training Loss = 0.00298
	Epoch[174/200]: Training Loss = 0.00276
	Epoch[175/200]: Training Loss = 0.00283
	Epoch[176/200]: Training Loss = 0.00267
	Epoch[177/200]: Training Loss = 0.00263
	Epoch[178/200]: Training Loss = 0.00282
	Epoch[179/200]: Training Loss = 0.00407
	Epoch[180/200]: Training Loss = 0.00275
	Epoch[181/200]: Training Loss = 0.00258
	Epoch[182/200]: Training Loss = 0.00274
	Epoch[183/200]: Training Loss = 0.00320
	Epoch[184/200]: Training Loss = 0.00273
	Epoch[185/200]: Training Loss = 0.00271
	Epoch[186/200]: Training Loss = 0.00252
	Epoch[187/200]: Training Loss = 0.00271
	Epoch[188/200]: Training Loss = 0.00278
	Epoch[189/200]: Training Loss = 0.00245
	Epoch[190/200]: Training Loss = 0.00304
	Epoch[191/200]: Training Loss = 0.00253
	Epoch[192/200]: Training Loss = 0.00228
	Epoch[193/200]: Training Loss = 0.00321
	Epoch[194/200]: Training Loss = 0.00261
	Epoch[195/200]: Training Loss = 0.00262
	Epoch[196/200]: Training Loss = 0.00251
	Epoch[197/200]: Training Loss = 0.00233
	Epoch[198/200]: Training Loss = 0.00231
	Epoch[199/200]: Training Loss = 0.00217
	Epoch[200/200]: Training Loss = 0.00236
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0037705821450799704
	mu : 0.08999007940292358

***Testing Results***
==============================
Test Accuracy = 70.900 %
Test Error = 29.100 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [0.009999999776482582, 0.009993454441428185, 0.009927252307534218, 0.009829427115619183, 0.00970067735761404, 0.00958336889743805, 0.009464566595852375, 0.009351111017167568, 0.009236098267138004, 0.00911786314100027, 0.009009825997054577, 0.008896301500499249, 0.008787104859948158, 0.008667009882628918, 0.008547686971724033, 0.008428110741078854, 0.00830544251948595, 0.00817826483398676, 0.008053592406213284, 0.007925215177237988, 0.007792308460921049, 0.007659876719117165, 0.007522208150476217, 0.007385261356830597, 0.007229941431432962, 0.007086018566042185, 0.006921066902577877, 0.006757420487701893, 0.006585636176168919, 0.006421202793717384, 0.006265397183597088, 0.006098809652030468, 0.00592856016010046, 0.005770055111497641, 0.005627963691949844, 0.005481775384396315, 0.00533737288787961, 0.005211524665355682, 0.005095981527119875, 0.004950521979480982, 0.004849217366427183, 0.0047556678764522076, 0.004669208079576492, 0.00458746450021863, 0.0045229592360556126, 0.004474659916013479, 0.004419910721480846, 0.004376889672130346, 0.00433954456821084, 0.004303418565541506, 0.004269590601325035, 0.004241575952619314, 0.00421482790261507, 0.0041953264735639095, 0.0041784304194152355, 0.004162800498306751, 0.004134449176490307, 0.004114964045584202, 0.004100284073501825, 0.0040894667617976665, 0.004079009406268597, 0.00407260749489069, 0.004065733402967453, 0.004057794343680143, 0.004049908369779587, 0.004045119974762201, 0.0040398347191512585, 0.004036235623061657, 0.00403090612962842, 0.004026890732347965, 0.00402296194806695, 0.004019252490252256, 0.004016030114144087, 0.0040123723447322845, 0.004009530413895845, 0.004007338546216488, 0.004001999739557505, 0.003998401574790478, 0.003995520528405905, 0.0039923712611198425, 0.00398982223123312, 0.003987601492553949, 0.0039858329109847546, 0.003984010778367519, 0.003972877282649279, 0.00393768772482872, 0.003936090972274542, 0.003934989683330059, 0.003933890722692013, 0.0039296140894293785, 0.003928082063794136, 0.0039246357046067715, 0.003923824988305569, 0.0039219362661242485, 0.00392106082290411, 0.003919009119272232, 0.0039182063192129135, 0.003916402813047171, 0.003913526423275471, 0.003912871237844229, 0.00391219649463892, 0.003911616746336222, 0.003911120351403952, 0.003907417878508568, 0.0039059265982359648, 0.003890729509294033, 0.0038903856184333563, 0.0038632971700280905, 0.0038626089226454496, 0.0038574899081140757, 0.003856891067698598, 0.003856069641187787, 0.003855610266327858, 0.0038519639056175947, 0.003847221378237009, 0.003846741747111082, 0.0038464218378067017, 0.0038459606003016233, 0.003845527768135071, 0.003845024388283491, 0.003844564314931631, 0.003844299353659153, 0.003843127517029643, 0.003835211507976055, 0.003834756324067712, 0.003830190049484372, 0.0038299199659377337, 0.0038295777048915625, 0.003829491790384054, 0.003828805172815919, 0.003828589105978608, 0.0038281152956187725, 0.0038276081904768944, 0.0038274298422038555, 0.0038272866513580084, 0.003827087813988328, 0.0038264396134763956, 0.0038262379821389914, 0.0038260703440755606, 0.0038255543913692236, 0.00382545730099082, 0.003825229126960039, 0.003825167194008827, 0.0038198872935026884, 0.003819710109382868, 0.003809967776760459, 0.003809697227552533, 0.0038096008356660604, 0.0038094695191830397, 0.0038089663721621037, 0.0038087856955826283, 0.003808618988841772, 0.0038084944244474173, 0.003807702334597707, 0.0038076082710176706, 0.003806880908086896, 0.0038065139669924974, 0.0038063814863562584, 0.003805780317634344, 0.0038057223428040743, 0.0038056725170463324, 0.003805447369813919, 0.00380528112873435, 0.0038051886949688196, 0.003800865262746811, 0.0037998005282133818, 0.003799395402893424, 0.0037992422003299, 0.0037991756107658148, 0.0037981895729899406, 0.003797953948378563, 0.0037941194605082273, 0.0037940116599202156, 0.0037939241155982018, 0.00379382586106658, 0.0037936922162771225, 0.0037936249282211065, 0.003793589770793915, 0.0037934782449156046, 0.0037825056351721287, 0.0037824574392288923, 0.0037823987659066916, 0.0037823987659066916, 0.003778712125495076, 0.0037785698659718037, 0.0037784921005368233, 0.0037784494925290346, 0.003778396872803569, 0.0037782052531838417, 0.0037781493738293648, 0.0037775945384055376, 0.0037774965167045593, 0.003777473233640194, 0.0037712925113737583, 0.0037711826153099537, 0.003770852694287896, 0.003770752577111125, 0.0037707118317484856, 0.0037706512957811356, 0.0037706305738538504, 0.0037705821450799704]
mu: [0.09000000357627869, 0.09000000357627869, 0.09000008553266525, 0.0900001972913742, 0.09000031650066376, 0.09000014513731003, 0.0899999588727951, 0.08999966084957123, 0.08999942243099213, 0.08999914675951004, 0.08999884128570557, 0.08999857306480408, 0.0899982750415802, 0.08999805152416229, 0.0899977907538414, 0.08999747037887573, 0.08999717235565186, 0.08999690413475037, 0.08999659866094589, 0.08999627828598022, 0.08999596536159515, 0.0899956002831459, 0.08999519795179367, 0.08999478071928024, 0.08999446034431458, 0.08999413996934891, 0.08999381959438324, 0.08999351412057877, 0.08999317139387131, 0.08999288827180862, 0.0899924635887146, 0.08999219536781311, 0.08999177068471909, 0.08999146521091461, 0.0899912491440773, 0.08999110758304596, 0.089990995824337, 0.08999086171388626, 0.08999069780111313, 0.08999055624008179, 0.08999044448137283, 0.08999040722846985, 0.08999034762382507, 0.08999031782150269, 0.0899902954697609, 0.0899902954697609, 0.0899902805685997, 0.0899902805685997, 0.08999026566743851, 0.08999025076627731, 0.08999023586511612, 0.08999022841453552, 0.08999022841453552, 0.08999022841453552, 0.08999022841453552, 0.08999022841453552, 0.08999022096395493, 0.08999019861221313, 0.08999019861221313, 0.08999019861221313, 0.08999019861221313, 0.08999019861221313, 0.08999019861221313, 0.08999019116163254, 0.08999019116163254, 0.08999019116163254, 0.08999018371105194, 0.08999018371105194, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999017626047134, 0.08999016880989075, 0.08999016880989075, 0.08999016880989075, 0.08999016880989075, 0.08999016880989075, 0.08999016135931015, 0.08999016135931015, 0.08999015390872955, 0.08999015390872955, 0.08999014645814896, 0.08999014645814896, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999013900756836, 0.08999012410640717, 0.08999012410640717, 0.08999015390872955, 0.08999015390872955, 0.08999012410640717, 0.08999012410640717, 0.08999012410640717, 0.08999012410640717, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999011665582657, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999010920524597, 0.08999009430408478, 0.08999009430408478, 0.08999009430408478, 0.08999009430408478, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999008685350418, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358, 0.08999007940292358]
Loss: [2.4624168447113037, 1.8448256451416016, 1.5232262073516847, 1.3766761183166505, 1.2608325925445556, 1.1552639207077027, 1.072972668762207, 1.006622329788208, 0.9483838338088989, 0.9035897821044921, 0.858147223777771, 0.8208065217781066, 0.7855339596557617, 0.7557056562423706, 0.7262630919647217, 0.695101358795166, 0.6643022416687012, 0.638731202583313, 0.6079364327430725, 0.5831107167434693, 0.5562260978126526, 0.5325527165222168, 0.5073748571014405, 0.4817111880111694, 0.4624206299972534, 0.43405773072242737, 0.4147333012390137, 0.38975617453575134, 0.36707940465927125, 0.34656500787734984, 0.3227993083190918, 0.3004662642097473, 0.2818675574398041, 0.2599379008436203, 0.23632481652259826, 0.21924116355895995, 0.20329266520500183, 0.18365780444145202, 0.16801068900108337, 0.1597819387292862, 0.13839638828754425, 0.1266186499786377, 0.1161519794034958, 0.10689463922977448, 0.0936493905711174, 0.08294665033817292, 0.07665870217323303, 0.06991592168807984, 0.06325028176784515, 0.060132595767974854, 0.055749343986511234, 0.05043003440499306, 0.0469853190612793, 0.042310459928512574, 0.03954977765083313, 0.03578552891969681, 0.03660451812982559, 0.033523232891559604, 0.030701550607681274, 0.027567778548002242, 0.02659108741402626, 0.02481964930653572, 0.02358984457015991, 0.022983571330308913, 0.02215556563317776, 0.020251769812107086, 0.019613854157328605, 0.017936691814661027, 0.01832395907998085, 0.017364350053071975, 0.01641187104165554, 0.016042656602859497, 0.015558240032792091, 0.015343713688850402, 0.014474195899367333, 0.013704120696783065, 0.013828354749679566, 0.013233273775577545, 0.012615512298345566, 0.01247942718744278, 0.012078310878276825, 0.011737853816747666, 0.01121547510355711, 0.010907324767112732, 0.012618973941802978, 0.01668107513666153, 0.010689271019101143, 0.00975911091685295, 0.009585836683511734, 0.010261392677426338, 0.009182518784999847, 0.00988634666979313, 0.008251657061576843, 0.009579120462238789, 0.008251509158611298, 0.008945357443094253, 0.008131603503823281, 0.007534379423856735, 0.008577195399999618, 0.007466825092434883, 0.0072667061227560045, 0.006885079425573349, 0.00668113090634346, 0.00765156303524971, 0.0071761525678634645, 0.009283841096162796, 0.006475972292423248, 0.00945798247396946, 0.006577135858535766, 0.008022826179265976, 0.006283249773383141, 0.006336067662239075, 0.005970068102478981, 0.006846725281476974, 0.006362267023026943, 0.00582806775867939, 0.005280518952608108, 0.005512540836185217, 0.005230396808385849, 0.005307242837548256, 0.005157863435745239, 0.0053273031616210935, 0.00534509599328041, 0.006408460626602172, 0.004946721355915069, 0.006422753645777702, 0.004530679186284542, 0.004927631335407495, 0.004476287227869034, 0.004791780260801315, 0.004221727766394615, 0.004791107164919376, 0.004700514781326055, 0.0043840735989809036, 0.003982569295018911, 0.004161599465608597, 0.004341957379430533, 0.004127167292237281, 0.0038496178326010706, 0.004307124047428369, 0.0038619156718254087, 0.0037887624828517436, 0.0038891451811790467, 0.004872927306741476, 0.004198688598871231, 0.005865526675581932, 0.004081494614034891, 0.003984425231814384, 0.003688401448726654, 0.003904644309878349, 0.0037809725725650787, 0.0034930043037235737, 0.003477068028450012, 0.003694724268615246, 0.003373469733595848, 0.0036734577721357345, 0.0035937945249676704, 0.0033142168188095094, 0.003636640957891941, 0.0029877867248654364, 0.0030402428078651427, 0.0032339814656972884, 0.0033038576517999173, 0.0030000569263100626, 0.0038496751457452773, 0.0033903055137395857, 0.0034803261837363245, 0.003050640625357628, 0.003058923801779747, 0.0032056950372457505, 0.003272558275461197, 0.003692463437616825, 0.0032683074063062667, 0.0029785976941138505, 0.002755671104490757, 0.002832982564270496, 0.0026684007983654736, 0.002630105614066124, 0.0028180705481767655, 0.004074804085940123, 0.002753688059002161, 0.0025812000009417533, 0.0027360932326316832, 0.0032041849541664122, 0.0027264602613449097, 0.0027059403201937678, 0.0025224091684818268, 0.002705270648598671, 0.0027775018571317197, 0.0024486657667160032, 0.00304079985961318, 0.0025292315098643303, 0.0022800859022140503, 0.0032090179327130316, 0.002611911016702652, 0.002624558837413788, 0.002514333545714617, 0.002334401387423277, 0.002305373673737049, 0.0021652166399359703, 0.002358787995874882]
