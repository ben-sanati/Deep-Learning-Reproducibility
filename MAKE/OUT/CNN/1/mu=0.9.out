Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.9}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 1.0
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 2.20364
	Epoch[2/200]: Training Loss = 1.77112
	Epoch[3/200]: Training Loss = 1.59792
	Epoch[4/200]: Training Loss = 1.45695
	Epoch[5/200]: Training Loss = 1.28255
	Epoch[6/200]: Training Loss = 1.13912
	Epoch[7/200]: Training Loss = 1.01628
	Epoch[8/200]: Training Loss = 0.92411
	Epoch[9/200]: Training Loss = 0.84439
	Epoch[10/200]: Training Loss = 0.76137
	Epoch[11/200]: Training Loss = 0.71110
	Epoch[12/200]: Training Loss = 0.64640
	Epoch[13/200]: Training Loss = 0.59881
	Epoch[14/200]: Training Loss = 0.55446
	Epoch[15/200]: Training Loss = 0.51631
	Epoch[16/200]: Training Loss = 0.48203
	Epoch[17/200]: Training Loss = 0.45018
	Epoch[18/200]: Training Loss = 0.41744
	Epoch[19/200]: Training Loss = 0.38632
	Epoch[20/200]: Training Loss = 0.35703
	Epoch[21/200]: Training Loss = 0.33172
	Epoch[22/200]: Training Loss = 0.30484
	Epoch[23/200]: Training Loss = 0.28643
	Epoch[24/200]: Training Loss = 0.26241
	Epoch[25/200]: Training Loss = 0.23586
	Epoch[26/200]: Training Loss = 0.21438
	Epoch[27/200]: Training Loss = 0.19356
	Epoch[28/200]: Training Loss = 0.17719
	Epoch[29/200]: Training Loss = 0.15968
	Epoch[30/200]: Training Loss = 0.15011
	Epoch[31/200]: Training Loss = 0.12872
	Epoch[32/200]: Training Loss = 0.11780
	Epoch[33/200]: Training Loss = 0.10499
	Epoch[34/200]: Training Loss = 0.09169
	Epoch[35/200]: Training Loss = 0.08499
	Epoch[36/200]: Training Loss = 0.08443
	Epoch[37/200]: Training Loss = 0.06570
	Epoch[38/200]: Training Loss = 0.06936
	Epoch[39/200]: Training Loss = 0.05515
	Epoch[40/200]: Training Loss = 0.06226
	Epoch[41/200]: Training Loss = 0.04316
	Epoch[42/200]: Training Loss = 0.04854
	Epoch[43/200]: Training Loss = 0.03624
	Epoch[44/200]: Training Loss = 0.04432
	Epoch[45/200]: Training Loss = 0.04068
	Epoch[46/200]: Training Loss = 0.05426
	Epoch[47/200]: Training Loss = 0.02993
	Epoch[48/200]: Training Loss = 0.02005
	Epoch[49/200]: Training Loss = 0.01684
	Epoch[50/200]: Training Loss = 0.00844
	Epoch[51/200]: Training Loss = 0.00535
	Epoch[52/200]: Training Loss = 0.00219
	Epoch[53/200]: Training Loss = 0.00140
	Epoch[54/200]: Training Loss = 0.00123
	Epoch[55/200]: Training Loss = 0.00087
	Epoch[56/200]: Training Loss = 0.00094
	Epoch[57/200]: Training Loss = 0.00076
	Epoch[58/200]: Training Loss = 0.00424
	Epoch[59/200]: Training Loss = 0.00181
	Epoch[60/200]: Training Loss = 0.00132
	Epoch[61/200]: Training Loss = 0.00088
	Epoch[62/200]: Training Loss = 0.00060
	Epoch[63/200]: Training Loss = 0.00071
	Epoch[64/200]: Training Loss = 0.00060
	Epoch[65/200]: Training Loss = 0.00044
	Epoch[66/200]: Training Loss = 0.00064
	Epoch[67/200]: Training Loss = 0.00142
	Epoch[68/200]: Training Loss = 0.00091
	Epoch[69/200]: Training Loss = 0.00055
	Epoch[70/200]: Training Loss = 0.00049
	Epoch[71/200]: Training Loss = 0.00133
	Epoch[72/200]: Training Loss = 0.00123
	Epoch[73/200]: Training Loss = 0.03172
	Epoch[74/200]: Training Loss = 0.11211
	Epoch[75/200]: Training Loss = 0.04221
	Epoch[76/200]: Training Loss = 0.02177
	Epoch[77/200]: Training Loss = 0.00999
	Epoch[78/200]: Training Loss = 0.00401
	Epoch[79/200]: Training Loss = 0.00270
	Epoch[80/200]: Training Loss = 0.00125
	Epoch[81/200]: Training Loss = 0.00096
	Epoch[82/200]: Training Loss = 0.00087
	Epoch[83/200]: Training Loss = 0.00073
	Epoch[84/200]: Training Loss = 0.00064
	Epoch[85/200]: Training Loss = 0.00067
	Epoch[86/200]: Training Loss = 0.00047
	Epoch[87/200]: Training Loss = 0.00065
	Epoch[88/200]: Training Loss = 0.00040
	Epoch[89/200]: Training Loss = 0.00035
	Epoch[90/200]: Training Loss = 0.00040
	Epoch[91/200]: Training Loss = 0.00078
	Epoch[92/200]: Training Loss = 0.00037
	Epoch[93/200]: Training Loss = 0.00033
	Epoch[94/200]: Training Loss = 0.00034
	Epoch[95/200]: Training Loss = 0.00028
	Epoch[96/200]: Training Loss = 0.00031
	Epoch[97/200]: Training Loss = 0.00138
	Epoch[98/200]: Training Loss = 0.00061
	Epoch[99/200]: Training Loss = 0.00828
	Epoch[100/200]: Training Loss = 0.00357
	Epoch[101/200]: Training Loss = 0.00203
	Epoch[102/200]: Training Loss = 0.03248
	Epoch[103/200]: Training Loss = 0.04143
	Epoch[104/200]: Training Loss = 0.02615
	Epoch[105/200]: Training Loss = 0.02390
	Epoch[106/200]: Training Loss = 0.01636
	Epoch[107/200]: Training Loss = 0.01324
	Epoch[108/200]: Training Loss = 0.00632
	Epoch[109/200]: Training Loss = 0.00267
	Epoch[110/200]: Training Loss = 0.00190
	Epoch[111/200]: Training Loss = 0.00121
	Epoch[112/200]: Training Loss = 0.00083
	Epoch[113/200]: Training Loss = 0.00138
	Epoch[114/200]: Training Loss = 0.00066
	Epoch[115/200]: Training Loss = 0.00050
	Epoch[116/200]: Training Loss = 0.00035
	Epoch[117/200]: Training Loss = 0.00034
	Epoch[118/200]: Training Loss = 0.00032
	Epoch[119/200]: Training Loss = 0.00026
	Epoch[120/200]: Training Loss = 0.00022
	Epoch[121/200]: Training Loss = 0.00022
	Epoch[122/200]: Training Loss = 0.00021
	Epoch[123/200]: Training Loss = 0.00020
	Epoch[124/200]: Training Loss = 0.00018
	Epoch[125/200]: Training Loss = 0.00021
	Epoch[126/200]: Training Loss = 0.00019
	Epoch[127/200]: Training Loss = 0.00019
	Epoch[128/200]: Training Loss = 0.00026
	Epoch[129/200]: Training Loss = 0.00303
	Epoch[130/200]: Training Loss = 0.00097
	Epoch[131/200]: Training Loss = 0.01044
	Epoch[132/200]: Training Loss = 0.01875
	Epoch[133/200]: Training Loss = 0.03155
	Epoch[134/200]: Training Loss = 0.04436
	Epoch[135/200]: Training Loss = 0.01160
	Epoch[136/200]: Training Loss = 0.00677
	Epoch[137/200]: Training Loss = 0.00225
	Epoch[138/200]: Training Loss = 0.00142
	Epoch[139/200]: Training Loss = 0.00119
	Epoch[140/200]: Training Loss = 0.00115
	Epoch[141/200]: Training Loss = 0.00088
	Epoch[142/200]: Training Loss = 0.00066
	Epoch[143/200]: Training Loss = 0.00304
	Epoch[144/200]: Training Loss = 0.00065
	Epoch[145/200]: Training Loss = 0.00047
	Epoch[146/200]: Training Loss = 0.00033
	Epoch[147/200]: Training Loss = 0.00034
	Epoch[148/200]: Training Loss = 0.00059
	Epoch[149/200]: Training Loss = 0.00025
	Epoch[150/200]: Training Loss = 0.00021
	Epoch[151/200]: Training Loss = 0.00036
	Epoch[152/200]: Training Loss = 0.00030
	Epoch[153/200]: Training Loss = 0.00021
	Epoch[154/200]: Training Loss = 0.00021
	Epoch[155/200]: Training Loss = 0.00015
	Epoch[156/200]: Training Loss = 0.00018
	Epoch[157/200]: Training Loss = 0.00014
	Epoch[158/200]: Training Loss = 0.00013
	Epoch[159/200]: Training Loss = 0.00015
	Epoch[160/200]: Training Loss = 0.00012
	Epoch[161/200]: Training Loss = 0.00011
	Epoch[162/200]: Training Loss = 0.00012
	Epoch[163/200]: Training Loss = 0.00011
	Epoch[164/200]: Training Loss = 0.00015
	Epoch[165/200]: Training Loss = 0.00262
	Epoch[166/200]: Training Loss = 0.00116
	Epoch[167/200]: Training Loss = 0.00038
	Epoch[168/200]: Training Loss = 0.00037
	Epoch[169/200]: Training Loss = 0.00021
	Epoch[170/200]: Training Loss = 0.00018
	Epoch[171/200]: Training Loss = 0.00016
	Epoch[172/200]: Training Loss = 0.00017
	Epoch[173/200]: Training Loss = 0.00018
	Epoch[174/200]: Training Loss = 0.00016
	Epoch[175/200]: Training Loss = 0.00014
	Epoch[176/200]: Training Loss = 0.00017
	Epoch[177/200]: Training Loss = 0.00239
	Epoch[178/200]: Training Loss = 0.00051
	Epoch[179/200]: Training Loss = 0.00028
	Epoch[180/200]: Training Loss = 0.00034
	Epoch[181/200]: Training Loss = 0.00022
	Epoch[182/200]: Training Loss = 0.00022
	Epoch[183/200]: Training Loss = 0.00014
	Epoch[184/200]: Training Loss = 0.00014
	Epoch[185/200]: Training Loss = 0.00015
	Epoch[186/200]: Training Loss = 0.00012
	Epoch[187/200]: Training Loss = 0.00010
	Epoch[188/200]: Training Loss = 0.00010
	Epoch[189/200]: Training Loss = 0.00012
	Epoch[190/200]: Training Loss = 0.00014
	Epoch[191/200]: Training Loss = 0.00012
	Epoch[192/200]: Training Loss = 0.00010
	Epoch[193/200]: Training Loss = 0.00009
	Epoch[194/200]: Training Loss = 0.00008
	Epoch[195/200]: Training Loss = 0.00009
	Epoch[196/200]: Training Loss = 0.00008
	Epoch[197/200]: Training Loss = 0.00008
	Epoch[198/200]: Training Loss = 0.00011
	Epoch[199/200]: Training Loss = 0.00008
	Epoch[200/200]: Training Loss = 0.00007
***Training Complete***

Final Optimizer Parameters
	alpha : 0.7506940960884094
	mu : 0.49785465002059937

***Testing Results***
==============================
Test Accuracy = 79.550 %
Test Error = 20.450 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [1.0, 0.9918592572212219, 0.985643208026886, 0.9798189997673035, 0.9736490249633789, 0.9655144214630127, 0.956991970539093, 0.9489985704421997, 0.9416957497596741, 0.9338104724884033, 0.9270104169845581, 0.9197383522987366, 0.9136488437652588, 0.9078195095062256, 0.9024204611778259, 0.897177517414093, 0.8913965225219727, 0.886121392250061, 0.8807792067527771, 0.8759303092956543, 0.8706647157669067, 0.8657504320144653, 0.860358715057373, 0.8549424409866333, 0.8497931957244873, 0.8449955582618713, 0.8401075005531311, 0.8353842496871948, 0.8305087089538574, 0.826213538646698, 0.8218130469322205, 0.8175479173660278, 0.8137493133544922, 0.8101637959480286, 0.8072929978370667, 0.8042101263999939, 0.800771176815033, 0.798165500164032, 0.7956806421279907, 0.7938063740730286, 0.7911753058433533, 0.7893632054328918, 0.7876359820365906, 0.7863584160804749, 0.7847310304641724, 0.7832141518592834, 0.7809903025627136, 0.7801446318626404, 0.7797642350196838, 0.7792155742645264, 0.7790873050689697, 0.7790029048919678, 0.7790014743804932, 0.7790014743804932, 0.7790001630783081, 0.7789999842643738, 0.7789993286132812, 0.7789981365203857, 0.778897762298584, 0.7788807153701782, 0.7788769602775574, 0.7788752317428589, 0.7788755297660828, 0.7788705229759216, 0.7788701057434082, 0.7788702845573425, 0.7788686156272888, 0.7788735628128052, 0.7788637280464172, 0.7788634300231934, 0.778862714767456, 0.7788279056549072, 0.7788243293762207, 0.7762742638587952, 0.7681911587715149, 0.7663781046867371, 0.7657942771911621, 0.7656890749931335, 0.7656534314155579, 0.7656286954879761, 0.7656258344650269, 0.7656232714653015, 0.7656236886978149, 0.7656241655349731, 0.7656247019767761, 0.7656252384185791, 0.7656249403953552, 0.7656248807907104, 0.7656247019767761, 0.7656247019767761, 0.7656247019767761, 0.7656079530715942, 0.7656080722808838, 0.7656083703041077, 0.7656086087226868, 0.7656086087226868, 0.7656089663505554, 0.7655320167541504, 0.7655315399169922, 0.7651399374008179, 0.7651010751724243, 0.7650824189186096, 0.7630264163017273, 0.7610316276550293, 0.7601066827774048, 0.7591160535812378, 0.7586907148361206, 0.7581878900527954, 0.7579293251037598, 0.7579196691513062, 0.7578668594360352, 0.7578662633895874, 0.7578639388084412, 0.7578611969947815, 0.7578607797622681, 0.7578610777854919, 0.7578609585762024, 0.7578602433204651, 0.7578601837158203, 0.7578601837158203, 0.7578601241111755, 0.7578601837158203, 0.7578597664833069, 0.7578597664833069, 0.7578598260879517, 0.757859468460083, 0.7578594088554382, 0.7578594088554382, 0.7578592896461487, 0.7577880024909973, 0.7577603459358215, 0.7570928335189819, 0.7565599679946899, 0.7546996474266052, 0.7519630193710327, 0.7516659498214722, 0.7515976428985596, 0.7515948414802551, 0.7515935897827148, 0.7515810132026672, 0.7515793442726135, 0.751573920249939, 0.7515734434127808, 0.751510739326477, 0.751509964466095, 0.7515102028846741, 0.7515104413032532, 0.7515096068382263, 0.7515053153038025, 0.7515053749084473, 0.751505434513092, 0.7515017986297607, 0.7515016198158264, 0.7515016198158264, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515014410018921, 0.7515012621879578, 0.7515010833740234, 0.7512110471725464, 0.7511934638023376, 0.7511934041976929, 0.7511906623840332, 0.7511906027793884, 0.7511903047561646, 0.7511903047561646, 0.7511900663375854, 0.7511900067329407, 0.7511900663375854, 0.7511900663375854, 0.7511897683143616, 0.7507085800170898, 0.7507004141807556, 0.7507001757621765, 0.7506998181343079, 0.7506999373435974, 0.7506994009017944, 0.7506993412971497, 0.7506991624832153, 0.7506992220878601, 0.7506992220878601, 0.7506992220878601, 0.7506992220878601, 0.7506991624832153, 0.7506942749023438, 0.750694215297699, 0.750694215297699, 0.7506941556930542, 0.7506940960884094, 0.7506940960884094, 0.7506940960884094, 0.7506940960884094, 0.7506940960884094, 0.7506940960884094, 0.7506940960884094]
mu: [0.8999999761581421, 0.8962712287902832, 0.8832800388336182, 0.8702019453048706, 0.8562625646591187, 0.8401688933372498, 0.824059009552002, 0.8095548748970032, 0.7958025336265564, 0.7818076014518738, 0.7698810696601868, 0.7562255263328552, 0.7452448606491089, 0.7351438403129578, 0.7254871129989624, 0.7164064645767212, 0.7066770792007446, 0.6974998116493225, 0.6893071532249451, 0.681228518486023, 0.6730446815490723, 0.6650107502937317, 0.65663743019104, 0.6481868028640747, 0.6399722099304199, 0.63251793384552, 0.6251946687698364, 0.6183865070343018, 0.6112161874771118, 0.605013370513916, 0.5980753898620605, 0.5924641489982605, 0.5868784189224243, 0.5817620158195496, 0.5773398280143738, 0.5731094479560852, 0.5682328343391418, 0.5646357536315918, 0.5609409809112549, 0.5580748319625854, 0.5545816421508789, 0.552331268787384, 0.549516499042511, 0.5477747917175293, 0.5450612306594849, 0.5427019000053406, 0.5389164686203003, 0.5376262068748474, 0.5369832515716553, 0.5364088416099548, 0.5361906290054321, 0.536060631275177, 0.5360549092292786, 0.5360531210899353, 0.536050021648407, 0.536050021648407, 0.5360486507415771, 0.5360478162765503, 0.5359065532684326, 0.5358811020851135, 0.5358673930168152, 0.5358663201332092, 0.5358659029006958, 0.5358597040176392, 0.5358585715293884, 0.5358580946922302, 0.5358558893203735, 0.5358343124389648, 0.5358195304870605, 0.5358186960220337, 0.5358178615570068, 0.5357706546783447, 0.535761296749115, 0.5324960350990295, 0.522111713886261, 0.5197383165359497, 0.518868088722229, 0.5186499953269958, 0.5186073184013367, 0.5185868144035339, 0.5185815691947937, 0.5185787081718445, 0.5185785889625549, 0.5185784697532654, 0.518578052520752, 0.5185779929161072, 0.5185778737068176, 0.5185772180557251, 0.5185771584510803, 0.5185771584510803, 0.5185768604278564, 0.5185497403144836, 0.5185497999191284, 0.5185497999191284, 0.5185498595237732, 0.5185498595237732, 0.5185502171516418, 0.518417477607727, 0.5184157490730286, 0.5176138281822205, 0.5175056457519531, 0.5174809098243713, 0.5147228837013245, 0.5117751955986023, 0.5104795098304749, 0.5090779066085815, 0.5084638595581055, 0.5077368021011353, 0.5075711607933044, 0.5075394511222839, 0.5074685215950012, 0.5074653029441833, 0.5074612498283386, 0.5074561238288879, 0.5074561834335327, 0.5074558258056641, 0.5074557662010193, 0.5074549317359924, 0.5074551105499268, 0.5074551105499268, 0.5074551105499268, 0.5074551701545715, 0.5074545741081238, 0.5074545741081238, 0.507454514503479, 0.5074543356895447, 0.5074543356895447, 0.5074542760848999, 0.507453978061676, 0.5072876811027527, 0.507267951965332, 0.5064247250556946, 0.5052394270896912, 0.5029749870300293, 0.49905797839164734, 0.4987435042858124, 0.49863138794898987, 0.49861231446266174, 0.49860844016075134, 0.49859708547592163, 0.49858927726745605, 0.49858254194259644, 0.49858173727989197, 0.49844250082969666, 0.4984409511089325, 0.49844127893447876, 0.49844127893447876, 0.4984404742717743, 0.49843913316726685, 0.49843916296958923, 0.4984392821788788, 0.49843835830688477, 0.4984382092952728, 0.49843814969062805, 0.49843791127204895, 0.49843791127204895, 0.4984378516674042, 0.49843788146972656, 0.4984378516674042, 0.4984377920627594, 0.4984377920627594, 0.4984377920627594, 0.4984377920627594, 0.4984377920627594, 0.4984380006790161, 0.49809592962265015, 0.4980613589286804, 0.4980609714984894, 0.4980555474758148, 0.49805542826652527, 0.49805498123168945, 0.49805498123168945, 0.4980546236038208, 0.498054563999176, 0.498054563999176, 0.4980545938014984, 0.49805471301078796, 0.4978654086589813, 0.4978635013103485, 0.4978632926940918, 0.4978625774383545, 0.4978627562522888, 0.4978625774383545, 0.4978625476360321, 0.49786245822906494, 0.49786242842674255, 0.49786242842674255, 0.49786242842674255, 0.49786242842674255, 0.4978622794151306, 0.49785488843917847, 0.4978548288345337, 0.4978548288345337, 0.4978548586368561, 0.4978547990322113, 0.4978547990322113, 0.4978547990322113, 0.4978547692298889, 0.49785465002059937, 0.49785465002059937, 0.49785465002059937]
Loss: [2.526044934463501, 2.203638963394165, 1.7711229357147216, 1.5979171965789796, 1.4569485084915161, 1.2825467042922973, 1.1391179172897339, 1.0162761299896241, 0.9241070826530456, 0.8443899471855164, 0.7613742813873291, 0.7111001171875, 0.6463962237739563, 0.5988138100337982, 0.5544563800430298, 0.5163053104400634, 0.4820299928283691, 0.4501787868309021, 0.41743723052978515, 0.38632062295913694, 0.357026493434906, 0.3317214690494537, 0.30483924757003783, 0.2864250230789185, 0.2624102270412445, 0.23585962962150575, 0.21437966971397399, 0.1935590306854248, 0.17719215189933776, 0.15968220858573914, 0.15011157325744628, 0.12872467415809632, 0.1177969823884964, 0.1049930548286438, 0.09169358583450317, 0.08499293053627015, 0.08443058087468147, 0.06569996178627015, 0.06935838044166565, 0.055146490678787234, 0.062257256417274476, 0.043155085105895996, 0.048543547605276106, 0.03623895150899887, 0.04431652624964714, 0.04067836894035339, 0.054260757050514224, 0.02993137398660183, 0.020050921536684037, 0.016836672915369274, 0.008438790010809898, 0.005345802114009857, 0.0021936136869341134, 0.0013958675453439354, 0.0012269564825296401, 0.0008737637248635292, 0.0009350017063133419, 0.0007624729654937982, 0.004243185491636396, 0.0018126603028178216, 0.0013168189472705127, 0.0008757080260291696, 0.0005988388127088547, 0.0007121060245111585, 0.0005997200511023402, 0.0004411115434020758, 0.0006350631121173501, 0.00141930154196918, 0.000909146761354059, 0.0005471919748745858, 0.0004926299847662449, 0.0013267395310476422, 0.0012263310654461383, 0.031718857375532386, 0.1121123378443718, 0.042214481729269025, 0.021772675912082196, 0.009994286634773015, 0.004009866422116756, 0.002700777858644724, 0.0012458917773514985, 0.0009580595202371478, 0.0008728347083926201, 0.000725779358819127, 0.0006383173360675574, 0.0006651905803382397, 0.0004715604693070054, 0.0006484671232523397, 0.0003981551822461188, 0.0003543294297531247, 0.0004026890091970563, 0.0007773132149502635, 0.000373273414876312, 0.0003309918576851487, 0.0003440074935462326, 0.00027520825162529947, 0.00030923398375511167, 0.001383606543764472, 0.0006147391475737094, 0.008276129710711538, 0.0035736693881452085, 0.0020297897508740424, 0.032483608837723735, 0.04142818956851959, 0.026145219283103944, 0.023900671124458314, 0.016362535721063613, 0.013236841405034065, 0.006316099109351635, 0.002668520524352789, 0.001902802994325757, 0.0012095078396238386, 0.0008268359141796827, 0.0013802489062212408, 0.0006575296156853438, 0.0004957824511826038, 0.00034553025003522634, 0.00034205827423371376, 0.00031837804518640043, 0.0002553684176597744, 0.000220827682800591, 0.00022004716442897916, 0.0002066714422777295, 0.00019769750263541936, 0.00018028129311278463, 0.0002148414509743452, 0.0001920665804296732, 0.00018920242263004183, 0.00025550559632480144, 0.0030295345521904526, 0.0009685236239433289, 0.01043771039545536, 0.01874754149198532, 0.03155484502017498, 0.044359207828044894, 0.011595582465082407, 0.006772429532557726, 0.0022541434112517164, 0.0014156575613841415, 0.0011875762695074082, 0.0011475117853283882, 0.0008807021941244602, 0.0006602982731163502, 0.003044698449075222, 0.0006458328893408179, 0.00046768477365374567, 0.0003293174374103546, 0.00034233512211591003, 0.0005868489671871066, 0.0002528743967693299, 0.0002073329742439091, 0.0003569556839112192, 0.0003034253578260541, 0.00020601217329502107, 0.00020545494060963393, 0.00015184575602412225, 0.00017793575007468462, 0.00014343380525708198, 0.00013368015944957733, 0.00015174599882448092, 0.00012243016494438053, 0.00011151620286516845, 0.00011973923476412893, 0.0001116048788651824, 0.00014837820833548903, 0.0026167757902294396, 0.0011642459175363184, 0.0003753073361515999, 0.0003660522165708244, 0.00020732541546225548, 0.00018187788314651698, 0.00015871464056894184, 0.0001749185469560325, 0.00018134612860158086, 0.00015798975247889756, 0.00014050808722153307, 0.00017241411871276796, 0.0023923542760312558, 0.0005132461114227772, 0.00027746544442139564, 0.00034324603339657187, 0.00022083973343484104, 0.00022366688162088393, 0.00014342318121343851, 0.000136822203444317, 0.00014986858421005308, 0.00012423085080459715, 9.803994309157134e-05, 0.00010381926629692316, 0.0001221368873678148, 0.00013838703698012978, 0.00011743860822170973, 9.683981792884878e-05, 8.58642774540931e-05, 8.206085512414575e-05, 8.612636615755036e-05, 7.903470055898652e-05, 7.582170655950904e-05, 0.00011233773653861136, 7.839550927281379e-05, 7.024713853374123e-05]
