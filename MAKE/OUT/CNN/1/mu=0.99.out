Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 1.0
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 2.42870
	Epoch[2/200]: Training Loss = 2.36148
	Epoch[3/200]: Training Loss = 2.35343
	Epoch[4/200]: Training Loss = 2.33353
	Epoch[5/200]: Training Loss = 2.34558
	Epoch[6/200]: Training Loss = 2.33324
	Epoch[7/200]: Training Loss = 2.33307
	Epoch[8/200]: Training Loss = 2.33407
	Epoch[9/200]: Training Loss = 2.33098
	Epoch[10/200]: Training Loss = 2.32180
	Epoch[11/200]: Training Loss = 2.33472
	Epoch[12/200]: Training Loss = 2.32801
	Epoch[13/200]: Training Loss = 2.31964
	Epoch[14/200]: Training Loss = 2.32487
	Epoch[15/200]: Training Loss = 2.31750
	Epoch[16/200]: Training Loss = 2.32451
	Epoch[17/200]: Training Loss = 2.32239
	Epoch[18/200]: Training Loss = 2.32159
	Epoch[19/200]: Training Loss = 2.32019
	Epoch[20/200]: Training Loss = 2.32192
	Epoch[21/200]: Training Loss = 2.32050
	Epoch[22/200]: Training Loss = 2.32189
	Epoch[23/200]: Training Loss = 2.31616
	Epoch[24/200]: Training Loss = 2.32456
	Epoch[25/200]: Training Loss = 2.32226
	Epoch[26/200]: Training Loss = 2.32064
	Epoch[27/200]: Training Loss = 2.32307
	Epoch[28/200]: Training Loss = 2.32191
	Epoch[29/200]: Training Loss = 2.31790
	Epoch[30/200]: Training Loss = 2.31772
	Epoch[31/200]: Training Loss = 2.32034
	Epoch[32/200]: Training Loss = 2.31580
	Epoch[33/200]: Training Loss = 2.31570
	Epoch[34/200]: Training Loss = 2.31624
	Epoch[35/200]: Training Loss = 2.31718
	Epoch[36/200]: Training Loss = 2.31906
	Epoch[37/200]: Training Loss = 2.32306
	Epoch[38/200]: Training Loss = 2.31701
	Epoch[39/200]: Training Loss = 2.31805
	Epoch[40/200]: Training Loss = 2.32052
	Epoch[41/200]: Training Loss = 2.31855
	Epoch[42/200]: Training Loss = 2.31656
	Epoch[43/200]: Training Loss = 2.31822
	Epoch[44/200]: Training Loss = 2.31400
	Epoch[45/200]: Training Loss = 2.31570
	Epoch[46/200]: Training Loss = 2.31563
	Epoch[47/200]: Training Loss = 2.31622
	Epoch[48/200]: Training Loss = 2.31512
	Epoch[49/200]: Training Loss = 2.31672
	Epoch[50/200]: Training Loss = 2.31497
	Epoch[51/200]: Training Loss = 2.31393
	Epoch[52/200]: Training Loss = 2.31676
	Epoch[53/200]: Training Loss = 2.31544
	Epoch[54/200]: Training Loss = 2.31629
	Epoch[55/200]: Training Loss = 2.31640
	Epoch[56/200]: Training Loss = 2.31628
	Epoch[57/200]: Training Loss = 2.31464
	Epoch[58/200]: Training Loss = 2.31430
	Epoch[59/200]: Training Loss = 2.31591
	Epoch[60/200]: Training Loss = 2.31410
	Epoch[61/200]: Training Loss = 2.31417
	Epoch[62/200]: Training Loss = 2.31488
	Epoch[63/200]: Training Loss = 2.31239
	Epoch[64/200]: Training Loss = 2.31642
	Epoch[65/200]: Training Loss = 2.31271
	Epoch[66/200]: Training Loss = 2.31103
	Epoch[67/200]: Training Loss = 2.31297
	Epoch[68/200]: Training Loss = 2.31542
	Epoch[69/200]: Training Loss = 2.31467
	Epoch[70/200]: Training Loss = 2.31213
	Epoch[71/200]: Training Loss = 2.31302
	Epoch[72/200]: Training Loss = 2.31248
	Epoch[73/200]: Training Loss = 2.31205
	Epoch[74/200]: Training Loss = 2.31184
	Epoch[75/200]: Training Loss = 2.31379
	Epoch[76/200]: Training Loss = 2.31321
	Epoch[77/200]: Training Loss = 2.31247
	Epoch[78/200]: Training Loss = 2.31474
	Epoch[79/200]: Training Loss = 2.31238
	Epoch[80/200]: Training Loss = 2.31256
	Epoch[81/200]: Training Loss = 2.31274
	Epoch[82/200]: Training Loss = 2.31267
	Epoch[83/200]: Training Loss = 2.31162
	Epoch[84/200]: Training Loss = 2.31101
	Epoch[85/200]: Training Loss = 2.31241
	Epoch[86/200]: Training Loss = 2.31325
	Epoch[87/200]: Training Loss = 2.31119
	Epoch[88/200]: Training Loss = 2.31056
	Epoch[89/200]: Training Loss = 2.31319
	Epoch[90/200]: Training Loss = 2.31260
	Epoch[91/200]: Training Loss = 2.31186
	Epoch[92/200]: Training Loss = 2.31306
	Epoch[93/200]: Training Loss = 2.31405
	Epoch[94/200]: Training Loss = 2.31110
	Epoch[95/200]: Training Loss = 2.31200
	Epoch[96/200]: Training Loss = 2.31081
	Epoch[97/200]: Training Loss = 2.31361
	Epoch[98/200]: Training Loss = 2.31221
	Epoch[99/200]: Training Loss = 2.31182
	Epoch[100/200]: Training Loss = 2.31139
	Epoch[101/200]: Training Loss = 2.31193
	Epoch[102/200]: Training Loss = 2.31055
	Epoch[103/200]: Training Loss = 2.31018
	Epoch[104/200]: Training Loss = 2.31233
	Epoch[105/200]: Training Loss = 2.31099
	Epoch[106/200]: Training Loss = 2.31118
	Epoch[107/200]: Training Loss = 2.31074
	Epoch[108/200]: Training Loss = 2.31068
	Epoch[109/200]: Training Loss = 2.31160
	Epoch[110/200]: Training Loss = 2.31143
	Epoch[111/200]: Training Loss = 2.31016
	Epoch[112/200]: Training Loss = 2.31157
	Epoch[113/200]: Training Loss = 2.30928
	Epoch[114/200]: Training Loss = 2.31094
	Epoch[115/200]: Training Loss = 2.31036
	Epoch[116/200]: Training Loss = 2.31146
	Epoch[117/200]: Training Loss = 2.31109
	Epoch[118/200]: Training Loss = 2.31100
	Epoch[119/200]: Training Loss = 2.31031
	Epoch[120/200]: Training Loss = 2.31065
	Epoch[121/200]: Training Loss = 2.31127
	Epoch[122/200]: Training Loss = 2.31194
	Epoch[123/200]: Training Loss = 2.31231
	Epoch[124/200]: Training Loss = 2.30864
	Epoch[125/200]: Training Loss = 2.31032
	Epoch[126/200]: Training Loss = 2.31087
	Epoch[127/200]: Training Loss = 2.31209
	Epoch[128/200]: Training Loss = 2.31075
	Epoch[129/200]: Training Loss = 2.31042
	Epoch[130/200]: Training Loss = 2.31063
	Epoch[131/200]: Training Loss = 2.31173
	Epoch[132/200]: Training Loss = 2.31109
	Epoch[133/200]: Training Loss = 2.31109
	Epoch[134/200]: Training Loss = 2.31076
	Epoch[135/200]: Training Loss = 2.31054
	Epoch[136/200]: Training Loss = 2.31281
	Epoch[137/200]: Training Loss = 2.30954
	Epoch[138/200]: Training Loss = 2.31084
	Epoch[139/200]: Training Loss = 2.31044
	Epoch[140/200]: Training Loss = 2.30953
	Epoch[141/200]: Training Loss = 2.31021
	Epoch[142/200]: Training Loss = 2.31019
	Epoch[143/200]: Training Loss = 2.30944
	Epoch[144/200]: Training Loss = 2.30996
	Epoch[145/200]: Training Loss = 2.31032
	Epoch[146/200]: Training Loss = 2.31023
	Epoch[147/200]: Training Loss = 2.30908
	Epoch[148/200]: Training Loss = 2.30998
	Epoch[149/200]: Training Loss = 2.31093
	Epoch[150/200]: Training Loss = 2.30973
	Epoch[151/200]: Training Loss = 2.31017
	Epoch[152/200]: Training Loss = 2.30908
	Epoch[153/200]: Training Loss = 2.30991
	Epoch[154/200]: Training Loss = 2.30872
	Epoch[155/200]: Training Loss = 2.31020
	Epoch[156/200]: Training Loss = 2.31000
	Epoch[157/200]: Training Loss = 2.31029
	Epoch[158/200]: Training Loss = 2.31032
	Epoch[159/200]: Training Loss = 2.30924
	Epoch[160/200]: Training Loss = 2.31020
	Epoch[161/200]: Training Loss = 2.30906
	Epoch[162/200]: Training Loss = 2.30915
	Epoch[163/200]: Training Loss = 2.30879
	Epoch[164/200]: Training Loss = 2.30877
	Epoch[165/200]: Training Loss = 2.30981
	Epoch[166/200]: Training Loss = 2.30914
	Epoch[167/200]: Training Loss = 2.30837
	Epoch[168/200]: Training Loss = 2.30912
	Epoch[169/200]: Training Loss = 2.30887
	Epoch[170/200]: Training Loss = 2.30793
	Epoch[171/200]: Training Loss = 2.30764
	Epoch[172/200]: Training Loss = 2.30967
	Epoch[173/200]: Training Loss = 2.30746
	Epoch[174/200]: Training Loss = 2.30860
	Epoch[175/200]: Training Loss = 2.30711
	Epoch[176/200]: Training Loss = 2.30781
	Epoch[177/200]: Training Loss = 2.30601
	Epoch[178/200]: Training Loss = 2.30549
	Epoch[179/200]: Training Loss = 2.30587
	Epoch[180/200]: Training Loss = 2.30337
	Epoch[181/200]: Training Loss = 2.30155
	Epoch[182/200]: Training Loss = 2.30056
	Epoch[183/200]: Training Loss = 2.29736
	Epoch[184/200]: Training Loss = 2.28958
	Epoch[185/200]: Training Loss = 2.27985
	Epoch[186/200]: Training Loss = 2.26782
	Epoch[187/200]: Training Loss = 2.25112
	Epoch[188/200]: Training Loss = 2.19244
	Epoch[189/200]: Training Loss = 2.07114
	Epoch[190/200]: Training Loss = 2.03776
	Epoch[191/200]: Training Loss = 2.01978
	Epoch[192/200]: Training Loss = 2.00597
	Epoch[193/200]: Training Loss = 1.99481
	Epoch[194/200]: Training Loss = 1.98441
	Epoch[195/200]: Training Loss = 1.97552
	Epoch[196/200]: Training Loss = 1.96716
	Epoch[197/200]: Training Loss = 1.96263
	Epoch[198/200]: Training Loss = 2.06126
	Epoch[199/200]: Training Loss = 1.97922
	Epoch[200/200]: Training Loss = 1.96806
***Training Complete***

Final Optimizer Parameters
	alpha : 0.9225898385047913
	mu : 0.8112973570823669

***Testing Results***
==============================
Test Accuracy = 21.660 %
Test Error = 78.340 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [1.0, 0.993015468120575, 0.9919397234916687, 0.9909451007843018, 0.9902856349945068, 0.989523708820343, 0.9888773560523987, 0.9882645010948181, 0.98763507604599, 0.987058162689209, 0.9865967631340027, 0.9860098958015442, 0.9855103492736816, 0.9850860238075256, 0.984638512134552, 0.9842089414596558, 0.9837865829467773, 0.9833561182022095, 0.9829584956169128, 0.9825490117073059, 0.9821798801422119, 0.9817908406257629, 0.9814109802246094, 0.9810506105422974, 0.9806695580482483, 0.9803420901298523, 0.9799938201904297, 0.9795953631401062, 0.9792709350585938, 0.9789434671401978, 0.9786187410354614, 0.9783152937889099, 0.9779837131500244, 0.9776656627655029, 0.9773454070091248, 0.9770530462265015, 0.9767465591430664, 0.976491391658783, 0.9761748909950256, 0.9758937358856201, 0.9756212830543518, 0.9753142595291138, 0.975064218044281, 0.9748021364212036, 0.974558413028717, 0.9742578268051147, 0.9740220308303833, 0.9737447500228882, 0.9734993577003479, 0.9732414484024048, 0.9729965925216675, 0.9727299809455872, 0.9724729061126709, 0.9722411036491394, 0.9720059633255005, 0.9717975854873657, 0.9715519547462463, 0.971279501914978, 0.971053421497345, 0.970822811126709, 0.9705982804298401, 0.9703587293624878, 0.9701581001281738, 0.9698782563209534, 0.96962970495224, 0.9693822860717773, 0.9691545367240906, 0.9689155220985413, 0.9687290787696838, 0.9685478210449219, 0.9683260917663574, 0.9681181311607361, 0.9679139852523804, 0.9676803350448608, 0.9674835205078125, 0.967281699180603, 0.9670608639717102, 0.9668810963630676, 0.9667003750801086, 0.9664913415908813, 0.9662750363349915, 0.966086208820343, 0.9658496379852295, 0.9656364917755127, 0.9654125571250916, 0.9652307629585266, 0.9650634527206421, 0.9648273587226868, 0.9646181464195251, 0.9644224643707275, 0.9642252922058105, 0.9640296697616577, 0.9638589024543762, 0.9636754393577576, 0.9634705781936646, 0.963291347026825, 0.96309894323349, 0.9629457592964172, 0.9627857804298401, 0.962583065032959, 0.9624240398406982, 0.9622126817703247, 0.9619727730751038, 0.9617906808853149, 0.9616125226020813, 0.961448609828949, 0.9612737894058228, 0.9611117839813232, 0.9609129428863525, 0.9607270956039429, 0.9605875015258789, 0.9603854417800903, 0.960221529006958, 0.960021436214447, 0.9598442316055298, 0.9596548676490784, 0.9594966769218445, 0.9593177437782288, 0.9591600894927979, 0.9589621424674988, 0.9587961435317993, 0.9586193561553955, 0.95844566822052, 0.9583056569099426, 0.958136796951294, 0.9579827785491943, 0.9578405618667603, 0.9577309489250183, 0.9575408101081848, 0.9573974609375, 0.9572073817253113, 0.9570700526237488, 0.9569523930549622, 0.9568039178848267, 0.9566624164581299, 0.9565067887306213, 0.9563825130462646, 0.956217348575592, 0.9560665488243103, 0.9559416174888611, 0.9558074474334717, 0.9556560516357422, 0.9554901123046875, 0.9553455710411072, 0.9551717042922974, 0.9550489783287048, 0.9549197554588318, 0.9547594785690308, 0.9546265602111816, 0.9544883370399475, 0.9543410539627075, 0.9541845321655273, 0.9540002346038818, 0.9538379907608032, 0.9537070393562317, 0.9535613059997559, 0.9534302353858948, 0.9533042907714844, 0.9531647562980652, 0.9530231952667236, 0.9528769850730896, 0.952733039855957, 0.9525820016860962, 0.952414870262146, 0.9522477984428406, 0.9521406292915344, 0.9519750475883484, 0.9518257975578308, 0.9516745805740356, 0.9515374302864075, 0.9513885378837585, 0.951227605342865, 0.9511352777481079, 0.9510064721107483, 0.9509095549583435, 0.9507682919502258, 0.950649082660675, 0.950502872467041, 0.9503700137138367, 0.9502698183059692, 0.9501311182975769, 0.9499600529670715, 0.9498144388198853, 0.9497188329696655, 0.949589192867279, 0.9494851231575012, 0.9493172764778137, 0.9491543769836426, 0.9487661719322205, 0.9473088383674622, 0.9457669258117676, 0.9438188076019287, 0.9415310621261597, 0.9395517110824585, 0.9375513792037964, 0.9355894327163696, 0.9333030581474304, 0.9311708807945251, 0.9271048903465271, 0.9248621463775635, 0.9225898385047913]
mu: [0.9900000095367432, 0.9839246273040771, 0.9807851314544678, 0.9780334830284119, 0.9763023853302002, 0.974024772644043, 0.9723031520843506, 0.9706193208694458, 0.9688861966133118, 0.967303454875946, 0.9661961197853088, 0.9644578695297241, 0.9630809426307678, 0.9621030688285828, 0.9608778357505798, 0.9599331617355347, 0.9587411880493164, 0.9576407670974731, 0.956576406955719, 0.9555595517158508, 0.9545255303382874, 0.9535059332847595, 0.952488899230957, 0.9516919255256653, 0.9505239129066467, 0.94949871301651, 0.9485346078872681, 0.9474338293075562, 0.9464305639266968, 0.9455638527870178, 0.9447422027587891, 0.9438264966011047, 0.9430561065673828, 0.9422767758369446, 0.9415042996406555, 0.9407204389572144, 0.9398467540740967, 0.9388731122016907, 0.9381073117256165, 0.9372895359992981, 0.9364292621612549, 0.9355564117431641, 0.9348187446594238, 0.9339896440505981, 0.9333490133285522, 0.9326335191726685, 0.9319596886634827, 0.9312321543693542, 0.9305478930473328, 0.9298135042190552, 0.9291356801986694, 0.9284877777099609, 0.9277328252792358, 0.927079975605011, 0.926357090473175, 0.9256932735443115, 0.9249621629714966, 0.9243048429489136, 0.9236616492271423, 0.922962486743927, 0.9223358035087585, 0.9217162132263184, 0.9210840463638306, 0.9204960465431213, 0.9197664260864258, 0.9191880822181702, 0.9187250733375549, 0.9181466102600098, 0.9175015091896057, 0.9169037342071533, 0.916344940662384, 0.9157842397689819, 0.9152589440345764, 0.9147249460220337, 0.9142111539840698, 0.9136245846748352, 0.9130421280860901, 0.9125367999076843, 0.9119517803192139, 0.9114180207252502, 0.9108372330665588, 0.9103000164031982, 0.9097430109977722, 0.909242570400238, 0.908738911151886, 0.9082683324813843, 0.9077255725860596, 0.907199501991272, 0.9067279100418091, 0.9061790108680725, 0.9056509137153625, 0.9051530361175537, 0.9046242237091064, 0.9040403962135315, 0.9035846590995789, 0.9031063318252563, 0.9026601314544678, 0.9021252989768982, 0.901656448841095, 0.9011610746383667, 0.9007229208946228, 0.900251030921936, 0.8997610211372375, 0.899329423904419, 0.8988147377967834, 0.898385763168335, 0.8979032039642334, 0.8974795341491699, 0.897042453289032, 0.8965543508529663, 0.8961191177368164, 0.8956964015960693, 0.8952241539955139, 0.8948761820793152, 0.8944293856620789, 0.8939987421035767, 0.8935436606407166, 0.8931154012680054, 0.8926599025726318, 0.8922114968299866, 0.8917447924613953, 0.8912627100944519, 0.8907589316368103, 0.890303909778595, 0.8899546265602112, 0.8895604014396667, 0.8891413807868958, 0.8886731863021851, 0.8882168531417847, 0.8878298401832581, 0.8873810172080994, 0.8869282603263855, 0.8864802122116089, 0.8860483765602112, 0.8856310248374939, 0.8851892948150635, 0.8846995830535889, 0.8843120336532593, 0.8839039206504822, 0.8835228085517883, 0.8831391930580139, 0.8827471733093262, 0.8823152780532837, 0.8819519877433777, 0.88153475522995, 0.8811482191085815, 0.8807430267333984, 0.8804092407226562, 0.880018949508667, 0.8795888423919678, 0.8791956305503845, 0.8788034319877625, 0.8783894181251526, 0.8779925107955933, 0.877673327922821, 0.8772737383842468, 0.8769296407699585, 0.8765453696250916, 0.8761476874351501, 0.8757982850074768, 0.8754104375839233, 0.8750566244125366, 0.8746728301048279, 0.8743271231651306, 0.8739786148071289, 0.8735896348953247, 0.8731796145439148, 0.8728780746459961, 0.8724642395973206, 0.8720747828483582, 0.8717408776283264, 0.8714094161987305, 0.8710342645645142, 0.8706692457199097, 0.8703030943870544, 0.8699529767036438, 0.8695624470710754, 0.8691969513893127, 0.8688116073608398, 0.8684536218643188, 0.8681365251541138, 0.8677536249160767, 0.8674212098121643, 0.8670678734779358, 0.8667060732841492, 0.866351306438446, 0.8659940958023071, 0.8656125664710999, 0.8645887970924377, 0.8611301183700562, 0.8577500581741333, 0.853971540927887, 0.849593460559845, 0.84568190574646, 0.8412765264511108, 0.8372155427932739, 0.8323743343353271, 0.8281022906303406, 0.820227861404419, 0.815987229347229, 0.8112973570823669]
Loss: [2.43484439743042, 2.428696296081543, 2.3614839247131347, 2.353427637786865, 2.3335310319519045, 2.3455821488952635, 2.3332391719055177, 2.3330690019226075, 2.3340736708068848, 2.330982506866455, 2.3218030296325685, 2.3347199646759034, 2.3280051879119874, 2.319644178314209, 2.3248712156677245, 2.317497381439209, 2.324512858734131, 2.3223913558197022, 2.3215893086242674, 2.320193156051636, 2.321916801147461, 2.320503744659424, 2.321889245223999, 2.3161573207092285, 2.3245572145080566, 2.3222616551208497, 2.32063541847229, 2.3230705893707277, 2.3219101205444335, 2.317904766693115, 2.317718009185791, 2.3203429241180418, 2.315804326629639, 2.3156990176391603, 2.31624424697876, 2.317181517868042, 2.3190595029449463, 2.323063235092163, 2.3170072161865236, 2.318053837814331, 2.320524235839844, 2.3185460961151123, 2.3165565324401856, 2.3182177992248536, 2.3139998389434813, 2.315699393157959, 2.3156250743103026, 2.3162241906738283, 2.3151205744934082, 2.316724878540039, 2.314965552062988, 2.3139306571960447, 2.3167632554626465, 2.3154438762664795, 2.3162913675689696, 2.3163973332977297, 2.3162776609802247, 2.3146351502990723, 2.314302768096924, 2.3159051315307617, 2.314098648376465, 2.314169841156006, 2.314884163360596, 2.31239289100647, 2.3164239324188234, 2.3127083766174317, 2.3110273167419435, 2.3129660718536376, 2.3154230738830566, 2.3146737483978272, 2.312128415985107, 2.313016885147095, 2.3124822985076903, 2.3120491748046876, 2.3118428938293456, 2.3137920957183837, 2.3132071923065185, 2.3124693702697754, 2.314735224075317, 2.3123810490417482, 2.31256305519104, 2.3127410459899904, 2.31266994682312, 2.3116191011047365, 2.31101298828125, 2.312409262924194, 2.3132454261779785, 2.311194779510498, 2.310556822891235, 2.3131911589813234, 2.312595986480713, 2.3118632135009767, 2.31306161026001, 2.3140534848022463, 2.311097434539795, 2.312003246307373, 2.310810899810791, 2.313613520355225, 2.312208995056152, 2.3118213314056395, 2.311386933670044, 2.311925934295654, 2.3105526080322267, 2.3101792329406736, 2.3123297102355957, 2.310994543914795, 2.3111809008026123, 2.31074479637146, 2.3106790356445313, 2.311602624053955, 2.3114323405456543, 2.3101576657867433, 2.3115700590515136, 2.309284557876587, 2.310942890853882, 2.3103617491149904, 2.3114629237365723, 2.3110914490509034, 2.3110028864288332, 2.3103104537963866, 2.3106542924499514, 2.311268580245972, 2.3119421391296386, 2.3123071922302247, 2.3086392222595213, 2.3103244519805908, 2.310874273529053, 2.312093112640381, 2.3107506925964354, 2.3104240670776366, 2.310625161361694, 2.311730743560791, 2.3110943593597413, 2.3110892317199707, 2.3107622430419923, 2.31054099899292, 2.312812003250122, 2.3095359046936035, 2.3108397984313966, 2.310440065155029, 2.309531226119995, 2.3102064431762694, 2.3101946520996095, 2.3094430098724366, 2.309964419555664, 2.3103216370391846, 2.310234142150879, 2.309081983642578, 2.3099762393951417, 2.310929181289673, 2.309728134613037, 2.3101717420959473, 2.3090774407958983, 2.3099116436767577, 2.3087220459747315, 2.310204995880127, 2.3099991205596924, 2.3102942883300783, 2.3103212866973877, 2.3092380753326416, 2.310198804779053, 2.3090638124084473, 2.3091531230163573, 2.3087928242492675, 2.3087689261627196, 2.309809620666504, 2.3091432868957518, 2.308371862564087, 2.309119411315918, 2.3088737248229982, 2.307930724639893, 2.3076379747009277, 2.3096698496246337, 2.307458408126831, 2.3085992208862303, 2.3071106730651856, 2.307814652862549, 2.3060100141906736, 2.305494250793457, 2.3058742360687257, 2.3033744529724123, 2.3015462632751467, 2.3005591190338133, 2.2973577388763426, 2.2895765757751465, 2.2798500361633303, 2.267824462966919, 2.251118533935547, 2.19243624458313, 2.0711396003723146, 2.037763119812012, 2.019777646636963, 2.0059683267974853, 1.994814412765503, 1.9844136785125732, 1.9755195404434205, 1.9671604488372803, 1.9626269357299804, 2.061264684677124, 1.9792152544403077, 1.9680575394058228]
