Args:
	model: CNN
	optimizer: SGD
	optimizer_args: {'mu': 0.09}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: CIFAR
	alpha: 1.0
	kappa: 0.0
	num_epochs: 200
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/200]: Training Loss = 2.00258
	Epoch[2/200]: Training Loss = 1.52456
	Epoch[3/200]: Training Loss = 1.18959
	Epoch[4/200]: Training Loss = 0.93291
	Epoch[5/200]: Training Loss = 0.75348
	Epoch[6/200]: Training Loss = 0.63732
	Epoch[7/200]: Training Loss = 0.54799
	Epoch[8/200]: Training Loss = 0.47561
	Epoch[9/200]: Training Loss = 0.41751
	Epoch[10/200]: Training Loss = 0.36126
	Epoch[11/200]: Training Loss = 0.31267
	Epoch[12/200]: Training Loss = 0.26610
	Epoch[13/200]: Training Loss = 0.22894
	Epoch[14/200]: Training Loss = 0.20069
	Epoch[15/200]: Training Loss = 0.16299
	Epoch[16/200]: Training Loss = 0.12926
	Epoch[17/200]: Training Loss = 0.11333
	Epoch[18/200]: Training Loss = 0.09541
	Epoch[19/200]: Training Loss = 0.07808
	Epoch[20/200]: Training Loss = 0.07023
	Epoch[21/200]: Training Loss = 0.06167
	Epoch[22/200]: Training Loss = 0.05384
	Epoch[23/200]: Training Loss = 0.03694
	Epoch[24/200]: Training Loss = 0.02621
	Epoch[25/200]: Training Loss = 0.02542
	Epoch[26/200]: Training Loss = 0.02001
	Epoch[27/200]: Training Loss = 0.03311
	Epoch[28/200]: Training Loss = 0.02406
	Epoch[29/200]: Training Loss = 0.03411
	Epoch[30/200]: Training Loss = 0.02856
	Epoch[31/200]: Training Loss = 0.03397
	Epoch[32/200]: Training Loss = 0.02144
	Epoch[33/200]: Training Loss = 0.01519
	Epoch[34/200]: Training Loss = 0.01132
	Epoch[35/200]: Training Loss = 0.00532
	Epoch[36/200]: Training Loss = 0.00353
	Epoch[37/200]: Training Loss = 0.00265
	Epoch[38/200]: Training Loss = 0.00134
	Epoch[39/200]: Training Loss = 0.00083
	Epoch[40/200]: Training Loss = 0.00054
	Epoch[41/200]: Training Loss = 0.00053
	Epoch[42/200]: Training Loss = 0.00051
	Epoch[43/200]: Training Loss = 0.00036
	Epoch[44/200]: Training Loss = 0.00030
	Epoch[45/200]: Training Loss = 0.00029
	Epoch[46/200]: Training Loss = 0.00023
	Epoch[47/200]: Training Loss = 0.00023
	Epoch[48/200]: Training Loss = 0.00020
	Epoch[49/200]: Training Loss = 0.00019
	Epoch[50/200]: Training Loss = 0.00024
	Epoch[51/200]: Training Loss = 0.00037
	Epoch[52/200]: Training Loss = 0.00022
	Epoch[53/200]: Training Loss = 0.00018
	Epoch[54/200]: Training Loss = 0.00020
	Epoch[55/200]: Training Loss = 0.00017
	Epoch[56/200]: Training Loss = 0.00022
	Epoch[57/200]: Training Loss = 0.00017
	Epoch[58/200]: Training Loss = 0.00015
	Epoch[59/200]: Training Loss = 0.00014
	Epoch[60/200]: Training Loss = 0.00012
	Epoch[61/200]: Training Loss = 0.00013
	Epoch[62/200]: Training Loss = 0.00012
	Epoch[63/200]: Training Loss = 0.00013
	Epoch[64/200]: Training Loss = 0.00013
	Epoch[65/200]: Training Loss = 0.00012
	Epoch[66/200]: Training Loss = 0.00011
	Epoch[67/200]: Training Loss = 0.00011
	Epoch[68/200]: Training Loss = 0.00010
	Epoch[69/200]: Training Loss = 0.00009
	Epoch[70/200]: Training Loss = 0.00010
	Epoch[71/200]: Training Loss = 0.00010
	Epoch[72/200]: Training Loss = 0.00010
	Epoch[73/200]: Training Loss = 0.00011
	Epoch[74/200]: Training Loss = 0.00009
	Epoch[75/200]: Training Loss = 0.00009
	Epoch[76/200]: Training Loss = 0.00007
	Epoch[77/200]: Training Loss = 0.00008
	Epoch[78/200]: Training Loss = 0.00008
	Epoch[79/200]: Training Loss = 0.00009
	Epoch[80/200]: Training Loss = 0.00008
	Epoch[81/200]: Training Loss = 0.00009
	Epoch[82/200]: Training Loss = 0.00012
	Epoch[83/200]: Training Loss = 0.00009
	Epoch[84/200]: Training Loss = 0.00011
	Epoch[85/200]: Training Loss = 0.00009
	Epoch[86/200]: Training Loss = 0.00009
	Epoch[87/200]: Training Loss = 0.00008
	Epoch[88/200]: Training Loss = 0.00007
	Epoch[89/200]: Training Loss = 0.00007
	Epoch[90/200]: Training Loss = 0.00007
	Epoch[91/200]: Training Loss = 0.00006
	Epoch[92/200]: Training Loss = 0.00006
	Epoch[93/200]: Training Loss = 0.00006
	Epoch[94/200]: Training Loss = 0.00007
	Epoch[95/200]: Training Loss = 0.00008
	Epoch[96/200]: Training Loss = 0.00006
	Epoch[97/200]: Training Loss = 0.00006
	Epoch[98/200]: Training Loss = 0.00006
	Epoch[99/200]: Training Loss = 0.00007
	Epoch[100/200]: Training Loss = 0.00007
	Epoch[101/200]: Training Loss = 0.00005
	Epoch[102/200]: Training Loss = 0.00007
	Epoch[103/200]: Training Loss = 0.00006
	Epoch[104/200]: Training Loss = 0.00006
	Epoch[105/200]: Training Loss = 0.00005
	Epoch[106/200]: Training Loss = 0.00006
	Epoch[107/200]: Training Loss = 0.00005
	Epoch[108/200]: Training Loss = 0.00005
	Epoch[109/200]: Training Loss = 0.00005
	Epoch[110/200]: Training Loss = 0.00005
	Epoch[111/200]: Training Loss = 0.00006
	Epoch[112/200]: Training Loss = 0.00005
	Epoch[113/200]: Training Loss = 0.00005
	Epoch[114/200]: Training Loss = 0.00004
	Epoch[115/200]: Training Loss = 0.00005
	Epoch[116/200]: Training Loss = 0.00004
	Epoch[117/200]: Training Loss = 0.00005
	Epoch[118/200]: Training Loss = 0.00005
	Epoch[119/200]: Training Loss = 0.00005
	Epoch[120/200]: Training Loss = 0.00005
	Epoch[121/200]: Training Loss = 0.00004
	Epoch[122/200]: Training Loss = 0.00004
	Epoch[123/200]: Training Loss = 0.00004
	Epoch[124/200]: Training Loss = 0.00004
	Epoch[125/200]: Training Loss = 0.00004
	Epoch[126/200]: Training Loss = 0.00005
	Epoch[127/200]: Training Loss = 0.00004
	Epoch[128/200]: Training Loss = 0.00004
	Epoch[129/200]: Training Loss = 0.00005
	Epoch[130/200]: Training Loss = 0.00004
	Epoch[131/200]: Training Loss = 0.00004
	Epoch[132/200]: Training Loss = 0.00004
	Epoch[133/200]: Training Loss = 0.00004
	Epoch[134/200]: Training Loss = 0.00005
	Epoch[135/200]: Training Loss = 0.00005
	Epoch[136/200]: Training Loss = 0.00004
	Epoch[137/200]: Training Loss = 0.00004
	Epoch[138/200]: Training Loss = 0.00004
	Epoch[139/200]: Training Loss = 0.00005
	Epoch[140/200]: Training Loss = 0.00004
	Epoch[141/200]: Training Loss = 0.00004
	Epoch[142/200]: Training Loss = 0.00004
	Epoch[143/200]: Training Loss = 0.00004
	Epoch[144/200]: Training Loss = 0.00003
	Epoch[145/200]: Training Loss = 0.00004
	Epoch[146/200]: Training Loss = 0.00003
	Epoch[147/200]: Training Loss = 0.00004
	Epoch[148/200]: Training Loss = 0.00004
	Epoch[149/200]: Training Loss = 0.00003
	Epoch[150/200]: Training Loss = 0.00003
	Epoch[151/200]: Training Loss = 0.00003
	Epoch[152/200]: Training Loss = 0.00003
	Epoch[153/200]: Training Loss = 0.00004
	Epoch[154/200]: Training Loss = 0.00004
	Epoch[155/200]: Training Loss = 0.00004
	Epoch[156/200]: Training Loss = 0.00005
	Epoch[157/200]: Training Loss = 0.00005
	Epoch[158/200]: Training Loss = 0.00004
	Epoch[159/200]: Training Loss = 0.00004
	Epoch[160/200]: Training Loss = 0.00004
	Epoch[161/200]: Training Loss = 0.00004
	Epoch[162/200]: Training Loss = 0.00004
	Epoch[163/200]: Training Loss = 0.00004
	Epoch[164/200]: Training Loss = 0.00004
	Epoch[165/200]: Training Loss = 0.00004
	Epoch[166/200]: Training Loss = 0.00004
	Epoch[167/200]: Training Loss = 0.00004
	Epoch[168/200]: Training Loss = 0.00009
	Epoch[169/200]: Training Loss = 0.00005
	Epoch[170/200]: Training Loss = 0.00004
	Epoch[171/200]: Training Loss = 0.00004
	Epoch[172/200]: Training Loss = 0.00004
	Epoch[173/200]: Training Loss = 0.00004
	Epoch[174/200]: Training Loss = 0.00004
	Epoch[175/200]: Training Loss = 0.00004
	Epoch[176/200]: Training Loss = 0.00003
	Epoch[177/200]: Training Loss = 0.00003
	Epoch[178/200]: Training Loss = 0.00003
	Epoch[179/200]: Training Loss = 0.00004
	Epoch[180/200]: Training Loss = 0.00003
	Epoch[181/200]: Training Loss = 0.00003
	Epoch[182/200]: Training Loss = 0.00003
	Epoch[183/200]: Training Loss = 0.00003
	Epoch[184/200]: Training Loss = 0.00003
	Epoch[185/200]: Training Loss = 0.00003
	Epoch[186/200]: Training Loss = 0.00003
	Epoch[187/200]: Training Loss = 0.00003
	Epoch[188/200]: Training Loss = 0.00003
	Epoch[189/200]: Training Loss = 0.00003
	Epoch[190/200]: Training Loss = 0.00003
	Epoch[191/200]: Training Loss = 0.00003
	Epoch[192/200]: Training Loss = 0.00003
	Epoch[193/200]: Training Loss = 0.00004
	Epoch[194/200]: Training Loss = 0.00004
	Epoch[195/200]: Training Loss = 0.00003
	Epoch[196/200]: Training Loss = 0.00003
	Epoch[197/200]: Training Loss = 0.00003
	Epoch[198/200]: Training Loss = 0.00003
	Epoch[199/200]: Training Loss = 0.00003
	Epoch[200/200]: Training Loss = 0.00002
***Training Complete***

Final Optimizer Parameters
	alpha : 0.5278870463371277
	mu : -0.013035614043474197

***Testing Results***
==============================
Test Accuracy = 80.680 %
Test Error = 19.320 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200]
alpha: [1.0, 0.9694893956184387, 0.9256962537765503, 0.8807163238525391, 0.8425707221031189, 0.8106034994125366, 0.7837852835655212, 0.7622389197349548, 0.7427548170089722, 0.7233273983001709, 0.7046201825141907, 0.6871960163116455, 0.6708058714866638, 0.6543932557106018, 0.6372065544128418, 0.6233812570571899, 0.6118054389953613, 0.5997090935707092, 0.5901400446891785, 0.5819257497787476, 0.5743480324745178, 0.5675917863845825, 0.561072826385498, 0.557227373123169, 0.5553241968154907, 0.5526160001754761, 0.5506291389465332, 0.5469246506690979, 0.5445818305015564, 0.540241003036499, 0.5368446111679077, 0.5323575735092163, 0.5304207801818848, 0.5291172862052917, 0.5285323858261108, 0.5283975005149841, 0.5281722545623779, 0.5279387831687927, 0.5279198288917542, 0.52791827917099, 0.5279173254966736, 0.5279126167297363, 0.5279129147529602, 0.5279126167297363, 0.5279122591018677, 0.5279115438461304, 0.5279114842414856, 0.5279111266136169, 0.5279110670089722, 0.5279109477996826, 0.5279107093811035, 0.5279123187065125, 0.5279121994972229, 0.5279121398925781, 0.5279119610786438, 0.5279119610786438, 0.527911901473999, 0.5279118418693542, 0.5279117822647095, 0.5279117822647095, 0.5279117822647095, 0.5279117226600647, 0.5279116630554199, 0.5279116034507751, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279116630554199, 0.5279117226600647, 0.5278909802436829, 0.5278909802436829, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908610343933, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278908014297485, 0.5278907418251038, 0.5278907418251038, 0.5278907418251038, 0.5278907418251038, 0.5278907418251038, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278906226158142, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278905630111694, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278904438018799, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278903841972351, 0.5278904438018799, 0.5278904438018799, 0.5278904438018799, 0.5278870463371277, 0.5278870463371277, 0.5278870463371277, 0.5278870463371277, 0.5278870463371277, 0.5278870463371277, 0.5278870463371277, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278869867324829, 0.5278870463371277, 0.5278870463371277]
mu: [0.09000000357627869, 0.08904021233320236, 0.09181460738182068, 0.09209473431110382, 0.08762259036302567, 0.08324501663446426, 0.07718385756015778, 0.06964091211557388, 0.062291182577610016, 0.05610427260398865, 0.04915361478924751, 0.042227692902088165, 0.03605296090245247, 0.030479473993182182, 0.025527438148856163, 0.02109675668179989, 0.01714923046529293, 0.013261517509818077, 0.009859640151262283, 0.007045507896691561, 0.004336650483310223, 0.0017704050987958908, -8.62415909068659e-05, -0.0013135350309312344, -0.00226567592471838, -0.0032419580966234207, -0.0038885234389454126, -0.005509182810783386, -0.006369732785969973, -0.008174463175237179, -0.009703104384243488, -0.011244688183069229, -0.012024312280118465, -0.012492351233959198, -0.012875157408416271, -0.012963307090103626, -0.012999794445931911, -0.01301434263586998, -0.013024698942899704, -0.013024324551224709, -0.013024620711803436, -0.013025044463574886, -0.013025591149926186, -0.013025657273828983, -0.013025695458054543, -0.013025766238570213, -0.013025731779634953, -0.013025675900280476, -0.013025728054344654, -0.013025709427893162, -0.013025742955505848, -0.013028907589614391, -0.013028905726969242, -0.013028940185904503, -0.013029024004936218, -0.013029000721871853, -0.013029059395194054, -0.013029072433710098, -0.013029084540903568, -0.013029081746935844, -0.01302907895296812, -0.013029090128839016, -0.013029086403548717, -0.013029099442064762, -0.013029093854129314, -0.01302910316735506, -0.013029106892645359, -0.013029111549258232, -0.013029112480580807, -0.013029119931161404, -0.013029105961322784, -0.013029112480580807, -0.013029145076870918, -0.013029150664806366, -0.013029170222580433, -0.013029178604483604, -0.013029182329773903, -0.013029193505644798, -0.013029185123741627, -0.013029190711677074, -0.013029196299612522, -0.013029289431869984, -0.013029595836997032, -0.01302958931773901, -0.013029622845351696, -0.013029624707996845, -0.013029629364609718, -0.01302961353212595, -0.013029608875513077, -0.013029605150222778, -0.013029603287577629, -0.013029608875513077, -0.013029602356255054, -0.01302960142493248, -0.013029622845351696, -0.013029888272285461, -0.013029887340962887, -0.013029887340962887, -0.013029889203608036, -0.013029905967414379, -0.013029922731220722, -0.013029920868575573, -0.013029909692704678, -0.013029921799898148, -0.013029922731220722, -0.013029925525188446, -0.013029925525188446, -0.013029933907091618, -0.013029932975769043, -0.013029932975769043, -0.013029934838414192, -0.013029953464865685, -0.013029955327510834, -0.013029953464865685, -0.01302995253354311, -0.013029954396188259, -0.013029954396188259, -0.01302995067089796, -0.013029941357672215, -0.013029943220317364, -0.013029967434704304, -0.01302996464073658, -0.01302996464073658, -0.01302996464073658, -0.013029965572059155, -0.013029966503381729, -0.013029967434704304, -0.013029966503381729, -0.013029965572059155, -0.013029978610575199, -0.01302997674793005, -0.013029979541897774, -0.013029979541897774, -0.013029979541897774, -0.013029898516833782, -0.013029899448156357, -0.013029896654188633, -0.013029895722866058, -0.013029898516833782, -0.013029899448156357, -0.013029903173446655, -0.013029907830059528, -0.013029906898736954, -0.013029905967414379, -0.01302990410476923, -0.013029905967414379, -0.013029908761382103, -0.013029909692704678, -0.013029908761382103, -0.013029917143285275, -0.013029915280640125, -0.0130299162119627, -0.013029917143285275, -0.013029919005930424, -0.013029919937252998, -0.013029919937252998, -0.013030542992055416, -0.013030551373958588, -0.013030561618506908, -0.013030561618506908, -0.013030564412474632, -0.013030561618506908, -0.013030561618506908, -0.013030562549829483, -0.013030563481152058, -0.013030564412474632, -0.01303057000041008, -0.013030567206442356, -0.013033455237746239, -0.013033455237746239, -0.013033448718488216, -0.013033449649810791, -0.013033449649810791, -0.013033448718488216, -0.013033449649810791, -0.013033452443778515, -0.013033452443778515, -0.013033454306423664, -0.013033456169068813, -0.013033460825681686, -0.013033460825681686, -0.013033459894359112, -0.013033458963036537, -0.013033458031713963, -0.013033459894359112, -0.013033459894359112, -0.013033458963036537, -0.013033458963036537, -0.013033454306423664, -0.013033444061875343, -0.013033444061875343, -0.013033443130552769, -0.013033426366746426, -0.013033644296228886, -0.013033643364906311, -0.013033644296228886, -0.013033643364906311, -0.013033643364906311, -0.01303365733474493, -0.013035613112151623, -0.013035614043474197]
Loss: [2.489138591156006, 2.002575711326599, 1.5245550716400147, 1.189589624557495, 0.9329143441581726, 0.7534793949699402, 0.6373215804100036, 0.5479933228969573, 0.4756086816978455, 0.4175083485412598, 0.3612614857292175, 0.3126665123748779, 0.2660955852985382, 0.22893714900970458, 0.2006936141872406, 0.16299443222045898, 0.1292631889629364, 0.11333401706933975, 0.09541091391563415, 0.07808409209728241, 0.07022846906661988, 0.061674072079658505, 0.05383541895627975, 0.03694263034105301, 0.02620824596762657, 0.025421174585819245, 0.020013719301223754, 0.03311002788186073, 0.02405770717382431, 0.03410785622358322, 0.02856364841222763, 0.03396714796543121, 0.02144077817797661, 0.01518841353416443, 0.011319784207195044, 0.005319116655886173, 0.0035278775817155837, 0.002646552554741502, 0.001339233019389212, 0.0008327696018666029, 0.00054174047768116, 0.0005343137272726744, 0.0005120809357520193, 0.0003642830711789429, 0.00030185983647592365, 0.0002906270623579621, 0.00023163465267047285, 0.00022760534913744777, 0.00020472902547102423, 0.00018524239807389678, 0.000238307759873569, 0.0003741865330841392, 0.00022105961441993714, 0.00017566411580890418, 0.0002027094040904194, 0.00016987431060522796, 0.00021989114901050925, 0.00016506363620981575, 0.00015429686334449797, 0.00014436346491798757, 0.00012213781224563718, 0.00012704430448822677, 0.00011843698678538203, 0.0001260111049283296, 0.0001268752097338438, 0.00011529621087945997, 0.00010699141260469332, 0.00011154138130135835, 9.572822588030249e-05, 9.448038769885898e-05, 0.00010152169906534255, 9.696326925535686e-05, 0.00010267317434772849, 0.00010507150895893574, 8.886728384532035e-05, 8.609628457110375e-05, 7.453287901589647e-05, 8.148746670223773e-05, 7.988323879893869e-05, 8.587540321052074e-05, 7.964429457671941e-05, 8.628404446877538e-05, 0.00011914136474952102, 9.315013692714273e-05, 0.00010523565856739878, 8.887471814639866e-05, 8.735501086339355e-05, 7.563182292506099e-05, 6.790805915254168e-05, 7.062948006670922e-05, 6.516828711610288e-05, 6.367841928265988e-05, 6.445552911609411e-05, 6.395917364861816e-05, 7.088341694325209e-05, 8.194641537964345e-05, 6.471065482124686e-05, 6.076257513952442e-05, 6.315746985957958e-05, 6.578809550032019e-05, 6.566055337432772e-05, 5.234535314142704e-05, 6.622279406990856e-05, 6.359024537727237e-05, 5.743424129090272e-05, 5.4856307706795636e-05, 5.7162307999096814e-05, 5.172830456402153e-05, 5.03010483039543e-05, 5.455302545800805e-05, 5.3768643473740665e-05, 6.049584593158215e-05, 5.202059012837708e-05, 5.204530949704349e-05, 4.4221303632948546e-05, 4.695744276046753e-05, 4.299757036846131e-05, 5.048068516422063e-05, 5.1009672349318865e-05, 4.9475379711948336e-05, 4.5706420987844465e-05, 4.3311200481839476e-05, 4.361114286351949e-05, 4.2686436132062225e-05, 4.04524979647249e-05, 4.1707771201618014e-05, 4.764046330936253e-05, 3.85092894686386e-05, 4.0825983015820385e-05, 4.8269797707907855e-05, 3.995297518093139e-05, 4.414037643466145e-05, 3.887694107717834e-05, 3.9406848791986703e-05, 5.465148703195155e-05, 4.9511082218959926e-05, 3.9186605799477545e-05, 4.123613898642361e-05, 3.971715813560877e-05, 4.799009295646101e-05, 3.908925456460565e-05, 4.0407633702270686e-05, 4.031594061641954e-05, 3.650760043878108e-05, 3.458227255789097e-05, 3.706172270176467e-05, 3.470066197682172e-05, 3.6476323848473836e-05, 3.5890201223082844e-05, 3.417375886812806e-05, 3.4425355810672046e-05, 3.387209299020469e-05, 3.155933999922126e-05, 3.95791091863066e-05, 3.5787604665383696e-05, 4.183944502845407e-05, 5.470807516016066e-05, 4.522163802757859e-05, 4.2227306766435504e-05, 3.575075072236359e-05, 4.2084154102194586e-05, 3.794272123661358e-05, 3.544074466452002e-05, 3.610576543142088e-05, 3.6822628234513105e-05, 3.792271945858374e-05, 3.634233274497092e-05, 4.185900581534952e-05, 9.249059609952383e-05, 4.8653809297829865e-05, 4.334157757926732e-05, 4.037399464461487e-05, 3.667439272627234e-05, 3.530838252569083e-05, 3.778516043908894e-05, 3.6660584325436505e-05, 3.305702357843984e-05, 3.190613316372037e-05, 3.316949835570995e-05, 3.558307446772233e-05, 2.9124784958548845e-05, 2.8291063526994548e-05, 3.151278825243935e-05, 2.78768679627683e-05, 2.7964335302822293e-05, 2.876115913008107e-05, 3.0369907000567764e-05, 2.5003837955882773e-05, 3.2041088603436945e-05, 2.9363909505773337e-05, 2.8242086383979768e-05, 2.777061993954703e-05, 2.6598172273952514e-05, 4.1518152547068894e-05, 3.8214717486407604e-05, 3.420220798347145e-05, 2.9589655215386302e-05, 2.6405049073509872e-05, 3.286353280534968e-05, 3.4421351356431845e-05, 2.4840402642730622e-05]
