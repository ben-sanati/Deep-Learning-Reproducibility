Args:
	model: CharRNN
	optimizer: Adam_alpha
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.0001, 'beta2': 0.0002}
	loss_fn: CrossEntropyLoss
	dataset: WarAndPeace
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 128
	save_model: False
	load_model: False
	device: cuda
Using 4 GPUs

***Beginning Training***
Training Loss: 4.661073983433735
	Epoch[1/30]: Training Loss = 3.23412
	Epoch[2/30]: Training Loss = 3.12224
	Epoch[3/30]: Training Loss = 3.12093
	Epoch[4/30]: Training Loss = 3.12091
	Epoch[5/30]: Training Loss = 3.12239
	Epoch[6/30]: Training Loss = 3.12518
	Epoch[7/30]: Training Loss = 3.12328
	Epoch[8/30]: Training Loss = 3.12540
	Epoch[9/30]: Training Loss = 3.12584
	Epoch[10/30]: Training Loss = 3.12991
	Epoch[11/30]: Training Loss = 3.13281
	Epoch[12/30]: Training Loss = 3.13281
	Epoch[13/30]: Training Loss = 3.13272
	Epoch[14/30]: Training Loss = 3.13429
	Epoch[15/30]: Training Loss = 3.13167
	Epoch[16/30]: Training Loss = 3.12665
	Epoch[17/30]: Training Loss = 3.12691
	Epoch[18/30]: Training Loss = 3.12651
	Epoch[19/30]: Training Loss = 3.12742
	Epoch[20/30]: Training Loss = 3.12549
	Epoch[21/30]: Training Loss = 3.12287
	Epoch[22/30]: Training Loss = 3.12907
	Epoch[23/30]: Training Loss = 3.14067
	Epoch[24/30]: Training Loss = 3.12725
	Epoch[25/30]: Training Loss = 3.12720
	Epoch[26/30]: Training Loss = 3.12941
	Epoch[27/30]: Training Loss = 3.13037
	Epoch[28/30]: Training Loss = 3.12915
	Epoch[29/30]: Training Loss = 3.13229
	Epoch[30/30]: Training Loss = 3.13155
***Training Complete***

Final Optimizer Parameters
	alpha : 0.012266507372260094

***Testing Results***
==============================
Test Accuracy = 16.027 %
Test Error = 83.973 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.09834397584199905, 0.008824501186609268, 0.03169785439968109, 0.020005211234092712, 0.005594578571617603, 0.021829789504408836, -0.0075551290065050125, 3.4564174711704254e-05, 0.001456497237086296, 0.0033761588856577873, 0.004243194125592709, 0.005685473792254925, 0.025973685085773468, 0.006195555441081524, 0.00467795692384243, -0.010813365690410137, 0.009000938385725021, 0.00926198624074459, -0.010638779029250145, 0.009813251905143261, 0.006372461095452309, 0.009806501679122448, -0.00937316007912159, 0.008968301117420197, -0.010798503644764423, -0.009187380783259869, 0.011054899543523788, 0.01106119155883789, 0.012098610401153564, 0.012266507372260094]
Loss: [4.661073983433735, 3.2341161521084336, 3.1222350338855422, 3.1209290286144578, 3.1209054969879517, 3.1223879894578315, 3.1251764871987953, 3.1232821912650603, 3.1254000376506026, 3.125835372740964, 3.129906344126506, 3.1328125, 3.1328125, 3.132718373493976, 3.1342949924698793, 3.1316712161144578, 3.1266472138554215, 3.126906061746988, 3.1265060240963853, 3.1274237575301207, 3.1254941641566263, 3.1228703878012047, 3.1290709713855422, 3.140672063253012, 3.1272472703313254, 3.1272002070783134, 3.1294121799698793, 3.1303652108433737, 3.1291533320783134, 3.1322948042168677, 3.1315535579819276]
