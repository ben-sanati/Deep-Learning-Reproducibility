Args:
	model: CharRNN
	optimizer: Adam_alpha
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.0001, 'beta2': 0.0002}
	loss_fn: CrossEntropyLoss
	dataset: WarAndPeace
	alpha: 0.0001
	kappa: 0.01
	num_epochs: 30
	batch_size: 128
	save_model: False
	load_model: False
	device: cuda
Using 4 GPUs

***Beginning Training***
Training Loss: 4.765836784638554
	Epoch[1/30]: Training Loss = 3.25306
	Epoch[2/30]: Training Loss = 3.12224
	Epoch[3/30]: Training Loss = 3.12111
	Epoch[4/30]: Training Loss = 3.12108
	Epoch[5/30]: Training Loss = 3.12187
	Epoch[6/30]: Training Loss = 3.12336
	Epoch[7/30]: Training Loss = 3.12278
	Epoch[8/30]: Training Loss = 3.12384
	Epoch[9/30]: Training Loss = 3.12339
	Epoch[10/30]: Training Loss = 3.12598
	Epoch[11/30]: Training Loss = 3.12578
	Epoch[12/30]: Training Loss = 3.12580
	Epoch[13/30]: Training Loss = 3.13053
	Epoch[14/30]: Training Loss = 3.12172
	Epoch[15/30]: Training Loss = 3.12776
	Epoch[16/30]: Training Loss = 3.13527
	Epoch[17/30]: Training Loss = 3.12847
	Epoch[18/30]: Training Loss = 3.12989
	Epoch[19/30]: Training Loss = 3.13139
	Epoch[20/30]: Training Loss = 3.12959
	Epoch[21/30]: Training Loss = 3.13126
	Epoch[22/30]: Training Loss = 3.13041
	Epoch[23/30]: Training Loss = 3.12914
	Epoch[24/30]: Training Loss = 3.13114
	Epoch[25/30]: Training Loss = 3.13005
	Epoch[26/30]: Training Loss = 3.12964
	Epoch[27/30]: Training Loss = 3.13192
	Epoch[28/30]: Training Loss = 3.13001
	Epoch[29/30]: Training Loss = 3.13181
	Epoch[30/30]: Training Loss = 3.13360
***Training Complete***

Final Optimizer Parameters
	alpha : 0.001340591348707676

***Testing Results***
==============================
Test Accuracy = 15.989 %
Test Error = 84.011 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [9.999999747378752e-05, 0.06949791312217712, 0.02046298421919346, 0.04092853143811226, 0.0021091699600219727, 0.005798629485070705, -0.012940618209540844, -0.012833810411393642, -0.01189834251999855, 0.009404214099049568, 0.006826060824096203, 0.019833996891975403, 0.020724335685372353, 0.005482337437570095, 0.0018088556826114655, 0.017494358122348785, 0.00023095682263374329, 0.020362716168165207, 0.00035030581057071686, 0.020146051421761513, 0.0002897065132856369, 0.0005916710942983627, 0.02026212587952614, -0.0003556152805685997, -0.0003901273012161255, 0.01971770077943802, -0.00010542850941419601, 0.00030810199677944183, 0.020495908334851265, 0.0009561879560351372, 0.001340591348707676]
Loss: [4.765836784638554, 3.253059111445783, 3.1222350338855422, 3.121105515813253, 3.121081984186747, 3.1218702936746987, 3.1233645519578315, 3.1227762612951806, 3.1238351844879517, 3.1233880835843375, 3.1259765625, 3.1257765436746987, 3.1258000753012047, 3.1305299322289155, 3.1217173381024095, 3.1277649661144578, 3.1352715549698793, 3.1284709149096384, 3.129894578313253, 3.1313888365963853, 3.1295886671686746, 3.1312594126506026, 3.1304122740963853, 3.1291415662650603, 3.1311417545180724, 3.1300475338855422, 3.1296357304216866, 3.131918298192771, 3.130012236445783, 3.131812405873494, 3.1336008094879517]
