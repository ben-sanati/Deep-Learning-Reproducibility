Args:
	model: CharRNN
	optimizer: Adam_alpha
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.0001, 'beta2': 0.0002}
	loss_fn: CrossEntropyLoss
	dataset: WarAndPeace
	alpha: 0.002
	kappa: 0.01
	num_epochs: 30
	batch_size: 128
	save_model: False
	load_model: False
	device: cuda
Using 4 GPUs

***Beginning Training***
Training Loss: 4.675922439759036
	Epoch[1/30]: Training Loss = 3.19698
	Epoch[2/30]: Training Loss = 3.12227
	Epoch[3/30]: Training Loss = 3.12101
	Epoch[4/30]: Training Loss = 3.12105
	Epoch[5/30]: Training Loss = 3.12175
	Epoch[6/30]: Training Loss = 3.12324
	Epoch[7/30]: Training Loss = 3.12408
	Epoch[8/30]: Training Loss = 3.12396
	Epoch[9/30]: Training Loss = 3.12574
	Epoch[10/30]: Training Loss = 3.12611
	Epoch[11/30]: Training Loss = 3.12500
	Epoch[12/30]: Training Loss = 3.12859
	Epoch[13/30]: Training Loss = 3.12861
	Epoch[14/30]: Training Loss = 3.12442
	Epoch[15/30]: Training Loss = 3.12544
	Epoch[16/30]: Training Loss = 3.12265
	Epoch[17/30]: Training Loss = 3.12857
	Epoch[18/30]: Training Loss = 3.14928
	Epoch[19/30]: Training Loss = 3.13134
	Epoch[20/30]: Training Loss = 3.12866
	Epoch[21/30]: Training Loss = 3.12959
	Epoch[22/30]: Training Loss = 3.13019
	Epoch[23/30]: Training Loss = 3.12952
	Epoch[24/30]: Training Loss = 3.14445
	Epoch[25/30]: Training Loss = 3.12685
	Epoch[26/30]: Training Loss = 3.13251
	Epoch[27/30]: Training Loss = 3.13640
	Epoch[28/30]: Training Loss = 3.13826
	Epoch[29/30]: Training Loss = 3.13775
	Epoch[30/30]: Training Loss = 3.13990
***Training Complete***

Final Optimizer Parameters
	alpha : 0.011788807809352875

***Testing Results***
==============================
Test Accuracy = 16.008 %
Test Error = 83.992 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0020000000949949026, 0.07282882183790207, 0.028308669105172157, 0.003371242433786392, 0.02791675180196762, 0.01866171695291996, -0.005280564539134502, 0.003009483218193054, 0.001141006126999855, 0.017591290175914764, -0.002159539610147476, -0.00481625460088253, 0.02198753133416176, 0.0029064370319247246, -0.0006998023018240929, 0.011226873844861984, 0.010932594537734985, 0.009282347746193409, -0.004110226407647133, -0.00419680867344141, -0.004066917113959789, 0.01596672460436821, -0.003891756758093834, -0.0039165206253528595, 0.01129872165620327, 0.009488746523857117, -0.009571148082613945, -0.008223590441048145, 0.011998485773801804, 0.012519306503236294, 0.011788807809352875]
Loss: [4.675922439759036, 3.1969832454819276, 3.1222703313253013, 3.121011389307229, 3.121046686746988, 3.1217526355421685, 3.1232351280120483, 3.124082266566265, 3.123964608433735, 3.1257412462349397, 3.126105986445783, 3.125, 3.1285885730421685, 3.1286121046686746, 3.1244234751506026, 3.1254353350903616, 3.1226468373493974, 3.1285650414156625, 3.149284638554217, 3.1313417733433737, 3.1286591679216866, 3.1295886671686746, 3.1301887236445785, 3.1295180722891565, 3.144448889307229, 3.126847232680723, 3.1325065888554215, 3.1364010730421685, 3.1382600715361444, 3.137754141566265, 3.1398955195783134]
