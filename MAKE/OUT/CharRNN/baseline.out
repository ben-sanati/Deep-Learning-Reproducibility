Args:
	model: CharRNN
	optimizer: Adam_alpha
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: WarAndPeace
	alpha: 0.002
	kappa: None
	num_epochs: 30
	batch_size: 128
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
Training Loss: 4.677781438253012
	Epoch[1/30]: Training Loss = 3.39234
	Epoch[2/30]: Training Loss = 3.19834
	Epoch[3/30]: Training Loss = 3.16020
	Epoch[4/30]: Training Loss = 3.14303
	Epoch[5/30]: Training Loss = 3.13433
	Epoch[6/30]: Training Loss = 3.12969
	Epoch[7/30]: Training Loss = 3.12726
	Epoch[8/30]: Training Loss = 3.12559
	Epoch[9/30]: Training Loss = 3.12474
	Epoch[10/30]: Training Loss = 3.12372
	Epoch[11/30]: Training Loss = 3.12328
	Epoch[12/30]: Training Loss = 3.12273
	Epoch[13/30]: Training Loss = 3.12224
	Epoch[14/30]: Training Loss = 3.12188
	Epoch[15/30]: Training Loss = 3.12167
	Epoch[16/30]: Training Loss = 3.12143
	Epoch[17/30]: Training Loss = 3.12168
	Epoch[18/30]: Training Loss = 3.12178
	Epoch[19/30]: Training Loss = 3.12180
	Epoch[20/30]: Training Loss = 3.12212
	Epoch[21/30]: Training Loss = 3.12211
	Epoch[22/30]: Training Loss = 3.12246
	Epoch[23/30]: Training Loss = 3.12235
	Epoch[24/30]: Training Loss = 3.12233
	Epoch[25/30]: Training Loss = 3.12253
	Epoch[26/30]: Training Loss = 3.12238
	Epoch[27/30]: Training Loss = 3.12267
	Epoch[28/30]: Training Loss = 3.12281
	Epoch[29/30]: Training Loss = 3.12289
	Epoch[30/30]: Training Loss = 3.12319
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0020000000949949026

***Testing Results***
==============================
Test Accuracy = 15.956 %
Test Error = 84.044 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026, 0.0020000000949949026]
Loss: [4.677781438253012, 3.3923428087349397, 3.198336314006024, 3.160203313253012, 3.1430252259036147, 3.1343302899096384, 3.1296945594879517, 3.1272590361445785, 3.1255882906626504, 3.1247411521084336, 3.1237175263554215, 3.1232821912650603, 3.1227291980421685, 3.1222350338855422, 3.1218820594879517, 3.1216702748493974, 3.1214349585843375, 3.1216820406626504, 3.1217761671686746, 3.1217996987951806, 3.122117375753012, 3.122105609939759, 3.1224585843373496, 3.1223526920180724, 3.1223291603915664, 3.1225291792168677, 3.1223762236445785, 3.1226703689759034, 3.1228115587349397, 3.1228939194277108, 3.123188064759036]
