Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.32093
	Epoch[2/30]: Training Loss = 0.09950
	Epoch[3/30]: Training Loss = 0.06062
	Epoch[4/30]: Training Loss = 0.04076
	Epoch[5/30]: Training Loss = 0.02983
	Epoch[6/30]: Training Loss = 0.02404
	Epoch[7/30]: Training Loss = 0.01969
	Epoch[8/30]: Training Loss = 0.01737
	Epoch[9/30]: Training Loss = 0.01558
	Epoch[10/30]: Training Loss = 0.01461
	Epoch[11/30]: Training Loss = 0.01350
	Epoch[12/30]: Training Loss = 0.01287
	Epoch[13/30]: Training Loss = 0.01208
	Epoch[14/30]: Training Loss = 0.01160
	Epoch[15/30]: Training Loss = 0.01129
	Epoch[16/30]: Training Loss = 0.01100
	Epoch[17/30]: Training Loss = 0.01081
	Epoch[18/30]: Training Loss = 0.01055
	Epoch[19/30]: Training Loss = 0.01045
	Epoch[20/30]: Training Loss = 0.01032
	Epoch[21/30]: Training Loss = 0.01013
	Epoch[22/30]: Training Loss = 0.01004
	Epoch[23/30]: Training Loss = 0.00996
	Epoch[24/30]: Training Loss = 0.00987
	Epoch[25/30]: Training Loss = 0.00984
	Epoch[26/30]: Training Loss = 0.00983
	Epoch[27/30]: Training Loss = 0.00982
	Epoch[28/30]: Training Loss = 0.00983
	Epoch[29/30]: Training Loss = 0.00981
	Epoch[30/30]: Training Loss = 0.00979
***Training Complete***

Final Optimizer Parameters
	alpha : 0.004030589479953051
	gamma : 0.906413733959198

***Testing Results***
==============================
Test Accuracy = 98.070 %
Test Error = 1.930 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.0832328274846077, 0.06794781982898712, 0.05475630611181259, 0.04422690346837044, 0.03782342001795769, 0.031129762530326843, 0.026837965473532677, 0.023552151396870613, 0.021696733310818672, 0.02030229941010475, 0.018754269927740097, 0.01631046272814274, 0.014243423007428646, 0.011676733382046223, 0.012235486879944801, 0.011090287007391453, 0.009807327762246132, 0.008810405619442463, 0.00965326838195324, 0.007248035166412592, 0.006710203364491463, 0.006205261684954166, 0.00422224635258317, 0.0027533138636499643, 0.002990453504025936, 0.0029752806294709444, 0.0034118127077817917, 0.004098551347851753, 0.0032917263451963663, 0.004030589479953051]
gamma: [0.9900000095367432, 0.9772390127182007, 0.9636052846908569, 0.950990617275238, 0.9398630261421204, 0.9330759644508362, 0.9263125061988831, 0.9219481348991394, 0.9182341694831848, 0.9160242676734924, 0.9152969121932983, 0.9136790037155151, 0.9118710160255432, 0.909885823726654, 0.9084102511405945, 0.9083848595619202, 0.9076569676399231, 0.9072695970535278, 0.9069274067878723, 0.9077319502830505, 0.9073408842086792, 0.9070555567741394, 0.9067487120628357, 0.9065121412277222, 0.9064365029335022, 0.906413733959198, 0.9064003229141235, 0.9063995480537415, 0.906434178352356, 0.906390368938446, 0.906413733959198]
Loss: [2.314053558095296, 0.32093093847433724, 0.09950258636871974, 0.06061874521970749, 0.04076326539218426, 0.029833914156258107, 0.02404390049179395, 0.019688330366214115, 0.017373028714954854, 0.015576999853551389, 0.014607157634198665, 0.013495250139633815, 0.012866388391455015, 0.012081944963335992, 0.011599082199359933, 0.01128867773997287, 0.010999062202125787, 0.010812949093431234, 0.010546007918318112, 0.010449439700941246, 0.010322011623221139, 0.010125954209516445, 0.010041657810906568, 0.009961551811297735, 0.00986781505048275, 0.009840104852616787, 0.009831254412606358, 0.009823825306942065, 0.009830317257841429, 0.009813713631033897, 0.009794932502259811]
