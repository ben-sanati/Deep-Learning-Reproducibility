Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.31268
	Epoch[2/30]: Training Loss = 0.09764
	Epoch[3/30]: Training Loss = 0.06045
	Epoch[4/30]: Training Loss = 0.04079
	Epoch[5/30]: Training Loss = 0.03081
	Epoch[6/30]: Training Loss = 0.02447
	Epoch[7/30]: Training Loss = 0.02039
	Epoch[8/30]: Training Loss = 0.01774
	Epoch[9/30]: Training Loss = 0.01563
	Epoch[10/30]: Training Loss = 0.01435
	Epoch[11/30]: Training Loss = 0.01352
	Epoch[12/30]: Training Loss = 0.01300
	Epoch[13/30]: Training Loss = 0.01242
	Epoch[14/30]: Training Loss = 0.01186
	Epoch[15/30]: Training Loss = 0.01141
	Epoch[16/30]: Training Loss = 0.01113
	Epoch[17/30]: Training Loss = 0.01109
	Epoch[18/30]: Training Loss = 0.01083
	Epoch[19/30]: Training Loss = 0.01062
	Epoch[20/30]: Training Loss = 0.01046
	Epoch[21/30]: Training Loss = 0.01044
	Epoch[22/30]: Training Loss = 0.01026
	Epoch[23/30]: Training Loss = 0.01021
	Epoch[24/30]: Training Loss = 0.01016
	Epoch[25/30]: Training Loss = 0.01015
	Epoch[26/30]: Training Loss = 0.01009
	Epoch[27/30]: Training Loss = 0.01004
	Epoch[28/30]: Training Loss = 0.01000
	Epoch[29/30]: Training Loss = 0.00994
	Epoch[30/30]: Training Loss = 0.00994
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0003461943124420941
	gamma : 0.9079725742340088

***Testing Results***
==============================
Test Accuracy = 98.060 %
Test Error = 1.940 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.08301592618227005, 0.06720118224620819, 0.05437442287802696, 0.04485318064689636, 0.03914051875472069, 0.03296787291765213, 0.0275217667222023, 0.024450547993183136, 0.020866330713033676, 0.017974186688661575, 0.017292363569140434, 0.01775958761572838, 0.014534181915223598, 0.012116461992263794, 0.011429395526647568, 0.01188724860548973, 0.010081072337925434, 0.009602908976376057, 0.0067544314078986645, 0.008790120482444763, 0.006421292200684547, 0.005484426394104958, 0.0053453897126019, 0.005950781051069498, 0.0050697531551122665, 0.005721243564039469, 0.005074902903288603, 0.0005783109809271991, 0.000654512841720134, 0.0003461943124420941]
gamma: [0.9900000095367432, 0.9773426055908203, 0.9630535244941711, 0.9507571458816528, 0.9413220286369324, 0.9355360269546509, 0.9289175271987915, 0.9237059354782104, 0.919212281703949, 0.9161690473556519, 0.9133071899414062, 0.9120808839797974, 0.9118185043334961, 0.9103040099143982, 0.9093817472457886, 0.9093725085258484, 0.9091284871101379, 0.9090467095375061, 0.9092098474502563, 0.9086936116218567, 0.9089117646217346, 0.908494770526886, 0.9081868529319763, 0.9082103967666626, 0.9083103537559509, 0.9080429077148438, 0.9080855250358582, 0.9080789685249329, 0.9079850912094116, 0.9079818725585938, 0.9079725742340088]
Loss: [2.318503142929077, 0.3126787642319997, 0.09764168678919474, 0.06044734237591426, 0.04079276960194111, 0.030808182922999063, 0.02447282617390156, 0.0203861873815457, 0.017738203632831573, 0.01562993703043709, 0.014354269796609878, 0.013519286281863848, 0.013001802810654044, 0.012420207518463334, 0.01185845457315445, 0.011409462380409241, 0.011133659606178601, 0.011088873079170784, 0.010826957816630602, 0.010621849301457405, 0.01046333044419686, 0.010438258152579268, 0.01026400414481759, 0.01021087221900622, 0.010156013929098845, 0.010145232014233868, 0.010094393797342976, 0.01003730254198114, 0.009995912563055754, 0.00993763256991903, 0.00993959180365006]
