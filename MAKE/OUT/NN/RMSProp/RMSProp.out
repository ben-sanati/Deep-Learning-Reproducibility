Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.31543
	Epoch[2/30]: Training Loss = 0.09715
	Epoch[3/30]: Training Loss = 0.05872
	Epoch[4/30]: Training Loss = 0.04030
	Epoch[5/30]: Training Loss = 0.02957
	Epoch[6/30]: Training Loss = 0.02331
	Epoch[7/30]: Training Loss = 0.01939
	Epoch[8/30]: Training Loss = 0.01676
	Epoch[9/30]: Training Loss = 0.01469
	Epoch[10/30]: Training Loss = 0.01344
	Epoch[11/30]: Training Loss = 0.01277
	Epoch[12/30]: Training Loss = 0.01225
	Epoch[13/30]: Training Loss = 0.01198
	Epoch[14/30]: Training Loss = 0.01171
	Epoch[15/30]: Training Loss = 0.01153
	Epoch[16/30]: Training Loss = 0.01141
	Epoch[17/30]: Training Loss = 0.01136
	Epoch[18/30]: Training Loss = 0.01129
	Epoch[19/30]: Training Loss = 0.01110
	Epoch[20/30]: Training Loss = 0.01095
	Epoch[21/30]: Training Loss = 0.01091
	Epoch[22/30]: Training Loss = 0.01089
	Epoch[23/30]: Training Loss = 0.01080
	Epoch[24/30]: Training Loss = 0.01076
	Epoch[25/30]: Training Loss = 0.01082
	Epoch[26/30]: Training Loss = 0.01064
	Epoch[27/30]: Training Loss = 0.01053
	Epoch[28/30]: Training Loss = 0.01049
	Epoch[29/30]: Training Loss = 0.01041
	Epoch[30/30]: Training Loss = 0.01038
***Training Complete***

Final Optimizer Parameters
	alpha : 0.003523885738104582
	gamma : 0.9092597961425781

***Testing Results***
==============================
Test Accuracy = 98.200 %
Test Error = 1.800 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.08339736610651016, 0.06643443554639816, 0.053834158927202225, 0.0449087992310524, 0.037727437913417816, 0.03168926760554314, 0.027073664590716362, 0.023208895698189735, 0.017966127023100853, 0.015645848587155342, 0.011800365522503853, 0.010634037666022778, 0.010431760922074318, 0.00724698044359684, 0.00811665877699852, 0.007477439474314451, 0.008729368448257446, 0.00769890146329999, 0.006324343383312225, 0.0046626548282802105, 0.006542609538882971, 0.006993643008172512, 0.0059466189704835415, 0.007837748154997826, 0.00849860068410635, 0.005642212461680174, 0.0071667213924229145, 0.006358953658491373, 0.0063921804539859295, 0.003523885738104582]
gamma: [0.9900000095367432, 0.9782111644744873, 0.9627506732940674, 0.9509454965591431, 0.9410440921783447, 0.9333946108818054, 0.9277099370956421, 0.9220467805862427, 0.9183893203735352, 0.9138482213020325, 0.9118757843971252, 0.90976482629776, 0.909867525100708, 0.9100695252418518, 0.9096144437789917, 0.9095726013183594, 0.9094821810722351, 0.9098222851753235, 0.909433901309967, 0.9093788266181946, 0.9091532826423645, 0.9094082117080688, 0.9094769954681396, 0.9094239473342896, 0.9096488356590271, 0.9098182320594788, 0.9092581272125244, 0.9093533754348755, 0.9093274474143982, 0.9094108939170837, 0.9092597961425781]
Loss: [2.3046654697418214, 0.3154285993893941, 0.09714730470180512, 0.05871918128331502, 0.040295888210833075, 0.029568582260608673, 0.023306125211467344, 0.01938893604079882, 0.016761103121439615, 0.014688093335429827, 0.013440857143700123, 0.012773429402709008, 0.012245068470637003, 0.01198486392920216, 0.011707156256834666, 0.011530858560899894, 0.011405121654768785, 0.01135896946216623, 0.011290369944771131, 0.01110425439948837, 0.010945090134690205, 0.010910949387898048, 0.010889393697182337, 0.010800864832599958, 0.010759301280975343, 0.010823873816989363, 0.010640577925990026, 0.01052967299123605, 0.010488242491707206, 0.010408868315567574, 0.0103759616676718]
