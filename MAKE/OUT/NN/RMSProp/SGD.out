Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.30524
	Epoch[2/30]: Training Loss = 0.12997
	Epoch[3/30]: Training Loss = 0.09954
	Epoch[4/30]: Training Loss = 0.08386
	Epoch[5/30]: Training Loss = 0.07341
	Epoch[6/30]: Training Loss = 0.06597
	Epoch[7/30]: Training Loss = 0.05997
	Epoch[8/30]: Training Loss = 0.05523
	Epoch[9/30]: Training Loss = 0.05140
	Epoch[10/30]: Training Loss = 0.04788
	Epoch[11/30]: Training Loss = 0.04532
	Epoch[12/30]: Training Loss = 0.04280
	Epoch[13/30]: Training Loss = 0.04064
	Epoch[14/30]: Training Loss = 0.03870
	Epoch[15/30]: Training Loss = 0.03670
	Epoch[16/30]: Training Loss = 0.03530
	Epoch[17/30]: Training Loss = 0.03384
	Epoch[18/30]: Training Loss = 0.03255
	Epoch[19/30]: Training Loss = 0.03133
	Epoch[20/30]: Training Loss = 0.03024
	Epoch[21/30]: Training Loss = 0.02917
	Epoch[22/30]: Training Loss = 0.02815
	Epoch[23/30]: Training Loss = 0.02721
	Epoch[24/30]: Training Loss = 0.02643
	Epoch[25/30]: Training Loss = 0.02561
	Epoch[26/30]: Training Loss = 0.02482
	Epoch[27/30]: Training Loss = 0.02408
	Epoch[28/30]: Training Loss = 0.02344
	Epoch[29/30]: Training Loss = 0.02275
	Epoch[30/30]: Training Loss = 0.02220
***Training Complete***

Final Optimizer Parameters
	alpha : 0.013965537771582603
	gamma : 0.9891674518585205

***Testing Results***
==============================
Test Accuracy = 98.000 %
Test Error = 2.000 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04174800217151642, 0.03404531255364418, 0.030154835432767868, 0.027715355157852173, 0.026322215795516968, 0.024815043434500694, 0.02342974953353405, 0.022427672520279884, 0.021289687603712082, 0.020727692171931267, 0.020084472373127937, 0.019582470878958702, 0.01925072632730007, 0.01846068538725376, 0.018087802454829216, 0.017633561044931412, 0.017316972836852074, 0.016854139044880867, 0.016488902270793915, 0.016270119696855545, 0.016191517934203148, 0.015804797410964966, 0.015573213808238506, 0.01540589053183794, 0.015203497372567654, 0.0148273054510355, 0.014658787287771702, 0.014455466531217098, 0.014271793887019157, 0.013965537771582603]
gamma: [0.9900000095367432, 0.9892421364784241, 0.9892152547836304, 0.989203929901123, 0.9891959428787231, 0.9891917109489441, 0.9891874194145203, 0.9891846179962158, 0.9891824722290039, 0.9891791939735413, 0.9891772270202637, 0.9891761541366577, 0.98917555809021, 0.9891748428344727, 0.9891737699508667, 0.989173173904419, 0.9891722202301025, 0.9891719222068787, 0.989171028137207, 0.9891706109046936, 0.9891701340675354, 0.9891698956489563, 0.9891694784164429, 0.9891695380210876, 0.9891693592071533, 0.9891690611839294, 0.9891685843467712, 0.9891681671142578, 0.9891679286956787, 0.9891677498817444, 0.9891674518585205]
Loss: [2.3046654697418214, 0.3052432660738627, 0.12997069211403528, 0.09954127806822459, 0.08385880840619406, 0.07341196664969127, 0.06597219738562903, 0.05997073725064596, 0.05522592499653498, 0.05140308343569438, 0.04787959225823482, 0.04531807022492091, 0.04279534327685833, 0.040642567547162375, 0.03869583969712257, 0.03670164919396242, 0.0353046335597833, 0.03383579447766145, 0.0325508936787645, 0.03133384211262067, 0.03024224214355151, 0.02917051325440407, 0.02814727281332016, 0.02721131642063459, 0.026430726743737858, 0.025607730578879516, 0.024824042654037475, 0.02407709860901038, 0.023444309922059377, 0.022750666252275308, 0.022196132099628448]
