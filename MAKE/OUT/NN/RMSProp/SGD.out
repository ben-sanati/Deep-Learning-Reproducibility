Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.30662
	Epoch[2/30]: Training Loss = 0.12948
	Epoch[3/30]: Training Loss = 0.09827
	Epoch[4/30]: Training Loss = 0.08170
	Epoch[5/30]: Training Loss = 0.07153
	Epoch[6/30]: Training Loss = 0.06356
	Epoch[7/30]: Training Loss = 0.05806
	Epoch[8/30]: Training Loss = 0.05347
	Epoch[9/30]: Training Loss = 0.04974
	Epoch[10/30]: Training Loss = 0.04645
	Epoch[11/30]: Training Loss = 0.04371
	Epoch[12/30]: Training Loss = 0.04104
	Epoch[13/30]: Training Loss = 0.03912
	Epoch[14/30]: Training Loss = 0.03711
	Epoch[15/30]: Training Loss = 0.03564
	Epoch[16/30]: Training Loss = 0.03428
	Epoch[17/30]: Training Loss = 0.03272
	Epoch[18/30]: Training Loss = 0.03145
	Epoch[19/30]: Training Loss = 0.03036
	Epoch[20/30]: Training Loss = 0.02928
	Epoch[21/30]: Training Loss = 0.02816
	Epoch[22/30]: Training Loss = 0.02716
	Epoch[23/30]: Training Loss = 0.02617
	Epoch[24/30]: Training Loss = 0.02547
	Epoch[25/30]: Training Loss = 0.02460
	Epoch[26/30]: Training Loss = 0.02384
	Epoch[27/30]: Training Loss = 0.02307
	Epoch[28/30]: Training Loss = 0.02236
	Epoch[29/30]: Training Loss = 0.02180
	Epoch[30/30]: Training Loss = 0.02117
***Training Complete***

Final Optimizer Parameters
	alpha : 0.014415938407182693
	gamma : 0.9892072081565857

***Testing Results***
==============================
Test Accuracy = 97.930 %
Test Error = 2.070 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04352520406246185, 0.035924676805734634, 0.03146454319357872, 0.02886425331234932, 0.02672400325536728, 0.025179581716656685, 0.02429947257041931, 0.023143306374549866, 0.022063111886382103, 0.021044084802269936, 0.020239362493157387, 0.01949065737426281, 0.018843695521354675, 0.018632547929883003, 0.01819472387433052, 0.017789531499147415, 0.01760403998196125, 0.017270762473344803, 0.01724742166697979, 0.01686674728989601, 0.016496839001774788, 0.016363130882382393, 0.01610352285206318, 0.015884369611740112, 0.01566963829100132, 0.015413019806146622, 0.01495672669261694, 0.014766434207558632, 0.014639286324381828, 0.014415938407182693]
gamma: [0.9900000095367432, 0.989290714263916, 0.9892627000808716, 0.9892475605010986, 0.9892393350601196, 0.9892327189445496, 0.989228367805481, 0.9892257452011108, 0.9892235398292542, 0.9892205595970154, 0.9892180562019348, 0.9892165660858154, 0.9892146587371826, 0.9892134070396423, 0.989213228225708, 0.9892124533653259, 0.9892119765281677, 0.9892118573188782, 0.9892114996910095, 0.98921138048172, 0.9892106652259827, 0.9892100095748901, 0.9892099499702454, 0.9892092943191528, 0.9892088770866394, 0.9892088770866394, 0.9892086982727051, 0.9892081618309021, 0.9892078638076782, 0.9892076253890991, 0.9892072081565857]
Loss: [2.3216932615915935, 0.30662022902965547, 0.12947792839209238, 0.09826679442723592, 0.08169895129998525, 0.07152881334026655, 0.06356166285276413, 0.05805875966151555, 0.053471785572171214, 0.04974006171226501, 0.046451412682731945, 0.0437120628029108, 0.04104495027760665, 0.03911705767313639, 0.03711493820051352, 0.03563621928095818, 0.03427536605596542, 0.03271734689772129, 0.03145127819577853, 0.03036045696338018, 0.02928415332734585, 0.028159140237172444, 0.027156887583931286, 0.026170887309809526, 0.02546619433561961, 0.024595058952768645, 0.02384135778148969, 0.023068168052782615, 0.02236426532169183, 0.021802059950431187, 0.021168013376990955]
