Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.32207
	Epoch[2/30]: Training Loss = 0.13771
	Epoch[3/30]: Training Loss = 0.10475
	Epoch[4/30]: Training Loss = 0.08777
	Epoch[5/30]: Training Loss = 0.07655
	Epoch[6/30]: Training Loss = 0.06832
	Epoch[7/30]: Training Loss = 0.06244
	Epoch[8/30]: Training Loss = 0.05768
	Epoch[9/30]: Training Loss = 0.05391
	Epoch[10/30]: Training Loss = 0.05093
	Epoch[11/30]: Training Loss = 0.04838
	Epoch[12/30]: Training Loss = 0.04601
	Epoch[13/30]: Training Loss = 0.04376
	Epoch[14/30]: Training Loss = 0.04191
	Epoch[15/30]: Training Loss = 0.04033
	Epoch[16/30]: Training Loss = 0.03882
	Epoch[17/30]: Training Loss = 0.03750
	Epoch[18/30]: Training Loss = 0.03609
	Epoch[19/30]: Training Loss = 0.03490
	Epoch[20/30]: Training Loss = 0.03385
	Epoch[21/30]: Training Loss = 0.03273
	Epoch[22/30]: Training Loss = 0.03175
	Epoch[23/30]: Training Loss = 0.03084
	Epoch[24/30]: Training Loss = 0.02997
	Epoch[25/30]: Training Loss = 0.02923
	Epoch[26/30]: Training Loss = 0.02838
	Epoch[27/30]: Training Loss = 0.02781
	Epoch[28/30]: Training Loss = 0.02718
	Epoch[29/30]: Training Loss = 0.02650
	Epoch[30/30]: Training Loss = 0.02593
***Training Complete***

Final Optimizer Parameters
	alpha : 0.01346069946885109
	gamma : 0.989202082157135

***Testing Results***
==============================
Test Accuracy = 98.050 %
Test Error = 1.950 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04228419438004494, 0.03502779081463814, 0.030826762318611145, 0.028168682008981705, 0.02601654827594757, 0.02410031110048294, 0.02251647599041462, 0.021444858983159065, 0.020377369597554207, 0.01979050040245056, 0.01908423751592636, 0.018576838076114655, 0.018064286559820175, 0.017712637782096863, 0.01719818450510502, 0.0169331356883049, 0.01647898741066456, 0.01609867997467518, 0.0158145260065794, 0.015537939965724945, 0.015260581858456135, 0.01502164825797081, 0.014648445881903172, 0.014285523444414139, 0.014120972715318203, 0.01399433147162199, 0.013873281888663769, 0.013671047985553741, 0.013694439083337784, 0.01346069946885109]
gamma: [0.9900000095367432, 0.9892758727073669, 0.9892510771751404, 0.9892390966415405, 0.9892317652702332, 0.9892263412475586, 0.9892212152481079, 0.9892170429229736, 0.9892148375511169, 0.9892123937606812, 0.9892112016677856, 0.9892097115516663, 0.9892085790634155, 0.9892075061798096, 0.9892067313194275, 0.9892059564590454, 0.9892057180404663, 0.9892045259475708, 0.989203929901123, 0.989203691482544, 0.989203155040741, 0.9892027378082275, 0.9892023205757141, 0.9892023801803589, 0.9892022609710693, 0.9892022609710693, 0.9892021417617798, 0.9892022013664246, 0.9892020225524902, 0.9892022013664246, 0.989202082157135]
Loss: [2.314581522623698, 0.3220665177702904, 0.13771190536022188, 0.1047526691754659, 0.08777416975895563, 0.07654684828718503, 0.06832441033919652, 0.06244051603674888, 0.05768498507142067, 0.05390503988265991, 0.050927253923813504, 0.04838277395665646, 0.04601243523359299, 0.04376440055370331, 0.041911305145422614, 0.040334669371445975, 0.0388245549723506, 0.03750118371148904, 0.03608833308815956, 0.03489645247956117, 0.0338519041766723, 0.032725916010141375, 0.031754983693361286, 0.030842079051335652, 0.02997000412742297, 0.029226907948652903, 0.028384663222233455, 0.027812513442834218, 0.027178926954666772, 0.026498849062124886, 0.02593136057257652]
