Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.29836
	Epoch[2/30]: Training Loss = 0.11303
	Epoch[3/30]: Training Loss = 0.09103
	Epoch[4/30]: Training Loss = 0.08408
	Epoch[5/30]: Training Loss = 0.08105
	Epoch[6/30]: Training Loss = 0.07863
	Epoch[7/30]: Training Loss = 0.07691
	Epoch[8/30]: Training Loss = 0.07688
	Epoch[9/30]: Training Loss = 0.07674
	Epoch[10/30]: Training Loss = 0.07681
	Epoch[11/30]: Training Loss = 0.07680
	Epoch[12/30]: Training Loss = 0.07476
	Epoch[13/30]: Training Loss = 0.07437
	Epoch[14/30]: Training Loss = 0.07434
	Epoch[15/30]: Training Loss = 0.07432
	Epoch[16/30]: Training Loss = 0.07427
	Epoch[17/30]: Training Loss = 0.07415
	Epoch[18/30]: Training Loss = 0.07421
	Epoch[19/30]: Training Loss = 0.07407
	Epoch[20/30]: Training Loss = 0.07408
	Epoch[21/30]: Training Loss = 0.07402
	Epoch[22/30]: Training Loss = 0.07400
	Epoch[23/30]: Training Loss = 0.07397
	Epoch[24/30]: Training Loss = 0.07414
	Epoch[25/30]: Training Loss = 0.07366
	Epoch[26/30]: Training Loss = 0.07352
	Epoch[27/30]: Training Loss = 0.07346
	Epoch[28/30]: Training Loss = 0.07347
	Epoch[29/30]: Training Loss = 0.07158
	Epoch[30/30]: Training Loss = 0.07128
***Training Complete***

Final Optimizer Parameters
	alpha : 3.8136744251460186e-07
	gamma : 0.8242454528808594

***Testing Results***
==============================
Test Accuracy = 97.360 %
Test Error = 2.640 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04261499643325806, 0.026095153763890266, 0.016405094414949417, 0.009818417951464653, 0.01585143618285656, 0.0014615325490012765, -0.00031919009052217007, 0.00013377923460211605, 3.597341856220737e-05, 0.017138056457042694, 0.007328453939408064, -0.00028300887788645923, 0.00189709581900388, 0.0002852354955393821, 0.007198969833552837, 0.0013397616567090154, 1.3056960597168654e-05, -5.572625741478987e-05, -1.115229315473698e-05, 5.567151788454794e-07, -0.0009849451016634703, 0.00016861410404089838, -0.013920255936682224, 0.0008497590315528214, -0.002248027129098773, 0.0009794365614652634, 0.025868192315101624, 0.005974399857223034, -0.0052163200452923775, 3.8136744251460186e-07]
gamma: [0.9900000095367432, 0.9724170565605164, 0.9706329107284546, 0.9700167179107666, 0.9694668054580688, 0.9693300724029541, 0.9666008353233337, 0.9665829539299011, 0.9638980627059937, 0.9638667702674866, 0.9702562689781189, 0.9610110521316528, 0.9571004509925842, 0.9548847079277039, 0.9489449858665466, 0.9403541684150696, 0.9205717444419861, 0.9198611378669739, 0.8972427845001221, 0.8972268104553223, 0.8881815075874329, 0.8775026798248291, 0.8505265116691589, 0.862944483757019, 0.8603910803794861, 0.8584814071655273, 0.8478674292564392, 0.8545492887496948, 0.8288432955741882, 0.8270187973976135, 0.8242454528808594]
Loss: [2.3046654697418214, 0.2983613326787949, 0.11302765696843466, 0.09102536906401316, 0.08408183758060138, 0.0810496507247289, 0.07863173409601053, 0.07691230293909708, 0.07688119116226833, 0.076743261496226, 0.07680926741162936, 0.07679854858716328, 0.07475966047445932, 0.07437124538818995, 0.07434398735761642, 0.07432082477807998, 0.07427415326038997, 0.07415020059347152, 0.07421315295100211, 0.07407306751608848, 0.07408011725346247, 0.07401754493713379, 0.07399770776033401, 0.07397419571479162, 0.07414379336039226, 0.07365665161013603, 0.07352260167598725, 0.07346224966446559, 0.07347061678767204, 0.07157511214017868, 0.07127917178869247]
