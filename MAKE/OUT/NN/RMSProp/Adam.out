Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.29548
	Epoch[2/30]: Training Loss = 0.11043
	Epoch[3/30]: Training Loss = 0.08865
	Epoch[4/30]: Training Loss = 0.08017
	Epoch[5/30]: Training Loss = 0.07818
	Epoch[6/30]: Training Loss = 0.07799
	Epoch[7/30]: Training Loss = 0.07794
	Epoch[8/30]: Training Loss = 0.07791
	Epoch[9/30]: Training Loss = 0.07791
	Epoch[10/30]: Training Loss = 0.07753
	Epoch[11/30]: Training Loss = 0.07718
	Epoch[12/30]: Training Loss = 0.07714
	Epoch[13/30]: Training Loss = 0.07739
	Epoch[14/30]: Training Loss = 0.07617
	Epoch[15/30]: Training Loss = 0.07544
	Epoch[16/30]: Training Loss = 0.07544
	Epoch[17/30]: Training Loss = 0.07562
	Epoch[18/30]: Training Loss = 0.07486
	Epoch[19/30]: Training Loss = 0.07483
	Epoch[20/30]: Training Loss = 0.07496
	Epoch[21/30]: Training Loss = 0.07446
	Epoch[22/30]: Training Loss = 0.07444
	Epoch[23/30]: Training Loss = 0.07448
	Epoch[24/30]: Training Loss = 0.07359
	Epoch[25/30]: Training Loss = 0.07358
	Epoch[26/30]: Training Loss = 0.07389
	Epoch[27/30]: Training Loss = 0.07235
	Epoch[28/30]: Training Loss = 0.07206
	Epoch[29/30]: Training Loss = 0.07215
	Epoch[30/30]: Training Loss = 0.07175
***Training Complete***

Final Optimizer Parameters
	alpha : -2.954723640868906e-06
	gamma : 0.8726622462272644

***Testing Results***
==============================
Test Accuracy = 97.210 %
Test Error = 2.790 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04374081641435623, 0.026897292584180832, 0.016036218032240868, 0.00668600108474493, 0.0008681860053911805, -0.000510161800775677, -0.0012893289094790816, 0.0010944466339424253, -0.011297807097434998, 0.0005407335120253265, -0.00010732580267358571, -0.0010680702980607748, -0.012767237611114979, 1.9037423726331326e-06, 6.298193966358667e-08, 0.0007233547512441874, 0.0024906829930841923, -2.5383320462424308e-05, 0.001164984074421227, 0.0017986686434596777, 0.0002527195611037314, -5.387921191868372e-05, -0.0009130666148848832, -8.46630719024688e-05, -0.00207063602283597, 0.007623395416885614, -0.0022907915990799665, 0.006394808180630207, -0.00044206599704921246, -2.954723640868906e-06]
gamma: [0.9900000095367432, 0.9722175598144531, 0.9702939391136169, 0.9693924784660339, 0.9689663648605347, 0.9688910245895386, 0.9688893556594849, 0.9679126739501953, 0.9668256044387817, 0.9608197808265686, 0.9511404037475586, 0.9508439302444458, 0.9447283148765564, 0.9427264332771301, 0.9341813325881958, 0.9341813325881958, 0.9342009425163269, 0.9262140393257141, 0.9257382154464722, 0.9222798943519592, 0.9184732437133789, 0.9182116389274597, 0.9176046252250671, 0.9022108316421509, 0.902093231678009, 0.901245653629303, 0.8858646154403687, 0.8823136687278748, 0.8834819197654724, 0.8727134466171265, 0.8726622462272644]
Loss: [2.3005936890920005, 0.2954768212080002, 0.1104327747742335, 0.08864619950056075, 0.0801719097296397, 0.07817637517849604, 0.07798960025906562, 0.07793695810039838, 0.07790757615168889, 0.07790970719655355, 0.07753395223617554, 0.07717651877800624, 0.077138568653663, 0.07738971665302913, 0.0761702820678552, 0.07544282099803289, 0.07544282671610514, 0.07562224105199178, 0.07485526353518168, 0.07482923680941264, 0.07496025637388229, 0.07446463945706686, 0.07444076206485431, 0.0744803867995739, 0.07358615806202094, 0.07357929386893908, 0.0738902527709802, 0.07235429650545121, 0.07205512605508169, 0.07215123116175334, 0.07174986158609391]
