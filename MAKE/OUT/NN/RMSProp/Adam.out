Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.29346
	Epoch[2/30]: Training Loss = 0.11343
	Epoch[3/30]: Training Loss = 0.09116
	Epoch[4/30]: Training Loss = 0.08396
	Epoch[5/30]: Training Loss = 0.08100
	Epoch[6/30]: Training Loss = 0.07904
	Epoch[7/30]: Training Loss = 0.07811
	Epoch[8/30]: Training Loss = 0.07811
	Epoch[9/30]: Training Loss = 0.07811
	Epoch[10/30]: Training Loss = 0.07809
	Epoch[11/30]: Training Loss = 0.07797
	Epoch[12/30]: Training Loss = 0.07803
	Epoch[13/30]: Training Loss = 0.07768
	Epoch[14/30]: Training Loss = 0.07768
	Epoch[15/30]: Training Loss = 0.07765
	Epoch[16/30]: Training Loss = 0.07771
	Epoch[17/30]: Training Loss = 0.07743
	Epoch[18/30]: Training Loss = 0.07716
	Epoch[19/30]: Training Loss = 0.07702
	Epoch[20/30]: Training Loss = 0.07709
	Epoch[21/30]: Training Loss = 0.07656
	Epoch[22/30]: Training Loss = 0.07582
	Epoch[23/30]: Training Loss = 0.07516
	Epoch[24/30]: Training Loss = 0.07520
	Epoch[25/30]: Training Loss = 0.07496
	Epoch[26/30]: Training Loss = 0.07496
	Epoch[27/30]: Training Loss = 0.07507
	Epoch[28/30]: Training Loss = 0.07457
	Epoch[29/30]: Training Loss = 0.07448
	Epoch[30/30]: Training Loss = 0.07458
***Training Complete***

Final Optimizer Parameters
	alpha : -0.004765209276229143
	gamma : 0.8368629217147827

***Testing Results***
==============================
Test Accuracy = 97.270 %
Test Error = 2.730 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.04292557016015053, 0.026448192074894905, 0.016228292137384415, 0.008988347835838795, 0.012585780583322048, 2.4564155864936765e-06, 0.0005291043198667467, -0.00024138086882885545, -0.0030549089424312115, -0.0006861926522105932, 5.676012733601965e-05, -0.0006840290734544396, 5.69730946153868e-05, -0.00011495800572447479, -3.363165524206124e-05, -0.00920284166932106, 0.0007655504741705954, 7.677034591324627e-05, -2.0170116840745322e-05, 0.013161496259272099, 0.008352016098797321, 0.00015668768901377916, 3.3363819511578185e-06, -0.00019158162467647344, 1.098353209272318e-06, 0.0007457506144419312, 0.0021509318612515926, -0.00036122871097177267, 1.0558793292148039e-05, -0.004765209276229143]
gamma: [0.9900000095367432, 0.9721065759658813, 0.9701880812644958, 0.9695855379104614, 0.9692429900169373, 0.9695367217063904, 0.9687851071357727, 0.9687795042991638, 0.9683752059936523, 0.9679780602455139, 0.9451674222946167, 0.9376421570777893, 0.933618426322937, 0.9334794282913208, 0.9297980070114136, 0.9229153990745544, 0.9261478185653687, 0.9186810851097107, 0.9124967455863953, 0.9124552011489868, 0.9173329472541809, 0.9017769694328308, 0.8909927606582642, 0.8907032608985901, 0.8824018239974976, 0.8823972940444946, 0.8795083165168762, 0.8596503138542175, 0.8556126952171326, 0.8548431992530823, 0.8368629217147827]
Loss: [2.309676047261556, 0.2934552663008372, 0.1134296022494634, 0.09115631134907405, 0.08395799649159114, 0.0809958320637544, 0.07904351886113485, 0.07811452921827634, 0.0781133384068807, 0.07810557352999846, 0.07808948927720388, 0.07796790733337403, 0.07803202265103658, 0.07768056419591109, 0.07767543470064799, 0.0776541040122509, 0.07770838547547658, 0.07743094937403996, 0.07716301911274592, 0.07702135888934135, 0.07708741631110509, 0.07656252753337224, 0.07581868743697802, 0.07515651446580887, 0.07520287956794103, 0.07495880718628566, 0.0749590468287468, 0.07506843526313703, 0.07457496463457744, 0.07447798767884573, 0.07457890702486038]
