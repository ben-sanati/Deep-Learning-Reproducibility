Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.32745
	Epoch[2/30]: Training Loss = 0.11964
	Epoch[3/30]: Training Loss = 0.08763
	Epoch[4/30]: Training Loss = 0.06827
	Epoch[5/30]: Training Loss = 0.05804
	Epoch[6/30]: Training Loss = 0.05036
	Epoch[7/30]: Training Loss = 0.04534
	Epoch[8/30]: Training Loss = 0.03755
	Epoch[9/30]: Training Loss = 0.03488
	Epoch[10/30]: Training Loss = 0.03227
	Epoch[11/30]: Training Loss = 0.02820
	Epoch[12/30]: Training Loss = 0.02556
	Epoch[13/30]: Training Loss = 0.02412
	Epoch[14/30]: Training Loss = 0.02425
	Epoch[15/30]: Training Loss = 0.02167
	Epoch[16/30]: Training Loss = 0.02107
	Epoch[17/30]: Training Loss = 0.01870
	Epoch[18/30]: Training Loss = 0.01953
	Epoch[19/30]: Training Loss = 0.01388
	Epoch[20/30]: Training Loss = 0.01546
	Epoch[21/30]: Training Loss = 0.01321
	Epoch[22/30]: Training Loss = 0.01264
	Epoch[23/30]: Training Loss = 0.01321
	Epoch[24/30]: Training Loss = 0.01225
	Epoch[25/30]: Training Loss = 0.01408
	Epoch[26/30]: Training Loss = 0.01257
	Epoch[27/30]: Training Loss = 0.01093
	Epoch[28/30]: Training Loss = 0.01002
	Epoch[29/30]: Training Loss = 0.01118
	Epoch[30/30]: Training Loss = 0.00880
***Training Complete***

Final Optimizer Parameters
	alpha : 0.09705425798892975
	gamma : 0.9881575107574463

***Testing Results***
==============================
Test Accuracy = 97.920 %
Test Error = 2.080 %
==============================
