Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.31644
	Epoch[2/30]: Training Loss = 0.11421
	Epoch[3/30]: Training Loss = 0.08473
	Epoch[4/30]: Training Loss = 0.06908
	Epoch[5/30]: Training Loss = 0.05802
	Epoch[6/30]: Training Loss = 0.04877
	Epoch[7/30]: Training Loss = 0.04455
	Epoch[8/30]: Training Loss = 0.04002
	Epoch[9/30]: Training Loss = 0.03457
	Epoch[10/30]: Training Loss = 0.03098
	Epoch[11/30]: Training Loss = 0.02920
	Epoch[12/30]: Training Loss = 0.02719
	Epoch[13/30]: Training Loss = 0.02622
	Epoch[14/30]: Training Loss = 0.02173
	Epoch[15/30]: Training Loss = 0.02075
	Epoch[16/30]: Training Loss = 0.02044
	Epoch[17/30]: Training Loss = 0.01831
	Epoch[18/30]: Training Loss = 0.01869
	Epoch[19/30]: Training Loss = 0.01767
	Epoch[20/30]: Training Loss = 0.01503
	Epoch[21/30]: Training Loss = 0.01451
	Epoch[22/30]: Training Loss = 0.01372
	Epoch[23/30]: Training Loss = 0.01392
	Epoch[24/30]: Training Loss = 0.01099
	Epoch[25/30]: Training Loss = 0.01154
	Epoch[26/30]: Training Loss = 0.01081
	Epoch[27/30]: Training Loss = 0.00986
	Epoch[28/30]: Training Loss = 0.01165
	Epoch[29/30]: Training Loss = 0.00999
	Epoch[30/30]: Training Loss = 0.01011
***Training Complete***

Final Optimizer Parameters
	alpha : 0.09749793261289597
	gamma : 0.9884576201438904

***Testing Results***
==============================
Test Accuracy = 97.840 %
Test Error = 2.160 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.09889806061983109, 0.09865722805261612, 0.0985139012336731, 0.0984024927020073, 0.09831682592630386, 0.09824267774820328, 0.09817564487457275, 0.09811516106128693, 0.09806405752897263, 0.09802086651325226, 0.09796507656574249, 0.09791828691959381, 0.09787993133068085, 0.09784320741891861, 0.097805917263031, 0.0977688878774643, 0.09773622453212738, 0.0977078229188919, 0.09767934679985046, 0.09766045957803726, 0.09764079004526138, 0.0976182073354721, 0.09759875386953354, 0.09758557379245758, 0.09756460040807724, 0.09755286574363708, 0.0975370779633522, 0.09752638638019562, 0.0975111573934555, 0.09749793261289597]
gamma: [0.9900000095367432, 0.989479660987854, 0.9893591403961182, 0.9892873167991638, 0.9892231822013855, 0.9891707897186279, 0.9891209602355957, 0.9890742301940918, 0.9890305399894714, 0.988993227481842, 0.9889575242996216, 0.9889132976531982, 0.988874077796936, 0.9888388514518738, 0.9888080954551697, 0.9887732863426208, 0.9887392520904541, 0.9887072443962097, 0.9886795878410339, 0.9886502027511597, 0.9886307120323181, 0.9886098504066467, 0.9885868430137634, 0.9885668158531189, 0.9885530471801758, 0.9885302186012268, 0.9885171055793762, 0.9885005354881287, 0.9884898662567139, 0.9884726405143738, 0.9884576201438904]
Loss: [2.31259986521403, 0.31644110453128815, 0.11420509778261184, 0.08472965667843818, 0.06908477890491485, 0.0580236373603344, 0.04876704565187295, 0.04455193359752496, 0.040019338354468345, 0.03457254822502533, 0.030975382869193952, 0.029204483885069688, 0.02719122143921753, 0.026218502732762137, 0.021725090564523514, 0.020752562849999715, 0.0204419194589369, 0.018313193338053924, 0.018689019813357542, 0.017665148769156078, 0.015033390091375137, 0.0145096098936289, 0.013724954509428547, 0.013919029582922546, 0.01099437163736123, 0.011540237853638002, 0.010809920706933675, 0.00985976154375021, 0.01165434968554376, 0.009991428520867096, 0.010112390806178154]
