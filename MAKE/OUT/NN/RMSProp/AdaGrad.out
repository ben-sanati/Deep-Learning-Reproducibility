Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.32850
	Epoch[2/30]: Training Loss = 0.12019
	Epoch[3/30]: Training Loss = 0.08570
	Epoch[4/30]: Training Loss = 0.07046
	Epoch[5/30]: Training Loss = 0.06019
	Epoch[6/30]: Training Loss = 0.05221
	Epoch[7/30]: Training Loss = 0.04404
	Epoch[8/30]: Training Loss = 0.04093
	Epoch[9/30]: Training Loss = 0.03860
	Epoch[10/30]: Training Loss = 0.03038
	Epoch[11/30]: Training Loss = 0.02884
	Epoch[12/30]: Training Loss = 0.02589
	Epoch[13/30]: Training Loss = 0.02461
	Epoch[14/30]: Training Loss = 0.02319
	Epoch[15/30]: Training Loss = 0.02128
	Epoch[16/30]: Training Loss = 0.02099
	Epoch[17/30]: Training Loss = 0.02166
	Epoch[18/30]: Training Loss = 0.01830
	Epoch[19/30]: Training Loss = 0.01639
	Epoch[20/30]: Training Loss = 0.01839
	Epoch[21/30]: Training Loss = 0.01492
	Epoch[22/30]: Training Loss = 0.01554
	Epoch[23/30]: Training Loss = 0.01576
	Epoch[24/30]: Training Loss = 0.01492
	Epoch[25/30]: Training Loss = 0.01346
	Epoch[26/30]: Training Loss = 0.01217
	Epoch[27/30]: Training Loss = 0.01220
	Epoch[28/30]: Training Loss = 0.01256
	Epoch[29/30]: Training Loss = 0.01197
	Epoch[30/30]: Training Loss = 0.01185
***Training Complete***

Final Optimizer Parameters
	alpha : 0.09710229188203812
	gamma : 0.9882907867431641

***Testing Results***
==============================
Test Accuracy = 97.800 %
Test Error = 2.200 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.09881272912025452, 0.0985359475016594, 0.09836525470018387, 0.09825282543897629, 0.09813665598630905, 0.09804802387952805, 0.09796921908855438, 0.09788981080055237, 0.09781710058450699, 0.09776311367750168, 0.09771169722080231, 0.09765627980232239, 0.097617968916893, 0.09757304936647415, 0.09753309190273285, 0.09748746454715729, 0.09744303673505783, 0.09740505367517471, 0.09737449139356613, 0.09733527153730392, 0.09729950875043869, 0.0972670167684555, 0.09724447131156921, 0.09722086042165756, 0.09720421582460403, 0.09718186408281326, 0.09716535359621048, 0.09714234620332718, 0.09712144732475281, 0.09710229188203812]
gamma: [0.9900000095367432, 0.9894887804985046, 0.9893714785575867, 0.9892861247062683, 0.9892235398292542, 0.9891542196273804, 0.9890987277030945, 0.9890443086624146, 0.9889882206916809, 0.9889342784881592, 0.9888921976089478, 0.9888533353805542, 0.9888097643852234, 0.9887803196907043, 0.9887431263923645, 0.9887070655822754, 0.9886664152145386, 0.9886273741722107, 0.9885932207107544, 0.9885663390159607, 0.9885276556015015, 0.9884922504425049, 0.9884623289108276, 0.9884387254714966, 0.9884160757064819, 0.9883989691734314, 0.988376259803772, 0.988357663154602, 0.9883341789245605, 0.9883115291595459, 0.9882907867431641]
Loss: [2.301705419286092, 0.3284971956968307, 0.12019468421936035, 0.08569928849140802, 0.07045691180825234, 0.06018763302763303, 0.052213358026991286, 0.04404127626518409, 0.040926003135244055, 0.03859509551405596, 0.030378067274640003, 0.02884219581373036, 0.025887471103916565, 0.024608115214047332, 0.023191025752139587, 0.021275189675670117, 0.020987826500770947, 0.021659496789732172, 0.01830317611763797, 0.016393134971030184, 0.018387529721894922, 0.014921961345669116, 0.01554145353223042, 0.0157638006070784, 0.014921876840984139, 0.013462763149565822, 0.012169801633187064, 0.012195175749519694, 0.012557994485429177, 0.011970284344124471, 0.011849493309262714]
