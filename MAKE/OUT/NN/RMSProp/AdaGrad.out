Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.0001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.32127
	Epoch[2/30]: Training Loss = 0.11932
	Epoch[3/30]: Training Loss = 0.08534
	Epoch[4/30]: Training Loss = 0.06950
	Epoch[5/30]: Training Loss = 0.05943
	Epoch[6/30]: Training Loss = 0.05131
	Epoch[7/30]: Training Loss = 0.04416
	Epoch[8/30]: Training Loss = 0.03819
	Epoch[9/30]: Training Loss = 0.03367
	Epoch[10/30]: Training Loss = 0.03135
	Epoch[11/30]: Training Loss = 0.02879
	Epoch[12/30]: Training Loss = 0.02532
	Epoch[13/30]: Training Loss = 0.02574
	Epoch[14/30]: Training Loss = 0.02227
	Epoch[15/30]: Training Loss = 0.02064
	Epoch[16/30]: Training Loss = 0.02057
	Epoch[17/30]: Training Loss = 0.02039
	Epoch[18/30]: Training Loss = 0.01867
	Epoch[19/30]: Training Loss = 0.01633
	Epoch[20/30]: Training Loss = 0.01597
	Epoch[21/30]: Training Loss = 0.01424
	Epoch[22/30]: Training Loss = 0.01464
	Epoch[23/30]: Training Loss = 0.01307
	Epoch[24/30]: Training Loss = 0.01193
	Epoch[25/30]: Training Loss = 0.00990
	Epoch[26/30]: Training Loss = 0.01199
	Epoch[27/30]: Training Loss = 0.01225
	Epoch[28/30]: Training Loss = 0.01114
	Epoch[29/30]: Training Loss = 0.01015
	Epoch[30/30]: Training Loss = 0.01181
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0973154678940773
	gamma : 0.988437831401825

***Testing Results***
==============================
Test Accuracy = 97.830 %
Test Error = 2.170 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.09886359423398972, 0.09861072152853012, 0.0984542965888977, 0.09833736717700958, 0.09823809564113617, 0.09814675152301788, 0.0980725958943367, 0.0980093702673912, 0.09794328361749649, 0.09788700193166733, 0.09784052520990372, 0.09778845310211182, 0.09775034338235855, 0.0977131575345993, 0.0976814553141594, 0.09764188528060913, 0.0976051464676857, 0.09756968915462494, 0.09754292666912079, 0.09751629084348679, 0.09749416261911392, 0.09746881574392319, 0.09745066612958908, 0.09742981195449829, 0.09741821140050888, 0.09739062190055847, 0.09736824780702591, 0.09734466671943665, 0.09733174741268158, 0.0973154678940773]
gamma: [0.9900000095367432, 0.9894952178001404, 0.9893877506256104, 0.9893109798431396, 0.9892457723617554, 0.9891872406005859, 0.9891294240951538, 0.9890804886817932, 0.9890373349189758, 0.9889939427375793, 0.9889524579048157, 0.9889182448387146, 0.9888789057731628, 0.9888482093811035, 0.9888157248497009, 0.9887899160385132, 0.9887526631355286, 0.9887197613716125, 0.9886865019798279, 0.9886618852615356, 0.9886360168457031, 0.9886144995689392, 0.988590657711029, 0.9885738492012024, 0.9885528087615967, 0.9885406494140625, 0.9885146021842957, 0.9884926676750183, 0.9884663224220276, 0.9884524941444397, 0.988437831401825]
Loss: [2.3046654697418214, 0.3212701211651166, 0.11932140296697616, 0.08534450047016144, 0.06949662099877993, 0.05943109974861145, 0.051306682571768764, 0.044163944412519536, 0.038191712963581086, 0.03367362414374948, 0.031349110925593414, 0.028789320609346032, 0.025320530820358546, 0.02573616885760178, 0.022265648705388108, 0.020637783604953438, 0.020570604854659176, 0.020392764248625222, 0.018671055745509996, 0.016332068791923424, 0.015972458707968082, 0.014238046922330007, 0.014637561784343172, 0.013071517906951097, 0.011930166919546901, 0.00989570261486474, 0.011990087757703926, 0.012253219877204537, 0.01113614343896091, 0.010152641863664637, 0.011813773325972337]
