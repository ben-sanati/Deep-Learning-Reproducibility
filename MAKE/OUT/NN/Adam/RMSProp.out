Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.48702
	Epoch[2/30]: Training Loss = 0.20785
	Epoch[3/30]: Training Loss = 0.15466
	Epoch[4/30]: Training Loss = 0.12352
	Epoch[5/30]: Training Loss = 0.10548
	Epoch[6/30]: Training Loss = 0.09472
	Epoch[7/30]: Training Loss = 0.08905
	Epoch[8/30]: Training Loss = 0.08543
	Epoch[9/30]: Training Loss = 0.08391
	Epoch[10/30]: Training Loss = 0.08061
	Epoch[11/30]: Training Loss = 0.07617
	Epoch[12/30]: Training Loss = 0.07202
	Epoch[13/30]: Training Loss = 0.06905
	Epoch[14/30]: Training Loss = 0.06678
	Epoch[15/30]: Training Loss = 0.06560
	Epoch[16/30]: Training Loss = 0.06446
	Epoch[17/30]: Training Loss = 0.06397
	Epoch[18/30]: Training Loss = 0.06360
	Epoch[19/30]: Training Loss = 0.06187
	Epoch[20/30]: Training Loss = 0.05971
	Epoch[21/30]: Training Loss = 0.05728
	Epoch[22/30]: Training Loss = 0.05585
	Epoch[23/30]: Training Loss = 0.05347
	Epoch[24/30]: Training Loss = 0.05170
	Epoch[25/30]: Training Loss = 0.05213
	Epoch[26/30]: Training Loss = 0.05211
	Epoch[27/30]: Training Loss = 0.04985
	Epoch[28/30]: Training Loss = 0.04960
	Epoch[29/30]: Training Loss = 0.04915
	Epoch[30/30]: Training Loss = 0.04871
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0003808429173659533
	beta1 : 0.8868943452835083
	beta2 : 0.9955606460571289

***Testing Results***
==============================
Test Accuracy = 97.560 %
Test Error = 2.440 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.001372956670820713, 0.001092920545488596, 0.000933077244553715, 0.0005722623900510371, 0.00036291396827436984, 0.00023905425041448325, 0.00018747807189356536, 0.00012410589260980487, 0.0002884575806092471, 0.00022556513431482017, 0.0002532337384764105, 0.00015290595183614641, 9.852178482105955e-05, 3.493766416795552e-05, 6.952907278900966e-05, 2.6664141842047684e-05, 0.00012603384675458074, 0.00016581255476921797, 0.00025982075021602213, 0.0001619427785044536, 0.0001576567010488361, 5.4955122323008254e-05, 2.024183231696952e-05, -6.423920422093943e-05, 0.00017054102499969304, 7.241933417390101e-06, 7.876659219618887e-05, 0.0001886274985736236, 0.00031066962401382625, 0.0003808429173659533]
beta1: [0.8999999761581421, 0.8993829488754272, 0.8982502818107605, 0.897127628326416, 0.8960812091827393, 0.8953679203987122, 0.8946221470832825, 0.8943653702735901, 0.8940724730491638, 0.8933832049369812, 0.8930249214172363, 0.8923159241676331, 0.8920628428459167, 0.891884446144104, 0.8917595148086548, 0.8916007280349731, 0.8913031816482544, 0.8911025524139404, 0.8907774090766907, 0.8903812766075134, 0.8900786638259888, 0.8898184895515442, 0.8894962668418884, 0.8893085718154907, 0.8891481757164001, 0.8887799978256226, 0.8886268734931946, 0.8885452151298523, 0.8881732225418091, 0.8877224922180176, 0.8868943452835083]
beta2: [0.9900000095367432, 0.9896146059036255, 0.9900068044662476, 0.9906437397003174, 0.9915529489517212, 0.992116391658783, 0.9925804734230042, 0.9926397204399109, 0.992713212966919, 0.993162214756012, 0.9933436512947083, 0.9935669898986816, 0.9936073422431946, 0.9935138821601868, 0.993550717830658, 0.9935711622238159, 0.9936554431915283, 0.9937588572502136, 0.9939374923706055, 0.9940928816795349, 0.9942909479141235, 0.9943406581878662, 0.9943616986274719, 0.9943819046020508, 0.9944105744361877, 0.9946251511573792, 0.994691014289856, 0.9946542978286743, 0.9949156641960144, 0.9951090216636658, 0.9955606460571289]
Loss: [2.3157622805277507, 0.48701618890762327, 0.20785092267990113, 0.15466118378241858, 0.12352329047520956, 0.10547814368406931, 0.09472021952867508, 0.08904639887015024, 0.08542794930140177, 0.0839128562927246, 0.08060938745339712, 0.07616754300196965, 0.07201879560550054, 0.06904939568241437, 0.06677881755431493, 0.06559756141503652, 0.06446223528981208, 0.06396733776330948, 0.063596457234025, 0.061871456066767375, 0.05971288942893346, 0.057279534244537356, 0.055846468353271485, 0.053471389605601626, 0.051703427012761434, 0.052134174036979675, 0.05211306431889534, 0.049854957183202105, 0.04960483136574427, 0.04915493977467219, 0.048706682852904]
