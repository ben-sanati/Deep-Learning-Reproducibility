Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.47904
	Epoch[2/30]: Training Loss = 0.20223
	Epoch[3/30]: Training Loss = 0.15006
	Epoch[4/30]: Training Loss = 0.11954
	Epoch[5/30]: Training Loss = 0.10122
	Epoch[6/30]: Training Loss = 0.08890
	Epoch[7/30]: Training Loss = 0.07890
	Epoch[8/30]: Training Loss = 0.07206
	Epoch[9/30]: Training Loss = 0.06555
	Epoch[10/30]: Training Loss = 0.06293
	Epoch[11/30]: Training Loss = 0.06246
	Epoch[12/30]: Training Loss = 0.06215
	Epoch[13/30]: Training Loss = 0.05999
	Epoch[14/30]: Training Loss = 0.05990
	Epoch[15/30]: Training Loss = 0.05907
	Epoch[16/30]: Training Loss = 0.05829
	Epoch[17/30]: Training Loss = 0.05774
	Epoch[18/30]: Training Loss = 0.05746
	Epoch[19/30]: Training Loss = 0.05783
	Epoch[20/30]: Training Loss = 0.05712
	Epoch[21/30]: Training Loss = 0.05723
	Epoch[22/30]: Training Loss = 0.05708
	Epoch[23/30]: Training Loss = 0.05684
	Epoch[24/30]: Training Loss = 0.05635
	Epoch[25/30]: Training Loss = 0.05600
	Epoch[26/30]: Training Loss = 0.05570
	Epoch[27/30]: Training Loss = 0.05586
	Epoch[28/30]: Training Loss = 0.05570
	Epoch[29/30]: Training Loss = 0.05553
	Epoch[30/30]: Training Loss = 0.05483
***Training Complete***

Final Optimizer Parameters
	alpha : 0.00012572253763210028
	beta1 : 0.8905144333839417
	beta2 : 0.9942193627357483

***Testing Results***
==============================
Test Accuracy = 97.370 %
Test Error = 2.630 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0014170383801683784, 0.0010549911530688405, 0.0009301332174800336, 0.0007697014953009784, 0.0006596380844712257, 0.0005221253377385437, 0.0004404979117680341, 0.0002816304040607065, 1.9781429728027433e-05, 0.0001612438791198656, 0.0001012528664432466, 5.886960934731178e-05, 1.4925590221537277e-05, 0.0001452070864615962, 0.00011831316805910319, 2.22655671677785e-05, 1.4576764442608692e-05, 4.706351683125831e-05, -3.270593879278749e-05, 2.839316221070476e-05, 4.246026946930215e-06, 5.109759513288736e-07, 2.240888170490507e-05, 7.607758016092703e-05, 1.1812446246040054e-05, 5.435435741674155e-05, 2.6773788704304025e-05, 6.83476246194914e-05, 6.957202276680619e-05, 0.00012572253763210028]
beta1: [0.8999999761581421, 0.899071991443634, 0.8977088928222656, 0.8963785171508789, 0.8953636288642883, 0.8942186236381531, 0.8934335112571716, 0.8927580714225769, 0.8918798565864563, 0.8917574882507324, 0.8915190696716309, 0.8913271427154541, 0.8909832239151001, 0.8908302187919617, 0.8908669948577881, 0.8907294273376465, 0.8907003998756409, 0.890618622303009, 0.8906248211860657, 0.8905662894248962, 0.8904962539672852, 0.8905496597290039, 0.8906271457672119, 0.8905637860298157, 0.8904475569725037, 0.8903727531433105, 0.8903560638427734, 0.8905256390571594, 0.8906070590019226, 0.8904556632041931, 0.8905144333839417]
beta2: [0.9900000095367432, 0.9895849823951721, 0.9901112914085388, 0.990929365158081, 0.9915950894355774, 0.9925920367240906, 0.9930288195610046, 0.9934142827987671, 0.9939109683036804, 0.9939128756523132, 0.9940333962440491, 0.9940758347511292, 0.9942042827606201, 0.9942137002944946, 0.9943172931671143, 0.9943198561668396, 0.9942949414253235, 0.9942616820335388, 0.9942867755889893, 0.9942967891693115, 0.9943193793296814, 0.9942813515663147, 0.994255542755127, 0.9942512512207031, 0.9942764043807983, 0.9942976236343384, 0.9942830204963684, 0.9941819310188293, 0.9941276907920837, 0.9942435026168823, 0.9942193627357483]
Loss: [2.3046654697418214, 0.4790439720630646, 0.20223129860957464, 0.1500557394425074, 0.11954323161840438, 0.10122015693982442, 0.08889511650800705, 0.0789038311411937, 0.0720578834493955, 0.06554768615563711, 0.06292913487652937, 0.06246325213511785, 0.06215117057959239, 0.05998946350018183, 0.059900443478425346, 0.05907232971986135, 0.0582854385972023, 0.05773803150455157, 0.05745908895730972, 0.057830994127194085, 0.057121100405852, 0.0572277860224247, 0.057081226720412576, 0.05683989956180255, 0.05635264270802339, 0.05599568459590276, 0.05570456320246061, 0.05586356518864632, 0.05570080180565516, 0.05553444973826408, 0.05482695590058963]
