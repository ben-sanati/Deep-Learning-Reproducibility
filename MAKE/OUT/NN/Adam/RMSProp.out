Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.48507
	Epoch[2/30]: Training Loss = 0.19297
	Epoch[3/30]: Training Loss = 0.13993
	Epoch[4/30]: Training Loss = 0.11134
	Epoch[5/30]: Training Loss = 0.09623
	Epoch[6/30]: Training Loss = 0.08760
	Epoch[7/30]: Training Loss = 0.08330
	Epoch[8/30]: Training Loss = 0.07920
	Epoch[9/30]: Training Loss = 0.07604
	Epoch[10/30]: Training Loss = 0.07434
	Epoch[11/30]: Training Loss = 0.07316
	Epoch[12/30]: Training Loss = 0.07281
	Epoch[13/30]: Training Loss = 0.07218
	Epoch[14/30]: Training Loss = 0.07140
	Epoch[15/30]: Training Loss = 0.07146
	Epoch[16/30]: Training Loss = 0.06955
	Epoch[17/30]: Training Loss = 0.06895
	Epoch[18/30]: Training Loss = 0.06905
	Epoch[19/30]: Training Loss = 0.06824
	Epoch[20/30]: Training Loss = 0.06682
	Epoch[21/30]: Training Loss = 0.06680
	Epoch[22/30]: Training Loss = 0.06643
	Epoch[23/30]: Training Loss = 0.06626
	Epoch[24/30]: Training Loss = 0.06583
	Epoch[25/30]: Training Loss = 0.06649
	Epoch[26/30]: Training Loss = 0.06480
	Epoch[27/30]: Training Loss = 0.06334
	Epoch[28/30]: Training Loss = 0.06197
	Epoch[29/30]: Training Loss = 0.05989
	Epoch[30/30]: Training Loss = 0.06012
***Training Complete***

Final Optimizer Parameters
	alpha : 7.789897063048556e-05
	beta1 : 0.8921222686767578
	beta2 : 0.9932481050491333

***Testing Results***
==============================
Test Accuracy = 97.290 %
Test Error = 2.710 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0015113446861505508, 0.0012698406353592873, 0.0009422065922990441, 0.0005397835629992187, 0.0004008584946859628, 0.00025667197769507766, 0.00023303396301344037, 0.00011696902947733179, 2.4925229809014127e-05, 3.5868662962457165e-05, 7.746862684143707e-05, 6.29639471299015e-05, 3.3329808502458036e-05, 0.0001298373972531408, 0.00012076324492227286, 6.384669541148469e-05, 3.998461033916101e-05, 0.00015351500769611448, 1.250631976290606e-05, -8.392335985263344e-06, -3.046411438845098e-05, 3.178074621246196e-05, 2.4861637939466164e-05, 5.011020039091818e-05, 0.00013480540656019002, 0.0001258285774383694, 0.0001238588010892272, 4.512882514973171e-05, 0.00011250093666603789, 7.789897063048556e-05]
beta1: [0.8999999761581421, 0.899136483669281, 0.897669792175293, 0.8964722156524658, 0.8955978751182556, 0.8952005505561829, 0.8949442505836487, 0.8945223093032837, 0.894108772277832, 0.8939756155014038, 0.8937952518463135, 0.8939998745918274, 0.8939520716667175, 0.8938143253326416, 0.8937374353408813, 0.8935554027557373, 0.8936183452606201, 0.8937092423439026, 0.8936352133750916, 0.8937425017356873, 0.8938981294631958, 0.8938060998916626, 0.8938227891921997, 0.8939740657806396, 0.8939240574836731, 0.8933432698249817, 0.8929451107978821, 0.8926840424537659, 0.8921635746955872, 0.8921754360198975, 0.8921222686767578]
beta2: [0.9900000095367432, 0.9894924759864807, 0.9899305701255798, 0.9907530546188354, 0.9915897250175476, 0.9919247627258301, 0.9920451641082764, 0.9923037886619568, 0.9924214482307434, 0.992484986782074, 0.9925786852836609, 0.9925332069396973, 0.9925519824028015, 0.992668628692627, 0.992680013179779, 0.9927801489830017, 0.9927186965942383, 0.9927113056182861, 0.9926809072494507, 0.9926590323448181, 0.9926527738571167, 0.9926914572715759, 0.9926828742027283, 0.9926098585128784, 0.9926208257675171, 0.9930102825164795, 0.9931321740150452, 0.9931294918060303, 0.993262529373169, 0.9932981133460999, 0.9932481050491333]
Loss: [2.3026538731892905, 0.4850743087609609, 0.19296568950017293, 0.13993218009471894, 0.1113379760424296, 0.09623345620234808, 0.08760107373197873, 0.0832959278066953, 0.07919910778303942, 0.07603632340033849, 0.07433518623014292, 0.07315606503486634, 0.07281041538119316, 0.07217574338118235, 0.07140230416854222, 0.07145729634563128, 0.0695511782248815, 0.06895252419908841, 0.06904935281276703, 0.06824226623773574, 0.06682209022641182, 0.0667988207479318, 0.06643035916090012, 0.06625858890016874, 0.06582789381742478, 0.06649057089090347, 0.0648005664229393, 0.0633415113846461, 0.061972798904776576, 0.05988587884704272, 0.06011703731417656]
