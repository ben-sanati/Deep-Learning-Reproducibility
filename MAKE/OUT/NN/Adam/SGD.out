Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.33561
	Epoch[2/30]: Training Loss = 0.12223
	Epoch[3/30]: Training Loss = 0.07862
	Epoch[4/30]: Training Loss = 0.05635
	Epoch[5/30]: Training Loss = 0.04161
	Epoch[6/30]: Training Loss = 0.03100
	Epoch[7/30]: Training Loss = 0.02405
	Epoch[8/30]: Training Loss = 0.01821
	Epoch[9/30]: Training Loss = 0.01400
	Epoch[10/30]: Training Loss = 0.01140
	Epoch[11/30]: Training Loss = 0.00914
	Epoch[12/30]: Training Loss = 0.00698
	Epoch[13/30]: Training Loss = 0.00582
	Epoch[14/30]: Training Loss = 0.00414
	Epoch[15/30]: Training Loss = 0.00350
	Epoch[16/30]: Training Loss = 0.00270
	Epoch[17/30]: Training Loss = 0.00221
	Epoch[18/30]: Training Loss = 0.00186
	Epoch[19/30]: Training Loss = 0.00154
	Epoch[20/30]: Training Loss = 0.00117
	Epoch[21/30]: Training Loss = 0.00096
	Epoch[22/30]: Training Loss = 0.00088
	Epoch[23/30]: Training Loss = 0.00071
	Epoch[24/30]: Training Loss = 0.00064
	Epoch[25/30]: Training Loss = 0.00050
	Epoch[26/30]: Training Loss = 0.00038
	Epoch[27/30]: Training Loss = 0.00035
	Epoch[28/30]: Training Loss = 0.00028
	Epoch[29/30]: Training Loss = 0.00025
	Epoch[30/30]: Training Loss = 0.00021
***Training Complete***

Final Optimizer Parameters
	alpha : 0.000685102422721684
	beta1 : 0.8997567296028137
	beta2 : 0.9897270798683167

***Testing Results***
==============================
Test Accuracy = 97.920 %
Test Error = 2.080 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.004448756575584412, 0.003786694724112749, 0.00321192666888237, 0.0027293541934341192, 0.0023607416078448296, 0.0021320723462849855, 0.0018561400938779116, 0.001686764182522893, 0.0015207675751298666, 0.0014544748701155186, 0.0013271963689476252, 0.0012525811325758696, 0.0011550781782716513, 0.0010890407720580697, 0.0010121683590114117, 0.0009568290552124381, 0.0009331774199381471, 0.0008971953648142517, 0.0008722517522983253, 0.000836870982311666, 0.000815827283076942, 0.0008011036552488804, 0.0007759411819279194, 0.0007433930295519531, 0.0007391318213194609, 0.0007284078747034073, 0.0007099053473211825, 0.0007010927074588835, 0.0006942942272871733, 0.000685102422721684]
beta1: [0.8999999761581421, 0.8999117016792297, 0.8998534679412842, 0.8998222947120667, 0.8998030424118042, 0.8997905850410461, 0.899782657623291, 0.899776041507721, 0.899770200252533, 0.8997673988342285, 0.8997641205787659, 0.899760901927948, 0.8997591733932495, 0.8997582793235779, 0.8997581005096436, 0.8997578024864197, 0.8997576236724854, 0.899757444858551, 0.8997570872306824, 0.899756908416748, 0.899756908416748, 0.8997568488121033, 0.8997567892074585, 0.8997567892074585, 0.8997567296028137, 0.8997567296028137, 0.8997567296028137, 0.8997567296028137, 0.8997567296028137, 0.8997567296028137, 0.8997567296028137]
beta2: [0.9900000095367432, 0.9896917939186096, 0.9897139668464661, 0.9897191524505615, 0.98972088098526, 0.9897221326828003, 0.9897225499153137, 0.9897229075431824, 0.9897236227989197, 0.9897239804267883, 0.9897246956825256, 0.9897251725196838, 0.9897258877754211, 0.9897260665893555, 0.9897260665893555, 0.9897262454032898, 0.9897264242172241, 0.9897264838218689, 0.9897267818450928, 0.9897269010543823, 0.9897269010543823, 0.9897269010543823, 0.9897269606590271, 0.9897269606590271, 0.9897270798683167, 0.9897270798683167, 0.9897270798683167, 0.9897270798683167, 0.9897270798683167, 0.9897270798683167, 0.9897270798683167]
Loss: [2.312411604690552, 0.3356148347059886, 0.12222836854060491, 0.07862414841453234, 0.05635107348759969, 0.04161103793382644, 0.030997510117292404, 0.024053669020036857, 0.018205930302912992, 0.013995238793889682, 0.011398416339357694, 0.009140291909811398, 0.006977986712257067, 0.005819116043796142, 0.004141627891610066, 0.0034953130008031925, 0.002703904267462591, 0.002210060053318739, 0.0018580553560207287, 0.001541209274965028, 0.001168088032429417, 0.0009615629576146602, 0.0008791440464245776, 0.0007088656179335278, 0.0006386306898047527, 0.0004973927157620589, 0.0003754590634101381, 0.0003474300560541451, 0.00028154322392850495, 0.00024708276641710353, 0.0002089551659844195]
