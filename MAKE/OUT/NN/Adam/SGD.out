Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.32781
	Epoch[2/30]: Training Loss = 0.11852
	Epoch[3/30]: Training Loss = 0.07804
	Epoch[4/30]: Training Loss = 0.05523
	Epoch[5/30]: Training Loss = 0.04157
	Epoch[6/30]: Training Loss = 0.03167
	Epoch[7/30]: Training Loss = 0.02355
	Epoch[8/30]: Training Loss = 0.01841
	Epoch[9/30]: Training Loss = 0.01364
	Epoch[10/30]: Training Loss = 0.01096
	Epoch[11/30]: Training Loss = 0.00895
	Epoch[12/30]: Training Loss = 0.00697
	Epoch[13/30]: Training Loss = 0.00521
	Epoch[14/30]: Training Loss = 0.00411
	Epoch[15/30]: Training Loss = 0.00315
	Epoch[16/30]: Training Loss = 0.00263
	Epoch[17/30]: Training Loss = 0.00208
	Epoch[18/30]: Training Loss = 0.00176
	Epoch[19/30]: Training Loss = 0.00166
	Epoch[20/30]: Training Loss = 0.00105
	Epoch[21/30]: Training Loss = 0.00122
	Epoch[22/30]: Training Loss = 0.00071
	Epoch[23/30]: Training Loss = 0.00065
	Epoch[24/30]: Training Loss = 0.00056
	Epoch[25/30]: Training Loss = 0.00045
	Epoch[26/30]: Training Loss = 0.00040
	Epoch[27/30]: Training Loss = 0.00035
	Epoch[28/30]: Training Loss = 0.00022
	Epoch[29/30]: Training Loss = 0.00026
	Epoch[30/30]: Training Loss = 0.00018
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0008587075280956924
	beta1 : 0.899735689163208
	beta2 : 0.9897190928459167

***Testing Results***
==============================
Test Accuracy = 98.160 %
Test Error = 1.840 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.004615527577698231, 0.0038163147401064634, 0.003237144323065877, 0.002861450193449855, 0.0025316434912383556, 0.0022334072273224592, 0.001971040153875947, 0.001799938501790166, 0.0015976299764588475, 0.00152486318256706, 0.0014208178035914898, 0.0013528179842978716, 0.0012582566123455763, 0.0011612781090661883, 0.0011026490246877074, 0.0010578662622720003, 0.001040601055137813, 0.0010219365358352661, 0.001020230120047927, 0.0010038571199402213, 0.0009488066425547004, 0.000942933838814497, 0.0009421100839972496, 0.0009216971229761839, 0.000906326633412391, 0.0008874420891515911, 0.0008770739659667015, 0.0008733684080652893, 0.0008595287217758596, 0.0008587075280956924]
beta1: [0.8999999761581421, 0.8999065160751343, 0.8998487591743469, 0.8998151421546936, 0.8997955322265625, 0.899776041507721, 0.89976567029953, 0.8997575640678406, 0.8997510075569153, 0.8997471928596497, 0.8997431397438049, 0.8997411727905273, 0.899739146232605, 0.8997383117675781, 0.8997379541397095, 0.8997374176979065, 0.8997371792793274, 0.8997368812561035, 0.8997367024421692, 0.8997364044189453, 0.8997363448143005, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208, 0.899735689163208]
beta2: [0.9900000095367432, 0.9896776080131531, 0.9897006750106812, 0.9897061586380005, 0.9897087216377258, 0.9897116422653198, 0.989712655544281, 0.9897133708000183, 0.9897143244743347, 0.9897149801254272, 0.989716112613678, 0.9897163510322571, 0.9897168278694153, 0.9897170066833496, 0.9897170662879944, 0.9897171258926392, 0.9897171854972839, 0.9897173643112183, 0.9897176623344421, 0.9897179007530212, 0.9897179007530212, 0.9897189140319824, 0.9897189140319824, 0.9897189140319824, 0.9897189736366272, 0.9897189736366272, 0.989719033241272, 0.9897190928459167, 0.9897190928459167, 0.9897190928459167, 0.9897190928459167]
Loss: [2.3046654697418214, 0.3278064351638158, 0.11851731337706248, 0.07804059961636861, 0.05522565175890923, 0.041566541874408724, 0.03166727360785007, 0.02354795295757552, 0.01840839780519406, 0.013644205290079117, 0.010957663047065337, 0.008946639354526997, 0.0069729566343128685, 0.005212625340869029, 0.004106913382063309, 0.003149204743032654, 0.002634196300742527, 0.0020769071123873193, 0.001757405601007243, 0.0016648472105463346, 0.0010454537872225046, 0.0012249853949993848, 0.0007095516245812178, 0.0006463718482137968, 0.0005636605881309758, 0.0004487200793654968, 0.0003950094115920365, 0.00035212113857269287, 0.00021624236496475835, 0.0002646489134368797, 0.00017868582736700774]
