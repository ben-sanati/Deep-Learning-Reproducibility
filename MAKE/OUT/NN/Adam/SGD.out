Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.33803
	Epoch[2/30]: Training Loss = 0.12727
	Epoch[3/30]: Training Loss = 0.08266
	Epoch[4/30]: Training Loss = 0.05800
	Epoch[5/30]: Training Loss = 0.04164
	Epoch[6/30]: Training Loss = 0.03248
	Epoch[7/30]: Training Loss = 0.02489
	Epoch[8/30]: Training Loss = 0.01961
	Epoch[9/30]: Training Loss = 0.01502
	Epoch[10/30]: Training Loss = 0.01156
	Epoch[11/30]: Training Loss = 0.00928
	Epoch[12/30]: Training Loss = 0.00688
	Epoch[13/30]: Training Loss = 0.00592
	Epoch[14/30]: Training Loss = 0.00418
	Epoch[15/30]: Training Loss = 0.00325
	Epoch[16/30]: Training Loss = 0.00268
	Epoch[17/30]: Training Loss = 0.00218
	Epoch[18/30]: Training Loss = 0.00180
	Epoch[19/30]: Training Loss = 0.00132
	Epoch[20/30]: Training Loss = 0.00121
	Epoch[21/30]: Training Loss = 0.00096
	Epoch[22/30]: Training Loss = 0.00088
	Epoch[23/30]: Training Loss = 0.00055
	Epoch[24/30]: Training Loss = 0.00053
	Epoch[25/30]: Training Loss = 0.00050
	Epoch[26/30]: Training Loss = 0.00035
	Epoch[27/30]: Training Loss = 0.00033
	Epoch[28/30]: Training Loss = 0.00025
	Epoch[29/30]: Training Loss = 0.00022
	Epoch[30/30]: Training Loss = 0.00018
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0007817455334588885
	beta1 : 0.8997503519058228
	beta2 : 0.9897286891937256

***Testing Results***
==============================
Test Accuracy = 98.010 %
Test Error = 1.990 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.004399874713271856, 0.0037234609480947256, 0.0031877874862402678, 0.002635478274896741, 0.0022822667378932238, 0.0020652515813708305, 0.0019114641472697258, 0.0017741654301062226, 0.0016340550500899553, 0.0015217151958495378, 0.00144370982889086, 0.0013072467409074306, 0.001205960987135768, 0.0011516375234350562, 0.001085422933101654, 0.0010182126425206661, 0.0009755865903571248, 0.000958107179030776, 0.0009207177208736539, 0.0008807580452412367, 0.0008609454380348325, 0.0008507269085384905, 0.000844005320686847, 0.000828655029181391, 0.0008190862718038261, 0.0008049291791394353, 0.000801407266408205, 0.0007918332703411579, 0.0007860027253627777, 0.0007817455334588885]
beta1: [0.8999999761581421, 0.8999208211898804, 0.8998568058013916, 0.899823009967804, 0.8998051881790161, 0.8997925519943237, 0.8997809886932373, 0.8997713923454285, 0.8997642397880554, 0.8997594714164734, 0.899756133556366, 0.8997536301612854, 0.8997528553009033, 0.8997520804405212, 0.8997519016265869, 0.8997514843940735, 0.8997513055801392, 0.8997510075569153, 0.899750828742981, 0.899750828742981, 0.899750828742981, 0.8997507691383362, 0.8997504711151123, 0.8997504711151123, 0.8997504711151123, 0.8997503519058228, 0.8997503519058228, 0.8997503519058228, 0.8997503519058228, 0.8997503519058228, 0.8997503519058228]
beta2: [0.9900000095367432, 0.989693820476532, 0.9897155165672302, 0.9897205829620361, 0.9897221922874451, 0.989722728729248, 0.9897239804267883, 0.9897252917289734, 0.9897263050079346, 0.9897267818450928, 0.9897271990776062, 0.9897275567054749, 0.9897276163101196, 0.9897274971008301, 0.9897274971008301, 0.9897277355194092, 0.989727795124054, 0.9897280335426331, 0.9897280931472778, 0.9897280931472778, 0.9897281527519226, 0.9897282123565674, 0.9897285103797913, 0.9897285103797913, 0.9897285103797913, 0.9897286891937256, 0.9897286891937256, 0.9897286891937256, 0.9897286891937256, 0.9897286891937256, 0.9897286891937256]
Loss: [2.2930675029754637, 0.3380258026123047, 0.1272710177063942, 0.08266015512943267, 0.05799940569003423, 0.041639960875113805, 0.03248144144614538, 0.024893496473630268, 0.01961146965275208, 0.015015795232603947, 0.011559515885263681, 0.009280081701713305, 0.006876725071668625, 0.00591515890955925, 0.0041795723132789135, 0.003251243279998501, 0.002684361386174957, 0.002179219655630489, 0.001801993548947697, 0.001319901091977954, 0.0012074027653162678, 0.0009578060013242065, 0.0008817048186125855, 0.000552314319089055, 0.0005275543150181571, 0.0004996924645733088, 0.0003544217137309412, 0.0003250921768291543, 0.0002451272023220857, 0.00022079726074201365, 0.00018346155397205925]
