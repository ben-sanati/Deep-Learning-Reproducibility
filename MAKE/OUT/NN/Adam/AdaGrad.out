Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.51024
	Epoch[2/30]: Training Loss = 0.22291
	Epoch[3/30]: Training Loss = 0.16485
	Epoch[4/30]: Training Loss = 0.12814
	Epoch[5/30]: Training Loss = 0.10397
	Epoch[6/30]: Training Loss = 0.08756
	Epoch[7/30]: Training Loss = 0.07289
	Epoch[8/30]: Training Loss = 0.06324
	Epoch[9/30]: Training Loss = 0.05353
	Epoch[10/30]: Training Loss = 0.04671
	Epoch[11/30]: Training Loss = 0.04082
	Epoch[12/30]: Training Loss = 0.03550
	Epoch[13/30]: Training Loss = 0.03088
	Epoch[14/30]: Training Loss = 0.02688
	Epoch[15/30]: Training Loss = 0.02343
	Epoch[16/30]: Training Loss = 0.02039
	Epoch[17/30]: Training Loss = 0.01783
	Epoch[18/30]: Training Loss = 0.01522
	Epoch[19/30]: Training Loss = 0.01418
	Epoch[20/30]: Training Loss = 0.01196
	Epoch[21/30]: Training Loss = 0.01012
	Epoch[22/30]: Training Loss = 0.00892
	Epoch[23/30]: Training Loss = 0.00787
	Epoch[24/30]: Training Loss = 0.00656
	Epoch[25/30]: Training Loss = 0.00626
	Epoch[26/30]: Training Loss = 0.00542
	Epoch[27/30]: Training Loss = 0.00494
	Epoch[28/30]: Training Loss = 0.00367
	Epoch[29/30]: Training Loss = 0.00344
	Epoch[30/30]: Training Loss = 0.00322
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010750957299023867
	beta1 : 0.8999189734458923
	beta2 : 0.9898840188980103

***Testing Results***
==============================
Test Accuracy = 97.750 %
Test Error = 2.250 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0011049904860556126, 0.0011028113076463342, 0.0011016634525731206, 0.0011003260733559728, 0.0010989452712237835, 0.0010981359519064426, 0.0010963556123897433, 0.001095023937523365, 0.0010929715353995562, 0.0010916036553680897, 0.001090203644707799, 0.0010889462428167462, 0.0010874723084270954, 0.0010858060559257865, 0.0010845753131434321, 0.001083261682651937, 0.0010821996256709099, 0.001081129303202033, 0.0010806843638420105, 0.0010800115996971726, 0.0010793564142659307, 0.0010786313796415925, 0.0010779909789562225, 0.0010773028479889035, 0.001076828921213746, 0.0010765361366793513, 0.0010763595346361399, 0.0010759872384369373, 0.001075515290722251, 0.0010750957299023867]
beta1: [0.8999999761581421, 0.9000500440597534, 0.9000349640846252, 0.9000194072723389, 0.9000110626220703, 0.9000014066696167, 0.8999917507171631, 0.8999869227409363, 0.8999789953231812, 0.8999746441841125, 0.8999694585800171, 0.8999645709991455, 0.8999592661857605, 0.8999546766281128, 0.8999511003494263, 0.8999473452568054, 0.8999444246292114, 0.8999413847923279, 0.8999397158622742, 0.8999354839324951, 0.899932861328125, 0.899930477142334, 0.8999291658401489, 0.8999274969100952, 0.8999263048171997, 0.8999239802360535, 0.8999221920967102, 0.8999208211898804, 0.8999204635620117, 0.8999198079109192, 0.8999189734458923]
beta2: [0.9900000095367432, 0.9898837804794312, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898839592933655, 0.9898840188980103, 0.9898840188980103, 0.9898840188980103, 0.9898840188980103]
Loss: [2.3046654697418214, 0.5102353168328603, 0.22291377308368682, 0.16485422972838085, 0.12814392420450846, 0.10397205069859823, 0.08755649684071541, 0.07288752883076668, 0.06323550408283869, 0.05353128730853399, 0.04671454244703054, 0.04081692421038945, 0.03549917780061563, 0.030876699177920817, 0.026876255836089453, 0.02343197289953629, 0.020389935648441316, 0.017830453646183013, 0.015223043409734965, 0.01417899550696214, 0.01195511519908905, 0.010119866641859213, 0.008917463434487582, 0.00787465065891544, 0.006556811195611954, 0.006259094872884452, 0.005423160012563069, 0.004941091576094429, 0.0036733895236626267, 0.003443166451094051, 0.0032186061092962823]
