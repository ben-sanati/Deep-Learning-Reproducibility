Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.51783
	Epoch[2/30]: Training Loss = 0.22770
	Epoch[3/30]: Training Loss = 0.16989
	Epoch[4/30]: Training Loss = 0.13200
	Epoch[5/30]: Training Loss = 0.10707
	Epoch[6/30]: Training Loss = 0.08814
	Epoch[7/30]: Training Loss = 0.07470
	Epoch[8/30]: Training Loss = 0.06391
	Epoch[9/30]: Training Loss = 0.05431
	Epoch[10/30]: Training Loss = 0.04768
	Epoch[11/30]: Training Loss = 0.04135
	Epoch[12/30]: Training Loss = 0.03654
	Epoch[13/30]: Training Loss = 0.03209
	Epoch[14/30]: Training Loss = 0.02755
	Epoch[15/30]: Training Loss = 0.02340
	Epoch[16/30]: Training Loss = 0.02119
	Epoch[17/30]: Training Loss = 0.01835
	Epoch[18/30]: Training Loss = 0.01562
	Epoch[19/30]: Training Loss = 0.01384
	Epoch[20/30]: Training Loss = 0.01220
	Epoch[21/30]: Training Loss = 0.01022
	Epoch[22/30]: Training Loss = 0.00873
	Epoch[23/30]: Training Loss = 0.00820
	Epoch[24/30]: Training Loss = 0.00689
	Epoch[25/30]: Training Loss = 0.00602
	Epoch[26/30]: Training Loss = 0.00501
	Epoch[27/30]: Training Loss = 0.00470
	Epoch[28/30]: Training Loss = 0.00374
	Epoch[29/30]: Training Loss = 0.00330
	Epoch[30/30]: Training Loss = 0.00322
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010743802413344383
	beta1 : 0.8999106884002686
	beta2 : 0.989881694316864

***Testing Results***
==============================
Test Accuracy = 97.820 %
Test Error = 2.180 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.00110704789403826, 0.0011055466020479798, 0.0011041767429560423, 0.0011017430806532502, 0.0011003244435414672, 0.0010980934603139758, 0.0010962357046082616, 0.0010943239321932197, 0.0010924917878583074, 0.0010909446282312274, 0.001089477795176208, 0.001088468823581934, 0.0010876200394704938, 0.0010863540228456259, 0.0010848381789401174, 0.001084130839444697, 0.0010831543477252126, 0.0010820234892889857, 0.0010811854153871536, 0.0010805315105244517, 0.0010796397691592574, 0.0010786502389237285, 0.0010780802695080638, 0.0010776084382086992, 0.0010770285734906793, 0.0010761530138552189, 0.0010758232092484832, 0.001075148698873818, 0.001074679777957499, 0.0010743802413344383]
beta1: [0.8999999761581421, 0.9000539779663086, 0.9000347852706909, 0.9000187516212463, 0.9000080823898315, 0.8999977111816406, 0.8999908566474915, 0.8999839425086975, 0.8999778032302856, 0.8999724984169006, 0.899965763092041, 0.8999593257904053, 0.8999542593955994, 0.8999481797218323, 0.8999443650245667, 0.8999412655830383, 0.8999370336532593, 0.899933934211731, 0.899931013584137, 0.8999273777008057, 0.8999249339103699, 0.8999226689338684, 0.8999217748641968, 0.8999198079109192, 0.8999180197715759, 0.8999161720275879, 0.8999151587486267, 0.8999137878417969, 0.89991295337677, 0.8999117612838745, 0.8999106884002686]
beta2: [0.9900000095367432, 0.9898815155029297, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864, 0.989881694316864]
Loss: [2.30346203511556, 0.5178279337485632, 0.2276995834986369, 0.16988877246777218, 0.13200490491787592, 0.107070640651385, 0.08814433661500613, 0.07469724818468094, 0.06390773491660753, 0.05430780066251755, 0.047679124212265014, 0.041349680948257445, 0.036544540070494014, 0.032086034627755486, 0.02754980482359727, 0.023397224738200505, 0.02118782351190845, 0.01835373794734478, 0.015617471688240767, 0.013841196632881959, 0.012199095530311267, 0.010223159971336524, 0.008727226009219885, 0.00819976175452272, 0.006893434736629327, 0.006019118837515513, 0.005008892095399399, 0.004696752170907955, 0.003738247781371077, 0.0032989507706214986, 0.0032177833788096903]
