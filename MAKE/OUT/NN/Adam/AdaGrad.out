Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.51868
	Epoch[2/30]: Training Loss = 0.22450
	Epoch[3/30]: Training Loss = 0.16351
	Epoch[4/30]: Training Loss = 0.12799
	Epoch[5/30]: Training Loss = 0.10433
	Epoch[6/30]: Training Loss = 0.08831
	Epoch[7/30]: Training Loss = 0.07508
	Epoch[8/30]: Training Loss = 0.06432
	Epoch[9/30]: Training Loss = 0.05617
	Epoch[10/30]: Training Loss = 0.04948
	Epoch[11/30]: Training Loss = 0.04356
	Epoch[12/30]: Training Loss = 0.03751
	Epoch[13/30]: Training Loss = 0.03298
	Epoch[14/30]: Training Loss = 0.02943
	Epoch[15/30]: Training Loss = 0.02562
	Epoch[16/30]: Training Loss = 0.02213
	Epoch[17/30]: Training Loss = 0.01947
	Epoch[18/30]: Training Loss = 0.01706
	Epoch[19/30]: Training Loss = 0.01405
	Epoch[20/30]: Training Loss = 0.01368
	Epoch[21/30]: Training Loss = 0.01129
	Epoch[22/30]: Training Loss = 0.00989
	Epoch[23/30]: Training Loss = 0.00852
	Epoch[24/30]: Training Loss = 0.00748
	Epoch[25/30]: Training Loss = 0.00691
	Epoch[26/30]: Training Loss = 0.00564
	Epoch[27/30]: Training Loss = 0.00450
	Epoch[28/30]: Training Loss = 0.00496
	Epoch[29/30]: Training Loss = 0.00378
	Epoch[30/30]: Training Loss = 0.00304
***Training Complete***

Final Optimizer Parameters
	alpha : 0.001078530098311603
	beta1 : 0.8999063372612
	beta2 : 0.9898800849914551

***Testing Results***
==============================
Test Accuracy = 97.820 %
Test Error = 2.180 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0011077008675783873, 0.0011069506872445345, 0.0011048512533307076, 0.0011025674175471067, 0.0011008544825017452, 0.0010997523786500096, 0.0010979407234117389, 0.001095961662940681, 0.001094612292945385, 0.0010935968020930886, 0.001092985738068819, 0.0010914461454376578, 0.0010899000335484743, 0.001089144148863852, 0.0010879912879317999, 0.0010866023367270827, 0.0010854400461539626, 0.001084530376829207, 0.0010834467830136418, 0.001082930131815374, 0.0010823297780007124, 0.0010818095179274678, 0.0010813047410920262, 0.0010808558436110616, 0.0010804596822708845, 0.0010800918098539114, 0.0010794380214065313, 0.0010791367385536432, 0.0010788002982735634, 0.001078530098311603]
beta1: [0.8999999761581421, 0.9000498652458191, 0.9000309109687805, 0.9000183343887329, 0.9000105261802673, 0.9000025391578674, 0.8999932408332825, 0.8999863862991333, 0.8999792337417603, 0.8999730944633484, 0.8999655842781067, 0.8999581933021545, 0.8999541401863098, 0.8999501466751099, 0.8999451398849487, 0.8999409079551697, 0.8999374508857727, 0.8999332189559937, 0.8999292850494385, 0.8999276161193848, 0.8999239206314087, 0.8999211192131042, 0.8999181985855103, 0.8999152183532715, 0.8999136686325073, 0.8999118208885193, 0.8999108672142029, 0.8999103903770447, 0.8999077081680298, 0.899906575679779, 0.8999063372612]
beta2: [0.9900000095367432, 0.9898799657821655, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800253868103, 0.9898800849914551, 0.9898800849914551, 0.9898800849914551]
Loss: [2.3127810535430906, 0.5186776940663655, 0.22450077992280323, 0.16350848846435548, 0.1279931401014328, 0.1043344028532505, 0.08831160961786906, 0.07508449728886286, 0.06432499691446622, 0.0561725687623024, 0.049479925245046616, 0.043561229790002105, 0.03751419193247954, 0.032984748872121175, 0.02942641822695732, 0.025623879344264668, 0.02213013148282965, 0.019471495586136978, 0.017060218260685604, 0.014052181799337269, 0.01368086871902148, 0.011290313945213954, 0.009893229768176873, 0.008521086802581946, 0.007481941621502241, 0.006910869147876898, 0.005638876125713189, 0.004500840290263295, 0.004960745166490475, 0.0037789644000430903, 0.003037295081652701]
