Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: 1e-05
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.51716
	Epoch[2/30]: Training Loss = 0.22455
	Epoch[3/30]: Training Loss = 0.16568
	Epoch[4/30]: Training Loss = 0.13064
	Epoch[5/30]: Training Loss = 0.10569
	Epoch[6/30]: Training Loss = 0.08879
	Epoch[7/30]: Training Loss = 0.07471
	Epoch[8/30]: Training Loss = 0.06391
	Epoch[9/30]: Training Loss = 0.05504
	Epoch[10/30]: Training Loss = 0.04819
	Epoch[11/30]: Training Loss = 0.04113
	Epoch[12/30]: Training Loss = 0.03600
	Epoch[13/30]: Training Loss = 0.03182
	Epoch[14/30]: Training Loss = 0.02726
	Epoch[15/30]: Training Loss = 0.02438
	Epoch[16/30]: Training Loss = 0.02122
	Epoch[17/30]: Training Loss = 0.01866
	Epoch[18/30]: Training Loss = 0.01694
	Epoch[19/30]: Training Loss = 0.01389
	Epoch[20/30]: Training Loss = 0.01252
	Epoch[21/30]: Training Loss = 0.01027
	Epoch[22/30]: Training Loss = 0.00886
	Epoch[23/30]: Training Loss = 0.00804
	Epoch[24/30]: Training Loss = 0.00700
	Epoch[25/30]: Training Loss = 0.00566
	Epoch[26/30]: Training Loss = 0.00520
	Epoch[27/30]: Training Loss = 0.00446
	Epoch[28/30]: Training Loss = 0.00437
	Epoch[29/30]: Training Loss = 0.00315
	Epoch[30/30]: Training Loss = 0.00329
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010775398695841432
	beta1 : 0.8999099135398865
	beta2 : 0.9898802042007446

***Testing Results***
==============================
Test Accuracy = 97.920 %
Test Error = 2.080 %
==============================
