Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.60591
	Epoch[2/30]: Training Loss = 0.32804
	Epoch[3/30]: Training Loss = 0.30143
	Epoch[4/30]: Training Loss = 0.28873
	Epoch[5/30]: Training Loss = 0.28210
	Epoch[6/30]: Training Loss = 0.27792
	Epoch[7/30]: Training Loss = 0.27531
	Epoch[8/30]: Training Loss = 0.27419
	Epoch[9/30]: Training Loss = 0.27448
	Epoch[10/30]: Training Loss = 0.27451
	Epoch[11/30]: Training Loss = 0.27247
	Epoch[12/30]: Training Loss = 0.26976
	Epoch[13/30]: Training Loss = 0.26684
	Epoch[14/30]: Training Loss = 0.26272
	Epoch[15/30]: Training Loss = 0.25731
	Epoch[16/30]: Training Loss = 0.25209
	Epoch[17/30]: Training Loss = 0.24803
	Epoch[18/30]: Training Loss = 0.24516
	Epoch[19/30]: Training Loss = 0.24301
	Epoch[20/30]: Training Loss = 0.24203
	Epoch[21/30]: Training Loss = 0.24062
	Epoch[22/30]: Training Loss = 0.23911
	Epoch[23/30]: Training Loss = 0.23715
	Epoch[24/30]: Training Loss = 0.23588
	Epoch[25/30]: Training Loss = 0.23474
	Epoch[26/30]: Training Loss = 0.23389
	Epoch[27/30]: Training Loss = 0.23348
	Epoch[28/30]: Training Loss = 0.23335
	Epoch[29/30]: Training Loss = 0.23233
	Epoch[30/30]: Training Loss = 0.23151
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0016469450201839209

***Testing Results***
==============================
Test Accuracy = 93.520 %
Test Error = 6.480 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.030514171347022057, 0.019024420529603958, 0.009132679551839828, 0.006820858456194401, 0.0043565137311816216, 0.0024872291833162308, 0.001247061532922089, -7.80800764914602e-05, -0.00021371786715462804, 0.0028852468822151423, 0.0031340518034994602, 0.0039545148611068726, 0.006584229879081249, 0.008034582249820232, 0.007673231419175863, 0.005985738709568977, 0.004175173118710518, 0.003467099042609334, 0.0014899763045832515, 0.0024775913916528225, 0.002568134106695652, 0.004103645216673613, 0.0019793708343058825, 0.0018633648287504911, 0.001341684372164309, 2.3551230697194114e-05, 0.0003392047365196049, 0.0027255555614829063, 7.199311221484095e-05, 0.0016469450201839209]
Loss: [2.3055778498331705, 0.605907983191808, 0.32803803627490996, 0.3014340703805288, 0.2887308526833852, 0.28209922816753386, 0.2779196228186289, 0.2753123425404231, 0.2741945601304372, 0.27447848025957744, 0.27450990509192147, 0.2724730105717977, 0.2697621299982071, 0.2668409150918325, 0.2627168829600016, 0.25730899683237074, 0.2520922289133072, 0.24802767073313395, 0.2451558828910192, 0.24301421990791958, 0.24202941761016847, 0.24062328156630197, 0.23911170738538107, 0.23714768668810526, 0.23587634706497193, 0.23473664793968202, 0.23388605330785114, 0.23348275673389435, 0.23334884078502655, 0.23232891435623168, 0.23151050068537393]