Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.59108
	Epoch[2/30]: Training Loss = 0.32622
	Epoch[3/30]: Training Loss = 0.30487
	Epoch[4/30]: Training Loss = 0.29337
	Epoch[5/30]: Training Loss = 0.28495
	Epoch[6/30]: Training Loss = 0.27771
	Epoch[7/30]: Training Loss = 0.27261
	Epoch[8/30]: Training Loss = 0.26833
	Epoch[9/30]: Training Loss = 0.26399
	Epoch[10/30]: Training Loss = 0.25987
	Epoch[11/30]: Training Loss = 0.25559
	Epoch[12/30]: Training Loss = 0.25139
	Epoch[13/30]: Training Loss = 0.24759
	Epoch[14/30]: Training Loss = 0.24497
	Epoch[15/30]: Training Loss = 0.24239
	Epoch[16/30]: Training Loss = 0.23939
	Epoch[17/30]: Training Loss = 0.23728
	Epoch[18/30]: Training Loss = 0.23631
	Epoch[19/30]: Training Loss = 0.23586
	Epoch[20/30]: Training Loss = 0.23603
	Epoch[21/30]: Training Loss = 0.23656
	Epoch[22/30]: Training Loss = 0.23616
	Epoch[23/30]: Training Loss = 0.23484
	Epoch[24/30]: Training Loss = 0.23369
	Epoch[25/30]: Training Loss = 0.23247
	Epoch[26/30]: Training Loss = 0.23151
	Epoch[27/30]: Training Loss = 0.23041
	Epoch[28/30]: Training Loss = 0.22920
	Epoch[29/30]: Training Loss = 0.22797
	Epoch[30/30]: Training Loss = 0.22711
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0005543046281673014

***Testing Results***
==============================
Test Accuracy = 93.650 %
Test Error = 6.350 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.027836499735713005, 0.012991700321435928, 0.010152725502848625, 0.00723928352817893, 0.008191235363483429, 0.005559031385928392, 0.004916721023619175, 0.0059433672577142715, 0.0044818962924182415, 0.006301042158156633, 0.006404364015907049, 0.005308603402227163, 0.003981619607657194, 0.0052243624813854694, 0.0051659406162798405, 0.003161911852657795, 0.0007544620893895626, 0.0002794098691083491, -0.0003188488190062344, -0.0005080378614366055, 0.0014094326179474592, 0.002221898641437292, 0.002405286766588688, 0.002186165191233158, 0.0017767212120816112, 0.001715807942673564, 0.0025899200700223446, 0.0018521173624321818, 0.0013942566001787782, 0.0005543046281673014]
Loss: [2.3046654697418214, 0.5910766454378764, 0.32621911709308626, 0.3048682614485423, 0.2933746668338776, 0.2849481951077779, 0.2777060896158218, 0.27261002693971, 0.26833046352068585, 0.2639914091110229, 0.25987229129473366, 0.25559406367937726, 0.25139265751838685, 0.24759378356933595, 0.2449694044272105, 0.2423879226287206, 0.23938603288332622, 0.23727511866092682, 0.23631431872049968, 0.23585964550177257, 0.2360308765967687, 0.23656411488056184, 0.23615783902009327, 0.23484341560999553, 0.23368550247351327, 0.23246973988215128, 0.23150741233825683, 0.2304133978684743, 0.2292003452539444, 0.22797488156159718, 0.2271111082792282]
