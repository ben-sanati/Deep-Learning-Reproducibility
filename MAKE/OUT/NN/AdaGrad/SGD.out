Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.59483
	Epoch[2/30]: Training Loss = 0.37497
	Epoch[3/30]: Training Loss = 0.34996
	Epoch[4/30]: Training Loss = 0.33504
	Epoch[5/30]: Training Loss = 0.32280
	Epoch[6/30]: Training Loss = 0.31422
	Epoch[7/30]: Training Loss = 0.31004
	Epoch[8/30]: Training Loss = 0.30285
	Epoch[9/30]: Training Loss = 0.29564
	Epoch[10/30]: Training Loss = 0.28902
	Epoch[11/30]: Training Loss = 0.28053
	Epoch[12/30]: Training Loss = 0.27750
	Epoch[13/30]: Training Loss = 0.27413
	Epoch[14/30]: Training Loss = 0.27178
	Epoch[15/30]: Training Loss = 0.26639
	Epoch[16/30]: Training Loss = 0.26193
	Epoch[17/30]: Training Loss = 0.25862
	Epoch[18/30]: Training Loss = 0.25636
	Epoch[19/30]: Training Loss = 0.25550
	Epoch[20/30]: Training Loss = 0.25515
	Epoch[21/30]: Training Loss = 0.25217
	Epoch[22/30]: Training Loss = 0.24715
	Epoch[23/30]: Training Loss = 0.24525
	Epoch[24/30]: Training Loss = 0.24250
	Epoch[25/30]: Training Loss = 0.24100
	Epoch[26/30]: Training Loss = 0.23926
	Epoch[27/30]: Training Loss = 0.23589
	Epoch[28/30]: Training Loss = 0.23247
	Epoch[29/30]: Training Loss = 0.22983
	Epoch[30/30]: Training Loss = 0.22694
***Training Complete***

Final Optimizer Parameters
	alpha : 0.002941722981631756

***Testing Results***
==============================
Test Accuracy = 93.670 %
Test Error = 6.330 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.007168477401137352, 0.007562549784779549, 0.000511184218339622, 0.010645747184753418, 0.006786216050386429, 0.002787363715469837, 0.005043142940849066, 0.00521003408357501, 0.007578817196190357, 0.013942795805633068, 0.005250878632068634, 0.004179119598120451, -0.0012921802699565887, 0.00470461742952466, 0.004048283211886883, 0.002765555866062641, 0.004794986452907324, 0.0007114708423614502, 0.000722232274711132, 0.0065491944551467896, 0.009578157216310501, 0.003842359408736229, 0.010845688171684742, 0.00451859924942255, 0.002616923302412033, 0.003008111845701933, 0.005119255278259516, 0.006896998733282089, 0.0018407070310786366, 0.002941722981631756]
Loss: [2.3049680803934733, 0.5948277943452199, 0.3749692570368449, 0.34995600994428, 0.3350353676954905, 0.3228047674973806, 0.3142192422389984, 0.31003623825709026, 0.3028498837153117, 0.29563778707186383, 0.28901936014493307, 0.2805344057321548, 0.2774956520318985, 0.27412613027095795, 0.271775076341629, 0.2663879250526428, 0.26193287773132323, 0.25862339049975075, 0.25635915447076163, 0.2555040377298991, 0.2551547637462616, 0.25217144718964896, 0.24714624430338542, 0.24525135637919107, 0.24249780089060466, 0.2410041609287262, 0.2392579888343811, 0.2358852076927821, 0.23247086950937906, 0.2298336565732956, 0.22693815704186757]
