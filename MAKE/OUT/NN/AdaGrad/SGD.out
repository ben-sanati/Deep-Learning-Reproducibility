Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.61774
	Epoch[2/30]: Training Loss = 0.39332
	Epoch[3/30]: Training Loss = 0.37348
	Epoch[4/30]: Training Loss = 0.35400
	Epoch[5/30]: Training Loss = 0.32598
	Epoch[6/30]: Training Loss = 0.32101
	Epoch[7/30]: Training Loss = 0.31513
	Epoch[8/30]: Training Loss = 0.30473
	Epoch[9/30]: Training Loss = 0.29207
	Epoch[10/30]: Training Loss = 0.28568
	Epoch[11/30]: Training Loss = 0.28065
	Epoch[12/30]: Training Loss = 0.27576
	Epoch[13/30]: Training Loss = 0.27071
	Epoch[14/30]: Training Loss = 0.26685
	Epoch[15/30]: Training Loss = 0.26347
	Epoch[16/30]: Training Loss = 0.25970
	Epoch[17/30]: Training Loss = 0.25312
	Epoch[18/30]: Training Loss = 0.25013
	Epoch[19/30]: Training Loss = 0.24811
	Epoch[20/30]: Training Loss = 0.24603
	Epoch[21/30]: Training Loss = 0.24451
	Epoch[22/30]: Training Loss = 0.24183
	Epoch[23/30]: Training Loss = 0.23813
	Epoch[24/30]: Training Loss = 0.23477
	Epoch[25/30]: Training Loss = 0.23297
	Epoch[26/30]: Training Loss = 0.23118
	Epoch[27/30]: Training Loss = 0.22974
	Epoch[28/30]: Training Loss = 0.22771
	Epoch[29/30]: Training Loss = 0.22564
	Epoch[30/30]: Training Loss = 0.22198
***Training Complete***

Final Optimizer Parameters
	alpha : 0.008208143524825573

***Testing Results***
==============================
Test Accuracy = 93.810 %
Test Error = 6.190 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.0020718735177069902, 0.004677692428231239, 0.007239645346999168, 0.01741090975701809, -0.0018841371638700366, 0.003038008464500308, 0.005273344460874796, 0.011033591814339161, 0.0024054080713540316, 0.013236221857368946, 0.0021591412369161844, 0.004472247790545225, 0.004360920749604702, 0.005464647430926561, 0.008949343115091324, 0.013708790764212608, 0.007307627238333225, 0.005001625511795282, 0.0030369479209184647, 0.0024929840583354235, 0.007049038540571928, 0.008975379168987274, 0.009965743869543076, 0.004601455759257078, 0.006189663428813219, -0.0012586390366777778, 0.004389028064906597, 0.008557762950658798, 0.008127315901219845, 0.008208143524825573]
Loss: [2.312792935816447, 0.6177384516239166, 0.3933230441411336, 0.37347924036979674, 0.3540003269672394, 0.32597967279752094, 0.3210129537741343, 0.3151311580657959, 0.30473295844395953, 0.29206558517615, 0.28568353799184165, 0.2806543847560883, 0.2757640615304311, 0.270707085053126, 0.2668500850756963, 0.26347366580963133, 0.25969896052678426, 0.25312488962809243, 0.250129501859347, 0.24810603286425273, 0.24603337454795837, 0.24450685787200926, 0.24183487032254536, 0.2381331754366557, 0.23477043487230936, 0.23297048625151318, 0.23118386214574177, 0.2297426179567973, 0.22771232998371124, 0.22563881546656292, 0.22198398126761118]
