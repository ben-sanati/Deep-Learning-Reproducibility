Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.60796
	Epoch[2/30]: Training Loss = 0.38011
	Epoch[3/30]: Training Loss = 0.36029
	Epoch[4/30]: Training Loss = 0.34460
	Epoch[5/30]: Training Loss = 0.33084
	Epoch[6/30]: Training Loss = 0.31852
	Epoch[7/30]: Training Loss = 0.31242
	Epoch[8/30]: Training Loss = 0.30733
	Epoch[9/30]: Training Loss = 0.30047
	Epoch[10/30]: Training Loss = 0.29420
	Epoch[11/30]: Training Loss = 0.28687
	Epoch[12/30]: Training Loss = 0.28026
	Epoch[13/30]: Training Loss = 0.27603
	Epoch[14/30]: Training Loss = 0.27288
	Epoch[15/30]: Training Loss = 0.26693
	Epoch[16/30]: Training Loss = 0.26104
	Epoch[17/30]: Training Loss = 0.26025
	Epoch[18/30]: Training Loss = 0.25795
	Epoch[19/30]: Training Loss = 0.25654
	Epoch[20/30]: Training Loss = 0.25390
	Epoch[21/30]: Training Loss = 0.25255
	Epoch[22/30]: Training Loss = 0.24827
	Epoch[23/30]: Training Loss = 0.24385
	Epoch[24/30]: Training Loss = 0.24183
	Epoch[25/30]: Training Loss = 0.23965
	Epoch[26/30]: Training Loss = 0.23782
	Epoch[27/30]: Training Loss = 0.23551
	Epoch[28/30]: Training Loss = 0.23301
	Epoch[29/30]: Training Loss = 0.23083
	Epoch[30/30]: Training Loss = 0.22900
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0020025609992444515

***Testing Results***
==============================
Test Accuracy = 93.620 %
Test Error = 6.380 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.005524547304958105, 0.007265274412930012, 0.009776804596185684, 0.0027333858888596296, 0.005001586861908436, 0.005011328496038914, 0.0017119733383879066, 0.007411701139062643, 0.0011373077286407351, 0.007505612447857857, 0.009570306167006493, 0.0016054778825491667, 0.0038218607660382986, 0.0163266584277153, 0.006842792499810457, 0.0029890285804867744, 0.0018163092900067568, -0.00010118662612512708, 0.0037369297351688147, 0.0017302606720477343, 0.010601582005620003, 0.0038035970646888018, 0.0058205872774124146, 0.0020341011695563793, 0.004063826519995928, 0.00034358317498117685, 0.007192195393145084, -0.0003646736731752753, 0.0008737189928069711, 0.0020025609992444515]
Loss: [2.3046654697418214, 0.6079608621438344, 0.3801087997595469, 0.3602940668265025, 0.3446008581002553, 0.3308379205385844, 0.3185163858493169, 0.31241861743927, 0.3073305680592855, 0.30047235277493795, 0.29420223468939466, 0.2868748993396759, 0.28025640227794646, 0.27603376344045005, 0.2728779078960419, 0.26692821416854856, 0.2610428499062856, 0.2602509672721227, 0.25795398654143015, 0.25654442088603974, 0.25389838608900706, 0.2525493913491567, 0.24827228796482087, 0.24385272409121195, 0.2418258364915848, 0.2396505586942037, 0.23781984717051188, 0.23550973035494485, 0.2330112942457199, 0.23082501782576242, 0.2290047364473343]
