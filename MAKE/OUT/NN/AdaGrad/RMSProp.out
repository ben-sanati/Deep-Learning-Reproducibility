Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.57847
	Epoch[2/30]: Training Loss = 0.36859
	Epoch[3/30]: Training Loss = 0.33067
	Epoch[4/30]: Training Loss = 0.30697
	Epoch[5/30]: Training Loss = 0.29141
	Epoch[6/30]: Training Loss = 0.27452
	Epoch[7/30]: Training Loss = 0.26122
	Epoch[8/30]: Training Loss = 0.24921
	Epoch[9/30]: Training Loss = 0.24048
	Epoch[10/30]: Training Loss = 0.22995
	Epoch[11/30]: Training Loss = 0.21835
	Epoch[12/30]: Training Loss = 0.20972
	Epoch[13/30]: Training Loss = 0.20022
	Epoch[14/30]: Training Loss = 0.19297
	Epoch[15/30]: Training Loss = 0.18530
	Epoch[16/30]: Training Loss = 0.17897
	Epoch[17/30]: Training Loss = 0.17222
	Epoch[18/30]: Training Loss = 0.16506
	Epoch[19/30]: Training Loss = 0.15979
	Epoch[20/30]: Training Loss = 0.15532
	Epoch[21/30]: Training Loss = 0.15170
	Epoch[22/30]: Training Loss = 0.14674
	Epoch[23/30]: Training Loss = 0.14292
	Epoch[24/30]: Training Loss = 0.13890
	Epoch[25/30]: Training Loss = 0.13538
	Epoch[26/30]: Training Loss = 0.13184
	Epoch[27/30]: Training Loss = 0.12904
	Epoch[28/30]: Training Loss = 0.12514
	Epoch[29/30]: Training Loss = 0.12252
	Epoch[30/30]: Training Loss = 0.12000
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0033306581899523735

***Testing Results***
==============================
Test Accuracy = 96.410 %
Test Error = 3.590 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, -0.00473168957978487, -0.007215817458927631, -0.011619627475738525, -0.022022036835551262, 0.01285016629844904, 0.015525195747613907, 0.00318729504942894, 0.05339784920215607, 0.011476827785372734, 0.027361197397112846, 0.03887354955077171, 0.03293859213590622, 0.02379715070128441, 0.026572857052087784, 0.03397209197282791, 0.036207668483257294, 0.05769508332014084, 0.031941454857587814, 0.021381475031375885, 0.038398608565330505, 0.04402177035808563, 0.04590846970677376, 0.031796302646398544, 0.011504977941513062, 0.003239493817090988, 0.009142208844423294, 0.0349993035197258, 0.05335584655404091, 0.044738657772541046, 0.0033306581899523735]
Loss: [2.307249734369914, 0.5784659679253896, 0.36858724473317467, 0.3306661603609721, 0.30697457739512124, 0.29140971308549246, 0.27452420903841657, 0.26122128858566285, 0.24921156818072002, 0.24047800959746043, 0.2299517293771108, 0.21834998995463054, 0.20972347246011097, 0.2002226020614306, 0.19296878504753112, 0.18530454845428468, 0.1789690909822782, 0.17221978390216827, 0.165061807068189, 0.1597935426990191, 0.15531967813968658, 0.15170131675402324, 0.1467394862651825, 0.1429163054704666, 0.13890221640666325, 0.13537941353321076, 0.1318380691051483, 0.1290405203183492, 0.125136150966088, 0.12251903988917669, 0.11999566024144491]
