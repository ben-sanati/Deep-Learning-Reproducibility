Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.56929
	Epoch[2/30]: Training Loss = 0.37147
	Epoch[3/30]: Training Loss = 0.33966
	Epoch[4/30]: Training Loss = 0.31628
	Epoch[5/30]: Training Loss = 0.29670
	Epoch[6/30]: Training Loss = 0.28067
	Epoch[7/30]: Training Loss = 0.26976
	Epoch[8/30]: Training Loss = 0.25819
	Epoch[9/30]: Training Loss = 0.24706
	Epoch[10/30]: Training Loss = 0.23671
	Epoch[11/30]: Training Loss = 0.22638
	Epoch[12/30]: Training Loss = 0.21725
	Epoch[13/30]: Training Loss = 0.20970
	Epoch[14/30]: Training Loss = 0.20269
	Epoch[15/30]: Training Loss = 0.19517
	Epoch[16/30]: Training Loss = 0.18972
	Epoch[17/30]: Training Loss = 0.18570
	Epoch[18/30]: Training Loss = 0.18138
	Epoch[19/30]: Training Loss = 0.17744
	Epoch[20/30]: Training Loss = 0.17312
	Epoch[21/30]: Training Loss = 0.16873
	Epoch[22/30]: Training Loss = 0.16263
	Epoch[23/30]: Training Loss = 0.15753
	Epoch[24/30]: Training Loss = 0.15335
	Epoch[25/30]: Training Loss = 0.14893
	Epoch[26/30]: Training Loss = 0.14524
	Epoch[27/30]: Training Loss = 0.14173
	Epoch[28/30]: Training Loss = 0.13636
	Epoch[29/30]: Training Loss = 0.13259
	Epoch[30/30]: Training Loss = 0.12877
***Training Complete***

Final Optimizer Parameters
	alpha : 0.005917803850024939

***Testing Results***
==============================
Test Accuracy = 96.100 %
Test Error = 3.900 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.0074260300025343895, 0.010007940232753754, 0.009456715546548367, 0.002051827497780323, -0.00985797680914402, 0.046999216079711914, -5.743931978940964e-07, 0.011996760964393616, 0.032021038234233856, 0.013230301439762115, 0.04059410095214844, -0.011125806719064713, 0.00809578225016594, 0.03167688846588135, 0.01147000677883625, 0.02713463082909584, -9.3141570687294e-05, 0.007183356676250696, -0.017179790884256363, 0.03276718407869339, 0.020263954997062683, -0.02163117192685604, 0.01825854927301407, 0.011839872226119041, 0.025441456586122513, 0.014113634824752808, 0.0410679467022419, -0.012215383350849152, 0.0011387486010789871, 0.005917803850024939]
Loss: [2.3046654697418214, 0.5692949224472046, 0.3714663586139679, 0.33965934074719745, 0.31628127077420554, 0.2967028980573018, 0.2806688575029373, 0.26975570625464124, 0.25819161059061685, 0.24706177763938905, 0.23671418331861496, 0.2263823528289795, 0.217245037372907, 0.20969996560414633, 0.2026900357166926, 0.19517439142068227, 0.18971603458722433, 0.18569765853881837, 0.18137629448572795, 0.17743802406787873, 0.17311820007165274, 0.16873209002017975, 0.16263316284020743, 0.15753025626341502, 0.15335223484039306, 0.14893432398239773, 0.145236380871137, 0.14173142026265462, 0.13635669971307118, 0.13258738430341085, 0.12876824195384978]
