Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.74707
	Epoch[2/30]: Training Loss = 0.34872
	Epoch[3/30]: Training Loss = 0.33564
	Epoch[4/30]: Training Loss = 0.32641
	Epoch[5/30]: Training Loss = 0.31683
	Epoch[6/30]: Training Loss = 0.30712
	Epoch[7/30]: Training Loss = 0.30230
	Epoch[8/30]: Training Loss = 0.29777
	Epoch[9/30]: Training Loss = 0.29121
	Epoch[10/30]: Training Loss = 0.28578
	Epoch[11/30]: Training Loss = 0.27867
	Epoch[12/30]: Training Loss = 0.27238
	Epoch[13/30]: Training Loss = 0.26852
	Epoch[14/30]: Training Loss = 0.26442
	Epoch[15/30]: Training Loss = 0.25809
	Epoch[16/30]: Training Loss = 0.25247
	Epoch[17/30]: Training Loss = 0.24977
	Epoch[18/30]: Training Loss = 0.24594
	Epoch[19/30]: Training Loss = 0.24417
	Epoch[20/30]: Training Loss = 0.24159
	Epoch[21/30]: Training Loss = 0.24020
	Epoch[22/30]: Training Loss = 0.23619
	Epoch[23/30]: Training Loss = 0.23225
	Epoch[24/30]: Training Loss = 0.22923
	Epoch[25/30]: Training Loss = 0.22673
	Epoch[26/30]: Training Loss = 0.22364
	Epoch[27/30]: Training Loss = 0.22098
	Epoch[28/30]: Training Loss = 0.21853
	Epoch[29/30]: Training Loss = 0.21620
	Epoch[30/30]: Training Loss = 0.21244
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0023816421162337065

***Testing Results***
==============================
Test Accuracy = 93.920 %
Test Error = 6.080 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.024183914065361023, 0.006909288931638002, 0.00801662728190422, 0.005446237977594137, 0.010411103256046772, -0.00024357606889680028, 0.0034147193655371666, 0.007242946419864893, -0.003279307158663869, 0.006383385043591261, 0.0074449386447668076, 0.0005163942696526647, 0.003034422639757395, 0.01884785108268261, 0.008421349339187145, 0.005816605873405933, 0.005635046865791082, 0.0017568340990692377, 0.00448335288092494, 0.0009514394914731383, 0.011929086409509182, 0.006790607236325741, 0.007866046391427517, 0.00041490665171295404, 0.005517477169632912, 0.002069775015115738, 0.011089819483458996, 6.295513594523072e-05, 0.001593919238075614, 0.0023816421162337065]
Loss: [2.3046654697418214, 0.7470655046463013, 0.3487165729045868, 0.3356398081620534, 0.32640680050849913, 0.3168305552482605, 0.30711826490561167, 0.3022989269177119, 0.29776561290423076, 0.29121168365478517, 0.285777268020312, 0.27867198238372803, 0.2723822431484858, 0.26851625548203784, 0.26441730949083964, 0.2580914391040802, 0.25246555562019346, 0.24976941518783569, 0.24593517331282297, 0.24417318601608276, 0.24159049761295318, 0.24020059917767841, 0.23618721361160278, 0.23225203159650168, 0.2292294659058253, 0.22673201611836752, 0.22363904846509297, 0.22097945030530294, 0.2185269317070643, 0.2162028661966324, 0.21243793768088023]
