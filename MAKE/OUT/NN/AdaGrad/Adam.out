Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.74952
	Epoch[2/30]: Training Loss = 0.34751
	Epoch[3/30]: Training Loss = 0.33391
	Epoch[4/30]: Training Loss = 0.32891
	Epoch[5/30]: Training Loss = 0.32174
	Epoch[6/30]: Training Loss = 0.31853
	Epoch[7/30]: Training Loss = 0.31465
	Epoch[8/30]: Training Loss = 0.31191
	Epoch[9/30]: Training Loss = 0.30806
	Epoch[10/30]: Training Loss = 0.30294
	Epoch[11/30]: Training Loss = 0.29821
	Epoch[12/30]: Training Loss = 0.29230
	Epoch[13/30]: Training Loss = 0.28748
	Epoch[14/30]: Training Loss = 0.28156
	Epoch[15/30]: Training Loss = 0.27468
	Epoch[16/30]: Training Loss = 0.27040
	Epoch[17/30]: Training Loss = 0.26766
	Epoch[18/30]: Training Loss = 0.26255
	Epoch[19/30]: Training Loss = 0.25782
	Epoch[20/30]: Training Loss = 0.25625
	Epoch[21/30]: Training Loss = 0.25488
	Epoch[22/30]: Training Loss = 0.25115
	Epoch[23/30]: Training Loss = 0.24678
	Epoch[24/30]: Training Loss = 0.24320
	Epoch[25/30]: Training Loss = 0.23780
	Epoch[26/30]: Training Loss = 0.23251
	Epoch[27/30]: Training Loss = 0.22826
	Epoch[28/30]: Training Loss = 0.22497
	Epoch[29/30]: Training Loss = 0.22135
	Epoch[30/30]: Training Loss = 0.21613
***Training Complete***

Final Optimizer Parameters
	alpha : 0.013829606585204601

***Testing Results***
==============================
Test Accuracy = 93.950 %
Test Error = 6.050 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.024492254480719566, 0.00699969008564949, 0.0052277520298957825, 0.005644580814987421, 0.0017753986176103354, 0.0067971087992191315, 0.001647717785090208, 0.008762647397816181, 0.00905541330575943, 0.012173092924058437, 0.008608941920101643, 0.007891597226262093, 0.012072073295712471, 0.0075592645443975925, 0.0067296321503818035, 0.003186789806932211, 0.009602808393537998, 0.005949384532868862, -0.004735048860311508, -0.002154467161744833, 0.00871664471924305, 0.004990268498659134, 0.008534098975360394, 0.007594917435199022, 0.009994874708354473, 0.004101537633687258, 0.006983468774706125, 0.009809501469135284, 0.009320609271526337, 0.013829606585204601]
Loss: [2.301129010772705, 0.7495188864231109, 0.34751166659990945, 0.33390598375002545, 0.328911921342214, 0.32174378657341, 0.31852859183152515, 0.31464717559814454, 0.31191070523262027, 0.308057270415624, 0.30294371113777163, 0.29820803944269814, 0.29230039250850676, 0.2874836757183075, 0.28156438070138295, 0.2746843654791514, 0.2704019783258438, 0.2676595153967539, 0.2625516282002131, 0.25782322975794475, 0.2562460799058278, 0.2548839227835337, 0.2511457518736521, 0.24678154555161794, 0.24319740836620332, 0.23779568819204966, 0.23250830937226613, 0.22826115226745605, 0.2249665535926819, 0.2213538793881734, 0.21613336805502573]
