Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.31519
	Epoch[2/30]: Training Loss = 0.11882
	Epoch[3/30]: Training Loss = 0.08778
	Epoch[4/30]: Training Loss = 0.06950
	Epoch[5/30]: Training Loss = 0.06008
	Epoch[6/30]: Training Loss = 0.05249
	Epoch[7/30]: Training Loss = 0.04515
	Epoch[8/30]: Training Loss = 0.03972
	Epoch[9/30]: Training Loss = 0.03760
	Epoch[10/30]: Training Loss = 0.03667
	Epoch[11/30]: Training Loss = 0.03010
	Epoch[12/30]: Training Loss = 0.02713
	Epoch[13/30]: Training Loss = 0.02615
	Epoch[14/30]: Training Loss = 0.02391
	Epoch[15/30]: Training Loss = 0.02591
	Epoch[16/30]: Training Loss = 0.02088
	Epoch[17/30]: Training Loss = 0.02330
	Epoch[18/30]: Training Loss = 0.01846
	Epoch[19/30]: Training Loss = 0.01910
	Epoch[20/30]: Training Loss = 0.01708
	Epoch[21/30]: Training Loss = 0.01770
	Epoch[22/30]: Training Loss = 0.01514
	Epoch[23/30]: Training Loss = 0.01686
	Epoch[24/30]: Training Loss = 0.01359
	Epoch[25/30]: Training Loss = 0.01372
	Epoch[26/30]: Training Loss = 0.01290
	Epoch[27/30]: Training Loss = 0.01365
	Epoch[28/30]: Training Loss = 0.01332
	Epoch[29/30]: Training Loss = 0.01245
	Epoch[30/30]: Training Loss = 0.01180
***Training Complete***

Final Optimizer Parameters
	alpha : 0.10000000149011612
	gamma : 0.9900000095367432

***Testing Results***
==============================
Test Accuracy = 97.630 %
Test Error = 2.370 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]
gamma: [0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432]
Loss: [0.009050441976388296, 0.3151871077855428, 0.11881780874530475, 0.0877843893925349, 0.06950361654758454, 0.06008435215950012, 0.052485199089845024, 0.045149919311205544, 0.039717211184899015, 0.03759743565917015, 0.03667127116325622, 0.030099592608089247, 0.02713127127761642, 0.026150652886430422, 0.02390892259652416, 0.02590940413552647, 0.02087570010290171, 0.023300252229774318, 0.018462700401637996, 0.019102347307636713, 0.017077696846457546, 0.01769864156548865, 0.015136311746819895, 0.016859487669465305, 0.013590374100920356, 0.013717365968006198, 0.012898301551416321, 0.01364505603665942, 0.013315959837201444, 0.012451218468438552, 0.011795059490993057]
