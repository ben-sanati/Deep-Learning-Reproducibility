Args:
	model: NN
	optimizer: RMSProp
	optimizer_args: {'gamma': 0.99}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.32409
	Epoch[2/30]: Training Loss = 0.12043
	Epoch[3/30]: Training Loss = 0.08647
	Epoch[4/30]: Training Loss = 0.06905
	Epoch[5/30]: Training Loss = 0.05962
	Epoch[6/30]: Training Loss = 0.05181
	Epoch[7/30]: Training Loss = 0.04530
	Epoch[8/30]: Training Loss = 0.04233
	Epoch[9/30]: Training Loss = 0.03704
	Epoch[10/30]: Training Loss = 0.03312
	Epoch[11/30]: Training Loss = 0.03293
	Epoch[12/30]: Training Loss = 0.02807
	Epoch[13/30]: Training Loss = 0.02805
	Epoch[14/30]: Training Loss = 0.02387
	Epoch[15/30]: Training Loss = 0.02323
	Epoch[16/30]: Training Loss = 0.02170
	Epoch[17/30]: Training Loss = 0.01962
	Epoch[18/30]: Training Loss = 0.02196
	Epoch[19/30]: Training Loss = 0.02160
	Epoch[20/30]: Training Loss = 0.01500
	Epoch[21/30]: Training Loss = 0.01658
	Epoch[22/30]: Training Loss = 0.01549
	Epoch[23/30]: Training Loss = 0.01596
	Epoch[24/30]: Training Loss = 0.01403
	Epoch[25/30]: Training Loss = 0.01268
	Epoch[26/30]: Training Loss = 0.01347
	Epoch[27/30]: Training Loss = 0.01416
	Epoch[28/30]: Training Loss = 0.01309
	Epoch[29/30]: Training Loss = 0.01108
	Epoch[30/30]: Training Loss = 0.01107
***Training Complete***

Final Optimizer Parameters
	alpha : 0.10000000149011612
	gamma : 0.9900000095367432

***Testing Results***
==============================
Test Accuracy = 97.760 %
Test Error = 2.240 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612]
gamma: [0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432]
Loss: [2.3046654697418214, 0.3240913198153178, 0.12043064458767573, 0.08646629067262014, 0.06904710856278737, 0.05961637732386589, 0.05181188885619243, 0.045304645230372746, 0.042333825379610064, 0.037044484143952526, 0.03311833993530211, 0.03293067743380865, 0.028070725820461908, 0.028054195534686247, 0.0238696736849534, 0.023227343514344346, 0.021695986037204663, 0.01962072097514368, 0.021963913011770152, 0.021604279090382626, 0.014997176735316559, 0.016580758847866674, 0.015492505036979371, 0.015960857988937642, 0.014029673237890044, 0.012675184868013546, 0.013472524474325473, 0.014159697985348794, 0.013085092354664509, 0.011084795403337922, 0.011074027147026936]
