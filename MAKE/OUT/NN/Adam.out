Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.52435
	Epoch[2/30]: Training Loss = 0.22991
	Epoch[3/30]: Training Loss = 0.17296
	Epoch[4/30]: Training Loss = 0.13603
	Epoch[5/30]: Training Loss = 0.11099
	Epoch[6/30]: Training Loss = 0.09376
	Epoch[7/30]: Training Loss = 0.07874
	Epoch[8/30]: Training Loss = 0.06850
	Epoch[9/30]: Training Loss = 0.05849
	Epoch[10/30]: Training Loss = 0.05127
	Epoch[11/30]: Training Loss = 0.04510
	Epoch[12/30]: Training Loss = 0.03944
	Epoch[13/30]: Training Loss = 0.03461
	Epoch[14/30]: Training Loss = 0.03012
	Epoch[15/30]: Training Loss = 0.02661
	Epoch[16/30]: Training Loss = 0.02327
	Epoch[17/30]: Training Loss = 0.02044
	Epoch[18/30]: Training Loss = 0.01762
	Epoch[19/30]: Training Loss = 0.01626
	Epoch[20/30]: Training Loss = 0.01388
	Epoch[21/30]: Training Loss = 0.01185
	Epoch[22/30]: Training Loss = 0.01022
	Epoch[23/30]: Training Loss = 0.00927
	Epoch[24/30]: Training Loss = 0.00811
	Epoch[25/30]: Training Loss = 0.00729
	Epoch[26/30]: Training Loss = 0.00639
	Epoch[27/30]: Training Loss = 0.00548
	Epoch[28/30]: Training Loss = 0.00479
	Epoch[29/30]: Training Loss = 0.00409
	Epoch[30/30]: Training Loss = 0.00344
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010000000474974513
	beta1 : 0.8999999761581421
	beta2 : 0.9900000095367432

***Testing Results***
==============================
Test Accuracy = 97.780 %
Test Error = 2.220 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513]
beta1: [0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421]
beta2: [0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432]
Loss: [2.3046654697418214, 0.5243458892822266, 0.22991181331475577, 0.1729628249088923, 0.13603063661257425, 0.11098937846819559, 0.09375749443769454, 0.07873929273188114, 0.06850109997590383, 0.05848873357375463, 0.05126689655929804, 0.045095802674690884, 0.039441373379031815, 0.03460797471404076, 0.0301211250603199, 0.026614431490997473, 0.023268380733331043, 0.020437687571843464, 0.017616991521418095, 0.01626082571198543, 0.013881356221437454, 0.011852455334365368, 0.0102188310397168, 0.009267414776484172, 0.008113701388426126, 0.007286754576737682, 0.006386158927281698, 0.005480708016455173, 0.004794419817253947, 0.004087065057642758, 0.0034408658663431803]
