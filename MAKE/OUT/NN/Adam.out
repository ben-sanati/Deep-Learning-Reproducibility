Args:
	model: NN
	optimizer: Adam
	optimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.001
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.53788
	Epoch[2/30]: Training Loss = 0.23343
	Epoch[3/30]: Training Loss = 0.17703
	Epoch[4/30]: Training Loss = 0.13894
	Epoch[5/30]: Training Loss = 0.11344
	Epoch[6/30]: Training Loss = 0.09460
	Epoch[7/30]: Training Loss = 0.08052
	Epoch[8/30]: Training Loss = 0.06986
	Epoch[9/30]: Training Loss = 0.06047
	Epoch[10/30]: Training Loss = 0.05333
	Epoch[11/30]: Training Loss = 0.04691
	Epoch[12/30]: Training Loss = 0.04135
	Epoch[13/30]: Training Loss = 0.03659
	Epoch[14/30]: Training Loss = 0.03261
	Epoch[15/30]: Training Loss = 0.02878
	Epoch[16/30]: Training Loss = 0.02518
	Epoch[17/30]: Training Loss = 0.02193
	Epoch[18/30]: Training Loss = 0.02018
	Epoch[19/30]: Training Loss = 0.01758
	Epoch[20/30]: Training Loss = 0.01559
	Epoch[21/30]: Training Loss = 0.01379
	Epoch[22/30]: Training Loss = 0.01263
	Epoch[23/30]: Training Loss = 0.01069
	Epoch[24/30]: Training Loss = 0.01001
	Epoch[25/30]: Training Loss = 0.00820
	Epoch[26/30]: Training Loss = 0.00776
	Epoch[27/30]: Training Loss = 0.00697
	Epoch[28/30]: Training Loss = 0.00547
	Epoch[29/30]: Training Loss = 0.00570
	Epoch[30/30]: Training Loss = 0.00438
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010000000474974513
	beta1 : 0.8999999761581421
	beta2 : 0.9900000095367432

***Testing Results***
==============================
Test Accuracy = 97.740 %
Test Error = 2.260 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513, 0.0010000000474974513]
beta1: [0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421, 0.8999999761581421]
beta2: [0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432, 0.9900000095367432]
Loss: [0.009057198564211527, 0.5378796242316564, 0.23343392407894134, 0.177025515294075, 0.13894438353379568, 0.11344208792845407, 0.09460191919803619, 0.08052144265373548, 0.06985725538134575, 0.0604668862303098, 0.053327301839987434, 0.04690716625849406, 0.04134547868768374, 0.036587172818680604, 0.03260643108213941, 0.02877897138694922, 0.025177914493282636, 0.021926388716697692, 0.0201790193532904, 0.01758292887409528, 0.015593355586876472, 0.013791089914242426, 0.012625367939968903, 0.01069007835884889, 0.01000858303743104, 0.00819923716088136, 0.007763209456702074, 0.006971572924156984, 0.0054682228808601695, 0.0057016504826645055, 0.004375406977906823]
