Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 2.04001
	Epoch[2/30]: Training Loss = 1.33275
	Epoch[3/30]: Training Loss = 0.86867
	Epoch[4/30]: Training Loss = 0.66796
	Epoch[5/30]: Training Loss = 0.56564
	Epoch[6/30]: Training Loss = 0.50447
	Epoch[7/30]: Training Loss = 0.46401
	Epoch[8/30]: Training Loss = 0.43523
	Epoch[9/30]: Training Loss = 0.41373
	Epoch[10/30]: Training Loss = 0.39692
	Epoch[11/30]: Training Loss = 0.38339
	Epoch[12/30]: Training Loss = 0.37206
	Epoch[13/30]: Training Loss = 0.36241
	Epoch[14/30]: Training Loss = 0.35411
	Epoch[15/30]: Training Loss = 0.34664
	Epoch[16/30]: Training Loss = 0.34011
	Epoch[17/30]: Training Loss = 0.33401
	Epoch[18/30]: Training Loss = 0.32848
	Epoch[19/30]: Training Loss = 0.32330
	Epoch[20/30]: Training Loss = 0.31852
	Epoch[21/30]: Training Loss = 0.31398
	Epoch[22/30]: Training Loss = 0.30963
	Epoch[23/30]: Training Loss = 0.30565
	Epoch[24/30]: Training Loss = 0.30179
	Epoch[25/30]: Training Loss = 0.29809
	Epoch[26/30]: Training Loss = 0.29458
	Epoch[27/30]: Training Loss = 0.29121
	Epoch[28/30]: Training Loss = 0.28797
	Epoch[29/30]: Training Loss = 0.28474
	Epoch[30/30]: Training Loss = 0.28174
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009999999776482582
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 92.410 %
Test Error = 7.590 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3046654697418214, 2.0400089515050253, 1.3327482162475586, 0.8686722310384115, 0.6679578457832337, 0.5656415320714315, 0.5044694927851359, 0.46400581482251485, 0.4352292733510335, 0.41372835817337034, 0.3969214689811071, 0.3833877556482951, 0.37206466515858966, 0.3624063731988271, 0.3541123263200124, 0.34663664830525714, 0.3401056711196899, 0.3340084550062815, 0.32847964701652527, 0.32329501897494, 0.31851578227678934, 0.31398439160982766, 0.30963350132306416, 0.30564934115409853, 0.30179099872907, 0.29809103749593097, 0.2945820277372996, 0.2912105756282806, 0.2879712274312973, 0.28473595465024315, 0.2817431068102519]
