Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
	Epoch[1/30]: Training Loss = 2.05026
	Epoch[2/30]: Training Loss = 1.35208
	Epoch[3/30]: Training Loss = 0.87428
	Epoch[4/30]: Training Loss = 0.66756
	Epoch[5/30]: Training Loss = 0.56441
	Epoch[6/30]: Training Loss = 0.50314
	Epoch[7/30]: Training Loss = 0.46294
	Epoch[8/30]: Training Loss = 0.43431
	Epoch[9/30]: Training Loss = 0.41296
	Epoch[10/30]: Training Loss = 0.39634
	Epoch[11/30]: Training Loss = 0.38292
	Epoch[12/30]: Training Loss = 0.37175
	Epoch[13/30]: Training Loss = 0.36228
	Epoch[14/30]: Training Loss = 0.35398
	Epoch[15/30]: Training Loss = 0.34670
	Epoch[16/30]: Training Loss = 0.34018
	Epoch[17/30]: Training Loss = 0.33427
	Epoch[18/30]: Training Loss = 0.32876
	Epoch[19/30]: Training Loss = 0.32378
	Epoch[20/30]: Training Loss = 0.31912
	Epoch[21/30]: Training Loss = 0.31467
	Epoch[22/30]: Training Loss = 0.31060
	Epoch[23/30]: Training Loss = 0.30657
	Epoch[24/30]: Training Loss = 0.30285
	Epoch[25/30]: Training Loss = 0.29932
	Epoch[26/30]: Training Loss = 0.29586
	Epoch[27/30]: Training Loss = 0.29248
	Epoch[28/30]: Training Loss = 0.28939
	Epoch[29/30]: Training Loss = 0.28619
	Epoch[30/30]: Training Loss = 0.28322
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009999999776482582
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 92.330 %
Test Error = 7.670 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [0.009063852083683014, 2.0502611780166626, 1.3520796106974284, 0.8742768581390381, 0.6675554010709127, 0.5644059210300446, 0.5031438182989756, 0.4629431073824565, 0.43431228669484456, 0.4129623862743378, 0.396335844039917, 0.38292208704948427, 0.3717461169878642, 0.3622823026021322, 0.3539766304175059, 0.346698409541448, 0.3401848221619924, 0.3342667297999064, 0.328757302681605, 0.32377914017041526, 0.3191218495210012, 0.31467448538144427, 0.31059909534454344, 0.30656824943224587, 0.3028541899760564, 0.2993232630888621, 0.2958563002506892, 0.29248123116493224, 0.289392125082016, 0.2861858677705129, 0.28321992530822754]
