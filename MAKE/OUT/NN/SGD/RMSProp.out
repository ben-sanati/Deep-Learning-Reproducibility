Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.1
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.52968
	Epoch[2/30]: Training Loss = 0.34090
	Epoch[3/30]: Training Loss = 0.30347
	Epoch[4/30]: Training Loss = 0.27436
	Epoch[5/30]: Training Loss = 0.25260
	Epoch[6/30]: Training Loss = 0.23341
	Epoch[7/30]: Training Loss = 0.21512
	Epoch[8/30]: Training Loss = 0.20007
	Epoch[9/30]: Training Loss = 0.18871
	Epoch[10/30]: Training Loss = 0.17667
	Epoch[11/30]: Training Loss = 0.16735
	Epoch[12/30]: Training Loss = 0.15821
	Epoch[13/30]: Training Loss = 0.14928
	Epoch[14/30]: Training Loss = 0.14331
	Epoch[15/30]: Training Loss = 0.13683
	Epoch[16/30]: Training Loss = 0.13198
	Epoch[17/30]: Training Loss = 0.12668
	Epoch[18/30]: Training Loss = 0.12246
	Epoch[19/30]: Training Loss = 0.11762
	Epoch[20/30]: Training Loss = 0.11312
	Epoch[21/30]: Training Loss = 0.10763
	Epoch[22/30]: Training Loss = 0.10352
	Epoch[23/30]: Training Loss = 0.09916
	Epoch[24/30]: Training Loss = 0.09613
	Epoch[25/30]: Training Loss = 0.09257
	Epoch[26/30]: Training Loss = 0.09002
	Epoch[27/30]: Training Loss = 0.08680
	Epoch[28/30]: Training Loss = 0.08530
	Epoch[29/30]: Training Loss = 0.08148
	Epoch[30/30]: Training Loss = 0.08013
***Training Complete***

Final Optimizer Parameters
	alpha : -0.04924757778644562
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 97.210 %
Test Error = 2.790 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.07783298939466476, 0.15231366455554962, 0.22387859225273132, -0.11763013899326324, 0.20363092422485352, 0.17505940794944763, 0.08386027812957764, 0.1996048390865326, 0.1891208440065384, -0.06364576518535614, -0.17705102264881134, 0.03731478750705719, 0.06901334226131439, 0.009384475648403168, 0.1759842187166214, 0.061865974217653275, 0.056583553552627563, 0.16990569233894348, 0.2695392072200775, 0.022645434364676476, 0.13621291518211365, 0.2711326479911804, -0.0312906950712204, 0.18881556391716003, -0.022343337535858154, 0.13220569491386414, -0.07419435679912567, 0.02053995430469513, 0.012700393795967102, -0.04924757778644562]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.290270575205485, 0.5296808254082997, 0.3408974464893341, 0.303466966287295, 0.27436055418650307, 0.2525961317618688, 0.23341023434003194, 0.21512077237764995, 0.20007304519812266, 0.18870681645870208, 0.1766706963777542, 0.16735066802501677, 0.15820815949440004, 0.14927900097767513, 0.1433056442419688, 0.13683023714621861, 0.13197953922351202, 0.12667556108236314, 0.12246430619359017, 0.1176166689435641, 0.11312133175929387, 0.10763228087822596, 0.10351513713598251, 0.09916087968349457, 0.09612925449609756, 0.09256712694764137, 0.09002390250364939, 0.08679963920116425, 0.08530285257101058, 0.08147582502365112, 0.08012657765944799]
