Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.1
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.53267
	Epoch[2/30]: Training Loss = 0.34417
	Epoch[3/30]: Training Loss = 0.30591
	Epoch[4/30]: Training Loss = 0.27698
	Epoch[5/30]: Training Loss = 0.25212
	Epoch[6/30]: Training Loss = 0.23214
	Epoch[7/30]: Training Loss = 0.21583
	Epoch[8/30]: Training Loss = 0.20102
	Epoch[9/30]: Training Loss = 0.18907
	Epoch[10/30]: Training Loss = 0.17649
	Epoch[11/30]: Training Loss = 0.16677
	Epoch[12/30]: Training Loss = 0.15675
	Epoch[13/30]: Training Loss = 0.14884
	Epoch[14/30]: Training Loss = 0.13994
	Epoch[15/30]: Training Loss = 0.13386
	Epoch[16/30]: Training Loss = 0.12799
	Epoch[17/30]: Training Loss = 0.12301
	Epoch[18/30]: Training Loss = 0.11800
	Epoch[19/30]: Training Loss = 0.11300
	Epoch[20/30]: Training Loss = 0.10873
	Epoch[21/30]: Training Loss = 0.10401
	Epoch[22/30]: Training Loss = 0.10046
	Epoch[23/30]: Training Loss = 0.09640
	Epoch[24/30]: Training Loss = 0.09308
	Epoch[25/30]: Training Loss = 0.08904
	Epoch[26/30]: Training Loss = 0.08711
	Epoch[27/30]: Training Loss = 0.08374
	Epoch[28/30]: Training Loss = 0.07976
	Epoch[29/30]: Training Loss = 0.07701
	Epoch[30/30]: Training Loss = 0.07485
***Training Complete***

Final Optimizer Parameters
	alpha : 0.006633952260017395
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 97.230 %
Test Error = 2.770 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.01558838039636612, 0.1442813277244568, 0.0030280649662017822, 0.08858615159988403, -0.0844271183013916, 0.2484992891550064, 0.04235360398888588, 0.14737512171268463, 0.11300686001777649, 0.11792029440402985, 0.29183951020240784, 0.06189170479774475, 0.09852732717990875, 0.15975290536880493, -0.012679889798164368, 0.07582764327526093, 0.09351328760385513, -0.04919065162539482, -0.2047668993473053, 0.3066047430038452, 0.14893804490566254, -0.03388975188136101, 0.03056931495666504, -0.026826053857803345, 0.3078513443470001, 0.09985663741827011, -0.014495611190795898, 0.10652711242437363, -0.11393067240715027, 0.006633952260017395]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3046654697418214, 0.532673135137558, 0.3441663958072662, 0.3059084647655487, 0.2769837844371796, 0.252122688515981, 0.23213596193790437, 0.21582713813781737, 0.20102123781840006, 0.18907197267214457, 0.1764875742852688, 0.16676567033926645, 0.156750180264314, 0.14884251303275425, 0.13993927285671234, 0.13386308892567952, 0.1279932869354884, 0.12301373811165492, 0.11799579287370046, 0.11300418340365093, 0.10873226016362508, 0.10400627321799596, 0.10046183315912882, 0.0964023194273313, 0.09307694602012634, 0.08904263116121292, 0.0871084667523702, 0.0837390530705452, 0.0797620224416256, 0.07700780425469081, 0.07484707741340002]
