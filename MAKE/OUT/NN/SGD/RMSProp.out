Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {'gamma': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.1
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.54226
	Epoch[2/30]: Training Loss = 0.34866
	Epoch[3/30]: Training Loss = 0.31097
	Epoch[4/30]: Training Loss = 0.28592
	Epoch[5/30]: Training Loss = 0.26172
	Epoch[6/30]: Training Loss = 0.24314
	Epoch[7/30]: Training Loss = 0.22394
	Epoch[8/30]: Training Loss = 0.20925
	Epoch[9/30]: Training Loss = 0.19824
	Epoch[10/30]: Training Loss = 0.18583
	Epoch[11/30]: Training Loss = 0.17296
	Epoch[12/30]: Training Loss = 0.16139
	Epoch[13/30]: Training Loss = 0.15338
	Epoch[14/30]: Training Loss = 0.14421
	Epoch[15/30]: Training Loss = 0.13788
	Epoch[16/30]: Training Loss = 0.13149
	Epoch[17/30]: Training Loss = 0.12497
	Epoch[18/30]: Training Loss = 0.11958
	Epoch[19/30]: Training Loss = 0.11432
	Epoch[20/30]: Training Loss = 0.10854
	Epoch[21/30]: Training Loss = 0.10464
	Epoch[22/30]: Training Loss = 0.10142
	Epoch[23/30]: Training Loss = 0.09754
	Epoch[24/30]: Training Loss = 0.09489
	Epoch[25/30]: Training Loss = 0.09106
	Epoch[26/30]: Training Loss = 0.08797
	Epoch[27/30]: Training Loss = 0.08420
	Epoch[28/30]: Training Loss = 0.08182
	Epoch[29/30]: Training Loss = 0.07973
	Epoch[30/30]: Training Loss = 0.07648
***Training Complete***

Final Optimizer Parameters
	alpha : 0.20491266250610352
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 97.180 %
Test Error = 2.820 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.06656996160745621, 0.04257766902446747, -0.12965647876262665, 0.009329292923212051, 0.08824661374092102, 0.04816870391368866, 0.1439427137374878, -0.016438761726021767, 0.177726149559021, 0.1757359802722931, 0.028768736869096756, -0.03493083268404007, 0.02760470286011696, 0.01833687722682953, 0.005577996373176575, 0.12537802755832672, 0.21779868006706238, 0.32058316469192505, 0.15697142481803894, -0.09558571875095367, 0.30446499586105347, 0.1486731916666031, 0.18126371502876282, 0.0025558024644851685, 0.24913081526756287, 0.143489807844162, 0.10812130570411682, 0.17084263265132904, 0.15245071053504944, 0.20491266250610352]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2993985410054525, 0.5422620051225027, 0.3486581476688385, 0.31096713858445485, 0.2859224064509074, 0.26172371065616606, 0.2431438577890396, 0.22394224855105083, 0.20924649183750152, 0.19823613011042276, 0.18583435282309851, 0.17296190439065298, 0.16139360809326173, 0.1533800150156021, 0.14421323899030686, 0.13787594776948292, 0.13148834660847983, 0.1249706759929657, 0.11958352682590484, 0.114321923828125, 0.10854400321443876, 0.10464366867144903, 0.10142184497515361, 0.09754084998369217, 0.09488855003913244, 0.09106443280776341, 0.08796931917170683, 0.08420203430255255, 0.08182375855048497, 0.07973318426012993, 0.07647878504991532]
