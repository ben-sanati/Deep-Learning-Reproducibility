Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.87624
	Epoch[2/30]: Training Loss = 0.34315
	Epoch[3/30]: Training Loss = 0.31141
	Epoch[4/30]: Training Loss = 0.29636
	Epoch[5/30]: Training Loss = 0.28653
	Epoch[6/30]: Training Loss = 0.27902
	Epoch[7/30]: Training Loss = 0.27416
	Epoch[8/30]: Training Loss = 0.27032
	Epoch[9/30]: Training Loss = 0.26667
	Epoch[10/30]: Training Loss = 0.26338
	Epoch[11/30]: Training Loss = 0.25993
	Epoch[12/30]: Training Loss = 0.25645
	Epoch[13/30]: Training Loss = 0.25324
	Epoch[14/30]: Training Loss = 0.25134
	Epoch[15/30]: Training Loss = 0.24943
	Epoch[16/30]: Training Loss = 0.24690
	Epoch[17/30]: Training Loss = 0.24549
	Epoch[18/30]: Training Loss = 0.24512
	Epoch[19/30]: Training Loss = 0.24575
	Epoch[20/30]: Training Loss = 0.24502
	Epoch[21/30]: Training Loss = 0.24433
	Epoch[22/30]: Training Loss = 0.24379
	Epoch[23/30]: Training Loss = 0.24246
	Epoch[24/30]: Training Loss = 0.24143
	Epoch[25/30]: Training Loss = 0.24048
	Epoch[26/30]: Training Loss = 0.23981
	Epoch[27/30]: Training Loss = 0.23890
	Epoch[28/30]: Training Loss = 0.23782
	Epoch[29/30]: Training Loss = 0.23679
	Epoch[30/30]: Training Loss = 0.23646
***Training Complete***

Final Optimizer Parameters
	alpha : -6.496059359051287e-05
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 93.450 %
Test Error = 6.550 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.1013486236333847, 0.053788844496011734, 0.0370514951646328, 0.02542940340936184, 0.024250024929642677, 0.015198217704892159, 0.012554061599075794, 0.0142328767105937, 0.009311413392424583, 0.01440186146646738, 0.014791630208492279, 0.011709846556186676, 0.007499593310058117, 0.011898599565029144, 0.011312751099467278, 0.005151917692273855, -0.0010261947754770517, -0.0023542551789432764, 0.003359088907018304, 0.002540349727496505, 0.004121184349060059, 0.005238774698227644, 0.005698086228221655, 0.0039517320692539215, 0.0027907625772058964, 0.0026314142160117626, 0.005980934947729111, 0.0026634654495865107, 0.00043855863623321056, -6.496059359051287e-05]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3046654697418214, 0.8762392689228058, 0.34315281267166137, 0.31140890924135844, 0.29636188979148864, 0.2865316075960795, 0.27902073690891266, 0.2741580485661825, 0.27032333381970725, 0.26667207264900206, 0.26337545320192973, 0.2599295271237691, 0.2564480478286743, 0.25323891280492145, 0.2513407416820526, 0.2494343377908071, 0.24689733068148295, 0.24549319261709848, 0.24512077086766562, 0.24575174939632416, 0.2450162269592285, 0.24433109095891317, 0.24378542476495108, 0.24245618348121642, 0.24142802127997082, 0.24048144639333088, 0.23981310884157817, 0.23890056223869324, 0.23782398382027944, 0.23678522398471832, 0.2364619545618693]
