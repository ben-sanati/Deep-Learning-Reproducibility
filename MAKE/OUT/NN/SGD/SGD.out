Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.92627
	Epoch[2/30]: Training Loss = 0.34756
	Epoch[3/30]: Training Loss = 0.31351
	Epoch[4/30]: Training Loss = 0.29674
	Epoch[5/30]: Training Loss = 0.28670
	Epoch[6/30]: Training Loss = 0.28079
	Epoch[7/30]: Training Loss = 0.27664
	Epoch[8/30]: Training Loss = 0.27422
	Epoch[9/30]: Training Loss = 0.27322
	Epoch[10/30]: Training Loss = 0.27286
	Epoch[11/30]: Training Loss = 0.27318
	Epoch[12/30]: Training Loss = 0.27191
	Epoch[13/30]: Training Loss = 0.26974
	Epoch[14/30]: Training Loss = 0.26791
	Epoch[15/30]: Training Loss = 0.26578
	Epoch[16/30]: Training Loss = 0.26277
	Epoch[17/30]: Training Loss = 0.26020
	Epoch[18/30]: Training Loss = 0.25759
	Epoch[19/30]: Training Loss = 0.25519
	Epoch[20/30]: Training Loss = 0.25317
	Epoch[21/30]: Training Loss = 0.25089
	Epoch[22/30]: Training Loss = 0.24865
	Epoch[23/30]: Training Loss = 0.24605
	Epoch[24/30]: Training Loss = 0.24334
	Epoch[25/30]: Training Loss = 0.24139
	Epoch[26/30]: Training Loss = 0.23941
	Epoch[27/30]: Training Loss = 0.23811
	Epoch[28/30]: Training Loss = 0.23728
	Epoch[29/30]: Training Loss = 0.23611
	Epoch[30/30]: Training Loss = 0.23531
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0055765677243471146
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 93.510 %
Test Error = 6.490 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.104949951171875, 0.06496929377317429, 0.03910524398088455, 0.02952355332672596, 0.01698293164372444, 0.015829764306545258, 0.006176778115332127, 0.005681285634636879, 0.001971825957298279, -0.0007191664772108197, 0.004891170654445887, 0.009521367959678173, 0.006540841888636351, 0.008609147742390633, 0.011328176595270634, 0.007892553694546223, 0.013174519874155521, 0.007551037706434727, 0.009389295242726803, 0.01139459665864706, 0.011349016800522804, 0.013213985599577427, 0.011116694658994675, 0.007621494121849537, 0.010902222245931625, 0.005412949714809656, 0.004118003416806459, 0.005177764222025871, 0.006922122556716204, 0.0055765677243471146]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.31759374554952, 0.9262711060682932, 0.3475648307959239, 0.31351039700508115, 0.29674383625189465, 0.2867007523536682, 0.28079412471453347, 0.27663526600201926, 0.2742168129285177, 0.27322482045491536, 0.2728630620400111, 0.2731807464758555, 0.2719065010388692, 0.2697446752389272, 0.26791269268194834, 0.26577705345153807, 0.26276567454338073, 0.26020154043833416, 0.25758664890925087, 0.2551944889227549, 0.25316624711354574, 0.25089320267041526, 0.24864534612496694, 0.2460453535079956, 0.24333669560750326, 0.24138819630146027, 0.23941462190945942, 0.23811184462706247, 0.23728165032863616, 0.2361063621600469, 0.23530760576725007]
