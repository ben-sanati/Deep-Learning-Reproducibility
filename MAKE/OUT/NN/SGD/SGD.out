Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: SGD
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.88963
	Epoch[2/30]: Training Loss = 0.34847
	Epoch[3/30]: Training Loss = 0.31610
	Epoch[4/30]: Training Loss = 0.30082
	Epoch[5/30]: Training Loss = 0.29090
	Epoch[6/30]: Training Loss = 0.28303
	Epoch[7/30]: Training Loss = 0.27751
	Epoch[8/30]: Training Loss = 0.27380
	Epoch[9/30]: Training Loss = 0.27154
	Epoch[10/30]: Training Loss = 0.26956
	Epoch[11/30]: Training Loss = 0.26816
	Epoch[12/30]: Training Loss = 0.26691
	Epoch[13/30]: Training Loss = 0.26496
	Epoch[14/30]: Training Loss = 0.26321
	Epoch[15/30]: Training Loss = 0.26167
	Epoch[16/30]: Training Loss = 0.25911
	Epoch[17/30]: Training Loss = 0.25610
	Epoch[18/30]: Training Loss = 0.25308
	Epoch[19/30]: Training Loss = 0.25053
	Epoch[20/30]: Training Loss = 0.24890
	Epoch[21/30]: Training Loss = 0.24712
	Epoch[22/30]: Training Loss = 0.24522
	Epoch[23/30]: Training Loss = 0.24355
	Epoch[24/30]: Training Loss = 0.24281
	Epoch[25/30]: Training Loss = 0.24231
	Epoch[26/30]: Training Loss = 0.24184
	Epoch[27/30]: Training Loss = 0.24179
	Epoch[28/30]: Training Loss = 0.24144
	Epoch[29/30]: Training Loss = 0.24147
	Epoch[30/30]: Training Loss = 0.24135
***Training Complete***

Final Optimizer Parameters
	alpha : 0.00438679987564683
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 93.330 %
Test Error = 6.670 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.09798195958137512, 0.0554930679500103, 0.04095054045319557, 0.029475199058651924, 0.026946421712636948, 0.018109746277332306, 0.010563689284026623, 0.007412024773657322, 0.005994465667754412, 0.00406587915495038, 0.0031902319751679897, 0.009204946458339691, 0.004313488490879536, 0.005313442088663578, 0.01230099517852068, 0.015880053862929344, 0.011895917356014252, 0.009511100128293037, 0.007476087659597397, 0.006638983730226755, 0.010177470743656158, 0.01057002879679203, -0.0019721409771591425, 0.005494256038218737, 0.0016684760339558125, 0.002472663763910532, 0.0019140357617288828, 0.0005618321592919528, 0.0012602184433490038, 0.00438679987564683]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3017010186513267, 0.8896304570674897, 0.34846936155955, 0.3160987837473551, 0.3008169056415558, 0.29090478098392486, 0.2830279103438059, 0.27751341711680094, 0.2737972763220469, 0.2715382372935613, 0.26956382325490313, 0.26816446776390074, 0.266905135846138, 0.2649591493527095, 0.26321015586853025, 0.26166876073678336, 0.2591094216346741, 0.256095409433047, 0.2530774468501409, 0.25053302470843, 0.24890012580553691, 0.2471164635181427, 0.2452249971230825, 0.2435520522117615, 0.24280736593405405, 0.24230816792647045, 0.24183914586702981, 0.24179239175319672, 0.24144370996157327, 0.24146933953762054, 0.24134749930699667]