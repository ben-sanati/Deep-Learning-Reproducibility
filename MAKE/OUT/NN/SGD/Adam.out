Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 1.04900
	Epoch[2/30]: Training Loss = 0.36223
	Epoch[3/30]: Training Loss = 0.32708
	Epoch[4/30]: Training Loss = 0.31523
	Epoch[5/30]: Training Loss = 0.30803
	Epoch[6/30]: Training Loss = 0.30120
	Epoch[7/30]: Training Loss = 0.29784
	Epoch[8/30]: Training Loss = 0.29509
	Epoch[9/30]: Training Loss = 0.29137
	Epoch[10/30]: Training Loss = 0.28804
	Epoch[11/30]: Training Loss = 0.28380
	Epoch[12/30]: Training Loss = 0.27979
	Epoch[13/30]: Training Loss = 0.27665
	Epoch[14/30]: Training Loss = 0.27489
	Epoch[15/30]: Training Loss = 0.27171
	Epoch[16/30]: Training Loss = 0.26745
	Epoch[17/30]: Training Loss = 0.26732
	Epoch[18/30]: Training Loss = 0.26425
	Epoch[19/30]: Training Loss = 0.26248
	Epoch[20/30]: Training Loss = 0.26008
	Epoch[21/30]: Training Loss = 0.25885
	Epoch[22/30]: Training Loss = 0.25690
	Epoch[23/30]: Training Loss = 0.25393
	Epoch[24/30]: Training Loss = 0.25257
	Epoch[25/30]: Training Loss = 0.25138
	Epoch[26/30]: Training Loss = 0.25015
	Epoch[27/30]: Training Loss = 0.24833
	Epoch[28/30]: Training Loss = 0.24652
	Epoch[29/30]: Training Loss = 0.24519
	Epoch[30/30]: Training Loss = 0.24359
***Training Complete***

Final Optimizer Parameters
	alpha : 0.002526737516745925
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 93.290 %
Test Error = 6.710 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.10554639995098114, 0.045513808727264404, 0.023676471784710884, 0.01518978737294674, 0.023650310933589935, 0.004212146159261465, 0.008294510655105114, 0.013684497214853764, 0.0003452566743362695, 0.014228392392396927, 0.012995649129152298, 0.007048708852380514, 0.0063361250795423985, 0.022161606699228287, 0.016196951270103455, 0.0017501245019957423, 0.0173284113407135, 0.00011343852384015918, 0.009872162714600563, 0.0039542801678180695, 0.014348717406392097, 0.010830027051270008, 0.007891294546425343, 0.0023944403510540724, 0.004633396398276091, 0.004137324169278145, 0.011706551536917686, 0.0014758501201868057, 0.001978802727535367, 0.002526737516745925]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3046654697418214, 1.0490001012166341, 0.362232735824585, 0.3270836154460907, 0.3152301758607229, 0.3080263648668925, 0.30119760026931763, 0.2978381202061971, 0.29508634339968365, 0.29137365899086, 0.28804126207828523, 0.28379762967427574, 0.27978549041748046, 0.276652353421847, 0.2748946552912394, 0.2717061905384064, 0.2674459961573283, 0.267316792289416, 0.26425345265865324, 0.2624792542457581, 0.260082959651947, 0.25885367798805237, 0.25690059934457143, 0.2539298482577006, 0.2525743365844091, 0.2513797828833262, 0.2501483653386434, 0.2483333843866984, 0.24651686069170634, 0.24518709410826364, 0.24359093826611836]
