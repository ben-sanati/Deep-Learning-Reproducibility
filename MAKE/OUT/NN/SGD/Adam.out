Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: Adam
	hyperoptimizer_args: {'beta1': 0.9, 'beta2': 0.99}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.001
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 1.06516
	Epoch[2/30]: Training Loss = 0.36266
	Epoch[3/30]: Training Loss = 0.32847
	Epoch[4/30]: Training Loss = 0.31758
	Epoch[5/30]: Training Loss = 0.31002
	Epoch[6/30]: Training Loss = 0.30787
	Epoch[7/30]: Training Loss = 0.30653
	Epoch[8/30]: Training Loss = 0.30424
	Epoch[9/30]: Training Loss = 0.30202
	Epoch[10/30]: Training Loss = 0.30076
	Epoch[11/30]: Training Loss = 0.29358
	Epoch[12/30]: Training Loss = 0.28785
	Epoch[13/30]: Training Loss = 0.28688
	Epoch[14/30]: Training Loss = 0.28210
	Epoch[15/30]: Training Loss = 0.27704
	Epoch[16/30]: Training Loss = 0.27514
	Epoch[17/30]: Training Loss = 0.27439
	Epoch[18/30]: Training Loss = 0.27244
	Epoch[19/30]: Training Loss = 0.26855
	Epoch[20/30]: Training Loss = 0.26443
	Epoch[21/30]: Training Loss = 0.26158
	Epoch[22/30]: Training Loss = 0.26133
	Epoch[23/30]: Training Loss = 0.25908
	Epoch[24/30]: Training Loss = 0.25728
	Epoch[25/30]: Training Loss = 0.25607
	Epoch[26/30]: Training Loss = 0.25388
	Epoch[27/30]: Training Loss = 0.25105
	Epoch[28/30]: Training Loss = 0.24816
	Epoch[29/30]: Training Loss = 0.24674
	Epoch[30/30]: Training Loss = 0.24499
***Training Complete***

Final Optimizer Parameters
	alpha : 0.007342264987528324
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 93.280 %
Test Error = 6.720 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.10625415295362473, 0.04211350902915001, 0.023621641099452972, 0.02439178340137005, -0.002311579417437315, 0.0041258251294493675, 0.012272742576897144, 0.006001814268529415, 0.003232467221096158, 0.032196078449487686, 0.011275574564933777, 0.0024603111669421196, 0.015914898365736008, 0.02261560596525669, 0.008215025998651981, 0.0014599128626286983, 0.004611778538674116, 0.012298108078539371, 0.015272931195795536, 0.008671420626342297, 0.0021557991858571768, 0.012299918569624424, 0.007964669726788998, 0.00020474338089115918, 0.009963748045265675, 0.007712709717452526, 0.0059769428335130215, 0.008363374508917332, 0.007138379849493504, 0.007342264987528324]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.309692253367106, 1.0651626471360525, 0.3626634153366089, 0.32846814964612325, 0.31757829825083417, 0.31001676618258156, 0.30787151285012565, 0.3065313459396362, 0.3042405408064524, 0.3020195483048757, 0.3007555729071299, 0.2935835183461507, 0.287849693163236, 0.28687949113845823, 0.28209988651275636, 0.2770415637493134, 0.2751377522468567, 0.2743907192548116, 0.2724354397773743, 0.26855122741063436, 0.2644311806122462, 0.26158065087795257, 0.26133294728597006, 0.25907748910586037, 0.25728404448032377, 0.2560696622927984, 0.2538802634557088, 0.2510525209267934, 0.24816182465553283, 0.2467391338030497, 0.2449923671245575]
