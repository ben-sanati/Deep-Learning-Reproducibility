Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.83729
	Epoch[2/30]: Training Loss = 0.33780
	Epoch[3/30]: Training Loss = 0.29762
	Epoch[4/30]: Training Loss = 0.27419
	Epoch[5/30]: Training Loss = 0.25762
	Epoch[6/30]: Training Loss = 0.24491
	Epoch[7/30]: Training Loss = 0.23527
	Epoch[8/30]: Training Loss = 0.22743
	Epoch[9/30]: Training Loss = 0.22067
	Epoch[10/30]: Training Loss = 0.21481
	Epoch[11/30]: Training Loss = 0.20997
	Epoch[12/30]: Training Loss = 0.20553
	Epoch[13/30]: Training Loss = 0.20159
	Epoch[14/30]: Training Loss = 0.19814
	Epoch[15/30]: Training Loss = 0.19492
	Epoch[16/30]: Training Loss = 0.19169
	Epoch[17/30]: Training Loss = 0.18875
	Epoch[18/30]: Training Loss = 0.18596
	Epoch[19/30]: Training Loss = 0.18322
	Epoch[20/30]: Training Loss = 0.18053
	Epoch[21/30]: Training Loss = 0.17788
	Epoch[22/30]: Training Loss = 0.17530
	Epoch[23/30]: Training Loss = 0.17288
	Epoch[24/30]: Training Loss = 0.17070
	Epoch[25/30]: Training Loss = 0.16879
	Epoch[26/30]: Training Loss = 0.16708
	Epoch[27/30]: Training Loss = 0.16526
	Epoch[28/30]: Training Loss = 0.16381
	Epoch[29/30]: Training Loss = 0.16242
	Epoch[30/30]: Training Loss = 0.16121
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0114141209051013
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 95.090 %
Test Error = 4.910 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.11744984239339828, 0.08870665729045868, 0.07347345352172852, 0.06252077221870422, 0.05193537473678589, 0.04379655420780182, 0.038687121123075485, 0.03527064621448517, 0.03132696822285652, 0.028158023953437805, 0.02742048352956772, 0.023830194026231766, 0.022338302806019783, 0.02338186278939247, 0.022571207955479622, 0.020427705720067024, 0.02184820920228958, 0.02192096970975399, 0.022265831008553505, 0.021836064755916595, 0.021703489124774933, 0.02047729305922985, 0.018048131838440895, 0.01735379919409752, 0.015904555097222328, 0.015612488612532616, 0.014242017641663551, 0.012250383384525776, 0.010957467369735241, 0.0114141209051013]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.301087348429362, 0.8372896760940551, 0.3378014950116475, 0.29761618588765465, 0.27418624335924785, 0.25761750863393146, 0.24490752654075623, 0.2352674726009369, 0.22742647296587626, 0.22066964213053386, 0.2148055289189021, 0.20996766521135965, 0.2055266584078471, 0.2015874077518781, 0.19813537471294404, 0.19492337058782577, 0.19169255313078562, 0.18874925492604575, 0.18596320256789525, 0.1832160961151123, 0.18053459508419037, 0.17787794080575306, 0.1752998233238856, 0.17288401148319243, 0.17069818023045857, 0.16879083148638407, 0.16708343159357708, 0.1652614313840866, 0.16381450039148332, 0.1624236585299174, 0.1612076647679011]
