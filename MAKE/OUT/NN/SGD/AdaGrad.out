Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	baseline: False
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 0.82895
	Epoch[2/30]: Training Loss = 0.33779
	Epoch[3/30]: Training Loss = 0.29755
	Epoch[4/30]: Training Loss = 0.27491
	Epoch[5/30]: Training Loss = 0.25848
	Epoch[6/30]: Training Loss = 0.24570
	Epoch[7/30]: Training Loss = 0.23580
	Epoch[8/30]: Training Loss = 0.22751
	Epoch[9/30]: Training Loss = 0.22047
	Epoch[10/30]: Training Loss = 0.21416
	Epoch[11/30]: Training Loss = 0.20872
	Epoch[12/30]: Training Loss = 0.20373
	Epoch[13/30]: Training Loss = 0.19923
	Epoch[14/30]: Training Loss = 0.19521
	Epoch[15/30]: Training Loss = 0.19171
	Epoch[16/30]: Training Loss = 0.18853
	Epoch[17/30]: Training Loss = 0.18556
	Epoch[18/30]: Training Loss = 0.18320
	Epoch[19/30]: Training Loss = 0.18102
	Epoch[20/30]: Training Loss = 0.17923
	Epoch[21/30]: Training Loss = 0.17766
	Epoch[22/30]: Training Loss = 0.17619
	Epoch[23/30]: Training Loss = 0.17480
	Epoch[24/30]: Training Loss = 0.17354
	Epoch[25/30]: Training Loss = 0.17235
	Epoch[26/30]: Training Loss = 0.17133
	Epoch[27/30]: Training Loss = 0.17037
	Epoch[28/30]: Training Loss = 0.16937
	Epoch[29/30]: Training Loss = 0.16846
	Epoch[30/30]: Training Loss = 0.16764
***Training Complete***

Final Optimizer Parameters
	alpha : 0.006232438609004021
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 95.220 %
Test Error = 4.780 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.11430715024471283, 0.0833965539932251, 0.06816499680280685, 0.05740572512149811, 0.05224110931158066, 0.043726082891225815, 0.03948948159813881, 0.03707725927233696, 0.03342138230800629, 0.032888758927583694, 0.03145230561494827, 0.028831038624048233, 0.026170438155531883, 0.024914616718888283, 0.023631015792489052, 0.02062246948480606, 0.017434269189834595, 0.01588836871087551, 0.013634827919304371, 0.012429256923496723, 0.011904021725058556, 0.011653461493551731, 0.010797152295708656, 0.009987333789467812, 0.008689667098224163, 0.008564477786421776, 0.008785820566117764, 0.007649832870811224, 0.00773124024271965, 0.006232438609004021]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3046654697418214, 0.828947336657842, 0.33779156835079194, 0.2975523266474406, 0.274914643239975, 0.25847936080296835, 0.24569694486459095, 0.23580340713659922, 0.22750694794654847, 0.22046801508267722, 0.21416352763175964, 0.2087242462873459, 0.20373295462528865, 0.19923497374455135, 0.1952110831975937, 0.19171461554368338, 0.188529621330897, 0.1855623322725296, 0.18319579521814983, 0.1810198041041692, 0.17923144007523856, 0.17765565996964772, 0.17619119850794474, 0.17480064143339794, 0.17353884885311127, 0.1723471586783727, 0.1713345718383789, 0.1703714664697647, 0.16936993120908736, 0.16845539325873057, 0.1676362325668335]
