Args:
	model: NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: AdaGrad
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: 0.01
	num_epochs: 30
	batch_size: 256
	device: cuda

***Beginning Training***
	Epoch[1/30]: Training Loss = 0.84073
	Epoch[2/30]: Training Loss = 0.33869
	Epoch[3/30]: Training Loss = 0.29884
	Epoch[4/30]: Training Loss = 0.27535
	Epoch[5/30]: Training Loss = 0.25754
	Epoch[6/30]: Training Loss = 0.24380
	Epoch[7/30]: Training Loss = 0.23276
	Epoch[8/30]: Training Loss = 0.22338
	Epoch[9/30]: Training Loss = 0.21516
	Epoch[10/30]: Training Loss = 0.20813
	Epoch[11/30]: Training Loss = 0.20164
	Epoch[12/30]: Training Loss = 0.19587
	Epoch[13/30]: Training Loss = 0.19096
	Epoch[14/30]: Training Loss = 0.18665
	Epoch[15/30]: Training Loss = 0.18304
	Epoch[16/30]: Training Loss = 0.18001
	Epoch[17/30]: Training Loss = 0.17725
	Epoch[18/30]: Training Loss = 0.17463
	Epoch[19/30]: Training Loss = 0.17220
	Epoch[20/30]: Training Loss = 0.16995
	Epoch[21/30]: Training Loss = 0.16782
	Epoch[22/30]: Training Loss = 0.16578
	Epoch[23/30]: Training Loss = 0.16374
	Epoch[24/30]: Training Loss = 0.16159
	Epoch[25/30]: Training Loss = 0.15966
	Epoch[26/30]: Training Loss = 0.15780
	Epoch[27/30]: Training Loss = 0.15606
	Epoch[28/30]: Training Loss = 0.15451
	Epoch[29/30]: Training Loss = 0.15284
	Epoch[30/30]: Training Loss = 0.15136
***Training Complete***

Final Optimizer Parameters
	alpha : 0.014081744477152824
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 95.340 %
Test Error = 4.660 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.1126658096909523, 0.0842427983880043, 0.07527748495340347, 0.06405828893184662, 0.054851047694683075, 0.05023163557052612, 0.04598231986165047, 0.042098965495824814, 0.04026561602950096, 0.03895965591073036, 0.03666479140520096, 0.03302682936191559, 0.02984951063990593, 0.0246762465685606, 0.023161832243204117, 0.021457672119140625, 0.020914997905492783, 0.02063130773603916, 0.01878497190773487, 0.01862536557018757, 0.017994727939367294, 0.019355397671461105, 0.02044118195772171, 0.01755334809422493, 0.017609601840376854, 0.015903431922197342, 0.01650988683104515, 0.01618489995598793, 0.016454258933663368, 0.014081744477152824]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.3015304805755616, 0.8407296403408051, 0.3386900739192963, 0.2988437146822612, 0.2753458548386892, 0.2575389536698659, 0.24380274952252706, 0.23276373229821523, 0.2233824362675349, 0.21516425357659658, 0.2081263292312622, 0.20163868769804635, 0.19587373181978862, 0.19096410038471223, 0.1866480831305186, 0.1830444544672966, 0.1800066633939743, 0.17724962637424468, 0.17463311726252237, 0.17220371233622234, 0.16994608806769054, 0.1678201486905416, 0.16578284413019817, 0.1637412612915039, 0.16158947786490122, 0.1596602974653244, 0.15779693880875906, 0.15605927780071893, 0.15450750911235808, 0.15283650801976523, 0.15136131939093272]
