Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.3046654697418214
	Epoch[1/30]: Training Loss = 1.40153
	Epoch[2/30]: Training Loss = 0.58313
	Epoch[3/30]: Training Loss = 0.44351
	Epoch[4/30]: Training Loss = 0.38984
	Epoch[5/30]: Training Loss = 0.35991
	Epoch[6/30]: Training Loss = 0.33960
	Epoch[7/30]: Training Loss = 0.32407
	Epoch[8/30]: Training Loss = 0.31117
	Epoch[9/30]: Training Loss = 0.30020
	Epoch[10/30]: Training Loss = 0.29042
	Epoch[11/30]: Training Loss = 0.28176
	Epoch[12/30]: Training Loss = 0.27378
	Epoch[13/30]: Training Loss = 0.26644
	Epoch[14/30]: Training Loss = 0.25949
	Epoch[15/30]: Training Loss = 0.25298
	Epoch[16/30]: Training Loss = 0.24704
	Epoch[17/30]: Training Loss = 0.24117
	Epoch[18/30]: Training Loss = 0.23576
	Epoch[19/30]: Training Loss = 0.23050
	Epoch[20/30]: Training Loss = 0.22557
	Epoch[21/30]: Training Loss = 0.22079
	Epoch[22/30]: Training Loss = 0.21620
	Epoch[23/30]: Training Loss = 0.21192
	Epoch[24/30]: Training Loss = 0.20770
	Epoch[25/30]: Training Loss = 0.20370
	Epoch[26/30]: Training Loss = 0.19993
	Epoch[27/30]: Training Loss = 0.19625
	Epoch[28/30]: Training Loss = 0.19268
	Epoch[29/30]: Training Loss = 0.18918
	Epoch[30/30]: Training Loss = 0.18597
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009999999776482582

***Testing Results***
==============================
Test Accuracy = 94.740 %
Test Error = 5.260 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582]
Loss: [2.3046654697418214, 1.4015250642140706, 0.5831331830501556, 0.4435100428899129, 0.389844685125351, 0.35990518538157146, 0.33959747642676036, 0.3240681730747223, 0.3111723114490509, 0.30020292809804283, 0.2904193488677343, 0.2817591736952464, 0.2737760240316391, 0.2664413574854533, 0.2594912750085195, 0.2529824280897776, 0.24703531958262126, 0.24116569017569223, 0.23575604561964672, 0.23049743777116138, 0.2255722058216731, 0.22078809758027396, 0.21619959339300793, 0.2119196620464325, 0.20770267206033072, 0.20369826390743256, 0.19993143802483876, 0.1962532872756322, 0.19268009306987127, 0.1891844580968221, 0.18596936829884847]
