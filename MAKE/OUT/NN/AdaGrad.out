Args:
	model: NN
	optimizer: AdaGrad
	optimizer_args: {}
	hyperoptimizer: NoOp
	hyperoptimizer_args: {}
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 30
	batch_size: 256
	baseline: True
	device: cuda
Using 4 GPUs

***Beginning Training***
	Epoch[1/30]: Training Loss = 1.43902
	Epoch[2/30]: Training Loss = 0.59222
	Epoch[3/30]: Training Loss = 0.44689
	Epoch[4/30]: Training Loss = 0.39244
	Epoch[5/30]: Training Loss = 0.36200
	Epoch[6/30]: Training Loss = 0.34148
	Epoch[7/30]: Training Loss = 0.32596
	Epoch[8/30]: Training Loss = 0.31327
	Epoch[9/30]: Training Loss = 0.30257
	Epoch[10/30]: Training Loss = 0.29305
	Epoch[11/30]: Training Loss = 0.28437
	Epoch[12/30]: Training Loss = 0.27653
	Epoch[13/30]: Training Loss = 0.26912
	Epoch[14/30]: Training Loss = 0.26218
	Epoch[15/30]: Training Loss = 0.25570
	Epoch[16/30]: Training Loss = 0.24943
	Epoch[17/30]: Training Loss = 0.24353
	Epoch[18/30]: Training Loss = 0.23777
	Epoch[19/30]: Training Loss = 0.23249
	Epoch[20/30]: Training Loss = 0.22730
	Epoch[21/30]: Training Loss = 0.22247
	Epoch[22/30]: Training Loss = 0.21783
	Epoch[23/30]: Training Loss = 0.21333
	Epoch[24/30]: Training Loss = 0.20928
	Epoch[25/30]: Training Loss = 0.20515
	Epoch[26/30]: Training Loss = 0.20131
	Epoch[27/30]: Training Loss = 0.19748
	Epoch[28/30]: Training Loss = 0.19392
	Epoch[29/30]: Training Loss = 0.19041
	Epoch[30/30]: Training Loss = 0.18713
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009999999776482582

***Testing Results***
==============================
Test Accuracy = 94.740 %
Test Error = 5.260 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]
alpha: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582]
Loss: [0.008978149918715159, 1.4390232450167337, 0.5922230015913645, 0.44688580193519595, 0.3924422764778137, 0.3620037571112315, 0.3414824965000153, 0.32596270486513773, 0.3132719861507416, 0.3025710452397664, 0.2930530886173248, 0.28436785740852355, 0.27653383685747784, 0.26912250396410625, 0.2621839666525523, 0.25569959416389465, 0.24943078295389812, 0.24352692074775695, 0.23777119612693787, 0.23249414045413336, 0.22729776493708292, 0.2224661337296168, 0.21783186394373577, 0.213331845053037, 0.2092783732732137, 0.20515380870501201, 0.20131401954491934, 0.1974836597363154, 0.19391596721808116, 0.19041006876627603, 0.18713424184322358]
