Args:
	model: stacked_NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 6
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 1.0
	kappa: None
	num_epochs: 20
	batch_size: 1024
	baseline: False
	device: cuda
	Alpha Values: {0: 1.0, 1: 0.005, 2: 0.0033333333333333335, 3: 0.0025, 4: 0.002, 5: 0.0016666666666666668, 6: 0.0014285714285714286}
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.2928077096303303
	Epoch[1/20]: Training Loss = 0.76796
	Epoch[2/20]: Training Loss = 0.30380
	Epoch[3/20]: Training Loss = 0.27744
	Epoch[4/20]: Training Loss = 0.26368
	Epoch[5/20]: Training Loss = 0.25305
	Epoch[6/20]: Training Loss = 0.24360
	Epoch[7/20]: Training Loss = 0.23505
	Epoch[8/20]: Training Loss = 0.22725
	Epoch[9/20]: Training Loss = 0.22046
	Epoch[10/20]: Training Loss = 0.21477
	Epoch[11/20]: Training Loss = 0.20958
	Epoch[12/20]: Training Loss = 0.20473
	Epoch[13/20]: Training Loss = 0.20009
	Epoch[14/20]: Training Loss = 0.19569
	Epoch[15/20]: Training Loss = 0.19150
	Epoch[16/20]: Training Loss = 0.18764
	Epoch[17/20]: Training Loss = 0.18376
	Epoch[18/20]: Training Loss = 0.18024
	Epoch[19/20]: Training Loss = 0.17684
	Epoch[20/20]: Training Loss = 0.17343
***Training Complete***

Final Optimizer Parameters
	alpha : 0.10145014524459839
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 95.010 %
Test Error = 4.990 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
alpha: [1.0, 0.4765876233577728, 0.13879460096359253, 0.13151556253433228, 0.12119249999523163, 0.12119244039058685, 0.1211918368935585, 0.1211898922920227, 0.12098770588636398, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839, 0.10145014524459839]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2928077096303303, 0.7679550185998281, 0.3037961107412974, 0.27744369290669757, 0.26368499460220335, 0.2530538266181946, 0.24360123399893444, 0.23505180529753367, 0.22724845651785533, 0.22045947014490763, 0.2147737830877304, 0.20958059650262198, 0.20472591688632966, 0.20008903208573658, 0.1956921219666799, 0.1914984642426173, 0.18763882727622985, 0.18376079858144123, 0.18023706914583842, 0.1768377383629481, 0.17343164150714874]
