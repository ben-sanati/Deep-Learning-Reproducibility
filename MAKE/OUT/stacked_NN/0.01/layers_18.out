Args:
	model: stacked_NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 18
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 20
	batch_size: 1024
	baseline: False
	device: cuda
	Alpha Values: {0: 0.01, 1: 5e-05, 2: 3.3333333333333335e-05, 3: 2.5e-05, 4: 2e-05, 5: 1.6666666666666667e-05, 6: 1.4285714285714285e-05, 7: 1.25e-05, 8: 1.1111111111111112e-05, 9: 1e-05, 10: 9.090909090909091e-06, 11: 8.333333333333334e-06, 12: 7.692307692307692e-06, 13: 7.142857142857143e-06, 14: 6.666666666666667e-06, 15: 6.25e-06, 16: 5.882352941176471e-06, 17: 5.555555555555556e-06, 18: 5.263157894736842e-06}
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.2928077096303303
	Epoch[1/20]: Training Loss = 2.23571
	Epoch[2/20]: Training Loss = 2.03754
	Epoch[3/20]: Training Loss = 1.63147
	Epoch[4/20]: Training Loss = 1.08292
	Epoch[5/20]: Training Loss = 0.70801
	Epoch[6/20]: Training Loss = 0.52238
	Epoch[7/20]: Training Loss = 0.43480
	Epoch[8/20]: Training Loss = 0.39108
	Epoch[9/20]: Training Loss = 0.36521
	Epoch[10/20]: Training Loss = 0.34878
	Epoch[11/20]: Training Loss = 0.33726
	Epoch[12/20]: Training Loss = 0.32923
	Epoch[13/20]: Training Loss = 0.32342
	Epoch[14/20]: Training Loss = 0.32003
	Epoch[15/20]: Training Loss = 0.31767
	Epoch[16/20]: Training Loss = 0.31581
	Epoch[17/20]: Training Loss = 0.31400
	Epoch[18/20]: Training Loss = 0.31219
	Epoch[19/20]: Training Loss = 0.31042
	Epoch[20/20]: Training Loss = 0.30834
***Training Complete***

Final Optimizer Parameters
	alpha : 0.019705815240740776
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 91.670 %
Test Error = 8.330 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
alpha: [0.009999999776482582, 0.014431474730372429, 0.021611075848340988, 0.03355579823255539, 0.05136905610561371, 0.07764405012130737, 0.09608327597379684, 0.09612773358821869, 0.09671911597251892, 0.08203045278787613, 0.07286642491817474, 0.05797679349780083, 0.047732215374708176, 0.025881852954626083, 0.018480105325579643, 0.01812027394771576, 0.015897467732429504, 0.018098123371601105, 0.016245726495981216, 0.022308427840471268, 0.019705815240740776]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2928077096303303, 2.235713469696045, 2.0375370403289796, 1.6314659224828085, 1.0829219409306845, 0.708011600748698, 0.5223771301746368, 0.4348045021533966, 0.39108123890558877, 0.3652055847009023, 0.3487789245605469, 0.3372601432323456, 0.32923218197822574, 0.3234177768866221, 0.32002546399434406, 0.31766746999422707, 0.3158124541441599, 0.31399760603904725, 0.31219148802757263, 0.31041996752421064, 0.3083426197528839]
