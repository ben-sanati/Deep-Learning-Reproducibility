Args:
	model: stacked_NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.01
	kappa: None
	num_epochs: 20
	batch_size: 1024
	baseline: False
	device: cuda
	Alpha Values: {}
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.2928077096303303
	Epoch[1/20]: Training Loss = 2.24374
	Epoch[2/20]: Training Loss = 2.12140
	Epoch[3/20]: Training Loss = 1.96195
	Epoch[4/20]: Training Loss = 1.77532
	Epoch[5/20]: Training Loss = 1.57688
	Epoch[6/20]: Training Loss = 1.38588
	Epoch[7/20]: Training Loss = 1.21817
	Epoch[8/20]: Training Loss = 1.08002
	Epoch[9/20]: Training Loss = 0.96965
	Epoch[10/20]: Training Loss = 0.88209
	Epoch[11/20]: Training Loss = 0.81217
	Epoch[12/20]: Training Loss = 0.75565
	Epoch[13/20]: Training Loss = 0.70926
	Epoch[14/20]: Training Loss = 0.67076
	Epoch[15/20]: Training Loss = 0.63823
	Epoch[16/20]: Training Loss = 0.61053
	Epoch[17/20]: Training Loss = 0.58664
	Epoch[18/20]: Training Loss = 0.56583
	Epoch[19/20]: Training Loss = 0.54756
	Epoch[20/20]: Training Loss = 0.53138
***Training Complete***

Final Optimizer Parameters
	alpha : 0.009999999776482582
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 87.700 %
Test Error = 12.300 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
alpha: [0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582, 0.009999999776482582]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2928077096303303, 2.243738358561198, 2.121404686864217, 1.9619521993001303, 1.7753170473734539, 1.576880376625061, 1.3858778862635295, 1.2181701567967733, 1.0800169235865276, 0.9696543922742208, 0.8820937119801839, 0.8121694459915161, 0.7556505428632101, 0.7092582005182902, 0.670761533387502, 0.6382338730812073, 0.6105318153063456, 0.586644719282786, 0.565829132715861, 0.5475625544230143, 0.5313808579762777]
