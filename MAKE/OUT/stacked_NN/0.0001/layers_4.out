Args:
	model: stacked_NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 4
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.0001
	kappa: None
	num_epochs: 20
	batch_size: 1024
	baseline: False
	device: cuda
	Alpha Values: {0: 0.0001, 1: 5e-07, 2: 3.3333333333333335e-07, 3: 2.5e-07, 4: 2.0000000000000002e-07}
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.2928077096303303
	Epoch[1/20]: Training Loss = 2.29228
	Epoch[2/20]: Training Loss = 2.29100
	Epoch[3/20]: Training Loss = 2.28941
	Epoch[4/20]: Training Loss = 2.28750
	Epoch[5/20]: Training Loss = 2.28526
	Epoch[6/20]: Training Loss = 2.28265
	Epoch[7/20]: Training Loss = 2.27967
	Epoch[8/20]: Training Loss = 2.27630
	Epoch[9/20]: Training Loss = 2.27250
	Epoch[10/20]: Training Loss = 2.26825
	Epoch[11/20]: Training Loss = 2.26352
	Epoch[12/20]: Training Loss = 2.25827
	Epoch[13/20]: Training Loss = 2.25245
	Epoch[14/20]: Training Loss = 2.24601
	Epoch[15/20]: Training Loss = 2.23889
	Epoch[16/20]: Training Loss = 2.23101
	Epoch[17/20]: Training Loss = 2.22230
	Epoch[18/20]: Training Loss = 2.21265
	Epoch[19/20]: Training Loss = 2.20197
	Epoch[20/20]: Training Loss = 2.19014
***Training Complete***

Final Optimizer Parameters
	alpha : 0.0010575995547696948
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 51.010 %
Test Error = 48.990 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
alpha: [9.999999747378752e-05, 0.00013338917051441967, 0.00016512932779733092, 0.00019859876192640513, 0.00023367135145235807, 0.0002704071521293372, 0.00030881763086654246, 0.00034911007969640195, 0.0003911030071321875, 0.00043511169496923685, 0.00048102738219313323, 0.0005287484964355826, 0.0005787727423012257, 0.000630687631200999, 0.0006848700577393174, 0.0007411499973386526, 0.0007997212815098464, 0.0008606984629295766, 0.0009238398633897305, 0.000989452819339931, 0.0010575995547696948]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2928077096303303, 2.2922803415934245, 2.2909992204030356, 2.2894118602752687, 2.287503251647949, 2.2852559820810954, 2.2826517064412437, 2.2796732151031494, 2.276297471745809, 2.272498478571574, 2.268251144282023, 2.2635196969350178, 2.25826825205485, 2.2524493724822996, 2.246011102294922, 2.238887541453044, 2.2310105843861896, 2.222296308135986, 2.212649308013916, 2.2019656728108723, 2.190136730957031]
