Args:
	model: stacked_NN
	optimizer: SGD
	optimizer_args: {}
	hyperoptimizer: RMSProp
	hyperoptimizer_args: {}
	num_hyperoptimizers: 0
	loss_fn: CrossEntropyLoss
	dataset: MNIST
	alpha: 0.0001
	kappa: None
	num_epochs: 20
	batch_size: 1024
	baseline: False
	device: cuda
	Alpha Values: {}
	Using 4 GPUs

***Beginning Training***
	Initial Train Loss: 2.2928077096303303
	Epoch[1/20]: Training Loss = 2.29234
	Epoch[2/20]: Training Loss = 2.29138
	Epoch[3/20]: Training Loss = 2.29042
	Epoch[4/20]: Training Loss = 2.28946
	Epoch[5/20]: Training Loss = 2.28850
	Epoch[6/20]: Training Loss = 2.28753
	Epoch[7/20]: Training Loss = 2.28657
	Epoch[8/20]: Training Loss = 2.28561
	Epoch[9/20]: Training Loss = 2.28465
	Epoch[10/20]: Training Loss = 2.28369
	Epoch[11/20]: Training Loss = 2.28272
	Epoch[12/20]: Training Loss = 2.28176
	Epoch[13/20]: Training Loss = 2.28080
	Epoch[14/20]: Training Loss = 2.27983
	Epoch[15/20]: Training Loss = 2.27886
	Epoch[16/20]: Training Loss = 2.27790
	Epoch[17/20]: Training Loss = 2.27693
	Epoch[18/20]: Training Loss = 2.27596
	Epoch[19/20]: Training Loss = 2.27499
	Epoch[20/20]: Training Loss = 2.27402
***Training Complete***

Final Optimizer Parameters
	alpha : 9.999999747378752e-05
	mu : 0.0

***Testing Results***
==============================
Test Accuracy = 19.980 %
Test Error = 80.020 %
==============================

Plotted Lists:
Epochs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
alpha: [9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05, 9.999999747378752e-05]
mu: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Loss: [2.2928077096303303, 2.2923393347422283, 2.291378468068441, 2.290417647298177, 2.289456930033366, 2.288495770136515, 2.2875341150919595, 2.286572550201416, 2.285611055246989, 2.2846488740285236, 2.2836864065806073, 2.282723248418172, 2.281759587987264, 2.2807954301198325, 2.2798307809193927, 2.278864965693156, 2.2778985248565675, 2.276930902862549, 2.275962048212687, 2.274992134094238, 2.2740210501352944]
